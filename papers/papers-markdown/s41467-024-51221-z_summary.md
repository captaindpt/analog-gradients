# Fast and robust analog in-memory deep neural network training

**Authors:** Malte J. Rasch (IBM Research & Sony AI), Fabio Carta, Omobayode Fagbohungbe, Tayfun Gokmen (IBM Research)
**Year:** 2024
**Journal:** Nature Communications
**DOI:** 10.1038/s41467-024-51221-z

## Core Thesis
This paper proposes **two improved algorithms (Chopped-TTv2 and AGAD)** for in-memory training that solve the critical problem of **reference conductance instability** that plagued the previous TTv2 algorithm, while maintaining the same O(1) fast runtime complexity.

## The Problem This Paper Solves

**Background context:**
- Paper 1 (2017): Demonstrated on-chip training with memristors - worked but had issues
- Paper 3 (2025): Showed improved memristor devices (2D materials) - but still used external training
- TTv2 (baseline, 2021): Introduced algorithm for in-memory training with better device tolerance

**The critical flaw of TTv2:**
```
TTv2 Algorithm Requirements:
    ✅ Use asymmetric devices (like ReRAM)
    ✅ Accumulate gradients on array A
    ✅ Use reference array R to subtract symmetry point
    ❌ PROBLEM: R must be programmed to within ~2% of correct value
    ❌ If R has 5%+ error → Training completely breaks down

Real-world reality:
    - Programming error for memristors: 5-10% typical
    - Conductance drift over time: Also 5-10%+
    - Result: TTv2 is theoretically elegant but practically unusable
```

## The Two Proposed Solutions

### Solution 1: Chopped-TTv2 (c-TTv2)

**Core idea:** Borrow the "chopper" technique from analog circuit design

```
Normal gradient accumulation:
    A ← A + λ·d·x^T
    [Accumulates with constant offset if R is wrong]

Chopped gradient accumulation:
    Flip signs randomly: c ∈ {-1, 1}
    A ← A + λ·d·(c⊙x)^T  [⊙ = element-wise multiply]

When reading (every ns updates):
    z_k = c_k · (A - R) · v_k  [Correct for sign flip]

Result: Offsets cancel out over time!
```

**Why it works:**
```
With fixed offset o in reference:
    Normal TTv2:
        Gradient accumulates: g + o, g + o, g + o, ... → Wrong!

    Chopped-TTv2:
        Gradient accumulates: g + o, -(g + o), g + o, -(g + o), ...
        After sign correction: g + o, g - o, g + o, g - o, ...
        Average = g → Correct!
```

**Performance:**
- Tolerates reference errors up to **25%** (vs 2% for TTv2)
- Same runtime as TTv2
- Still requires differential read of (A - R)
- Still needs asymmetric devices

### Solution 2: AGAD (Analog Gradient Accumulation with Dynamic reference)

**Core idea:** Don't program a reference at all - compute it on-the-fly from recent history

```
Instead of:
    z_k = c_k · (A - R_programmed) · v_k

Do:
    z_k = c_k · (A · v_k - p_k^ref)

Where p_k^ref is computed dynamically:
    p_k ← (1-β)·p_k + β·ω        [Leaky average of recent readouts]
    ω = A · v_k                   [Current readout]

    p_k^ref ← p_k                 [Copy to reference when chopper flips]
```

**Why this is brilliant:**
```
The chopper flips direction of gradient accumulation
    ↓
The conductance value BEFORE the flip serves as reference for AFTER
    ↓
Reference tracks the actual device dynamics, not a pre-programmed value
    ↓
Works regardless of device characteristics!
```

**Advantages over c-TTv2 and TTv2:**
1. **No reference array R needed** → Simpler chip design, less area
2. **No differential read needed** → Simpler circuitry
3. **No need to program/estimate symmetry point** → Simpler operation
4. **Works with symmetric OR asymmetric devices** → Broader material choices
5. **Completely invariant to reference errors** → Robust!

**Trade-offs:**
- Requires storing digital matrices P and P^ref (O(N²) memory)
- Slightly more digital compute: O(50N/ns) operations vs O(18N/ns) for c-TTv2
- Still much faster than digital gradient accumulation: O(2N² + N)

## Key Results

### Weight Programming Task (Linear Layer)
Testing: Train 20×20 matrix to match target weights

**Effect of reference offset (σ_r):**
| Algorithm | σ_r = 0 | σ_r = 0.1 | σ_r = 0.5 | σ_r = 1.0 |
|-----------|---------|-----------|-----------|-----------|
| TTv2 | 5% error | 9% error | >25% error | Breaks |
| c-TTv2 | 5% error | 5% error | 7% error | 15% error |
| **AGAD** | **0.8% error** | **0.8% error** | **0.8% error** | **0.8% error** |

**Effect of device symmetry:**
```
TTv2 & c-TTv2:
    Need asymmetric devices
    Symmetric devices → Worse performance

AGAD:
    Works equally well with symmetric OR asymmetric devices
    Opens up more material choices (capacitors, ECRAM)
```

### DNN Training Results

**MNIST (handwritten digits):**
- FP baseline (GPU): 98.07%
- TTv2 (no offset): 98.00%
- TTv2 (with 5% offset): Fails completely
- c-TTv2 (with 25% offset): 98.00%
- **AGAD (any offset): 98.00%** ✅

**CIFAR-10 (color images):**
- FP baseline: 87.51%
- TTv2 (no offset): 86.80%
- TTv2 (with offset): Severe degradation
- **AGAD (any offset): 86.80%** ✅

**LSTM (War & Peace text prediction):**
- Similar patterns - AGAD most robust

**Vision Transformer (CIFAR-10, 4.3M parameters):**
- Tested to verify trends hold for larger networks
- AGAD maintains stable accuracy with offsets
- TTv2 degrades significantly

## Device Requirements Analysis

The paper systematically analyzes what each algorithm needs from device materials:

### Endurance (number of pulses device can handle)
```
Array A (gradient accumulation):
    - Gets pulsed EVERY input sample
    - Needs: 0.5-4 pulses per sample
    - For 1M training samples: 500k-4M pulses total
    - Required endurance: VERY HIGH (10^6 - 10^7)

Array R (reference):
    - Only written once before training
    - Needs: Almost no endurance

Array W (weights):
    - Written ~once per 1000 samples
    - Needs: Moderate endurance (10^3 - 10^4)
```

**Implication:** The gradient accumulation array A is the bottleneck for endurance

### Retention (how long conductance stays stable)
```
Array A:
    TTv2/c-TTv2: Must retain until next readout (Nns samples)
    AGAD: Only needs retention for ~100-1000 samples
        → 1000× less retention needed!

Array R:
    TTv2/c-TTv2: Must retain for ENTIRE training run
    AGAD: Not needed at all

Array W:
    All algorithms: Must retain for entire training run
```

**Implication for AGAD:** Could use high-endurance, low-retention materials (or even volatile capacitors!) for array A

### Symmetry (linearity of response)
```
TTv2/c-TTv2:
    REQUIRE asymmetric devices
    Algorithm relies on devices returning to symmetry point

AGAD:
    Works with symmetric OR asymmetric
    Much broader material choice
```

## Performance Analysis

### Runtime Complexity Per Input Sample

| Algorithm | FP Ops | Memory Ops | Analog Ops | Est. Time |
|-----------|--------|------------|------------|-----------|
| TTv2 | O(2N(1+1/ns)) | O(16N/ns) | (l_avg + 1/ns)t_pulse + t_MVM/ns | 56.3 ns |
| c-TTv2 | O(2N(1+3/ns)) | O(18N/ns) | (l_avg)t_pulse | 56.3 ns |
| **AGAD** | O(2N(1+3/ns)) | O(50N/ns) | (l_avg)t_pulse | **62.1 ns** |
| Mixed-Precision (digital) | O(2N²+N) | O(16N²/B) | N/B t_pulse | **3024.5 ns** |

**For N=512, ns=2, assuming 0.7 TFLOPS throughput:**
- AGAD: ~62 ns per sample
- Digital gradient accumulation: ~3000 ns per sample
- **Speed-up: ~50×**

**With more aggressive settings (ns=10, l_max=1):**
- AGAD: ~17 ns per sample
- **Speed-up: ~175×**

## The Technical Innovation: The Chopper Technique

This is borrowed from analog circuit design (specifically, amplifier offset correction):

```
Problem in analog circuits:
    Amplifiers have DC offsets that accumulate

Solution (1996 technique):
    1. Modulate signal with square wave (chopper)
    2. Amplify
    3. Demodulate with same square wave
    4. Offsets average to zero over time

Application to memristors (this paper):
    1. Flip gradient signs randomly: g → c·g where c ∈ {-1,1}
    2. Accumulate on memristor array A
    3. Flip sign back when reading: output = c·(readout)
    4. Reference errors average to zero!
```

**Mathematical insight:**
```
Without chopper:
    Accumulated value = Σ(gradient + offset)
                      = Σ(gradient) + n·offset
                      → Drift proportional to time

With chopper:
    Accumulated value = Σ(c_i · (gradient + offset))
                      = Σ(c_i · gradient) + Σ(c_i · offset)
                      = Σ(c_i · gradient) + ~0
                      → Offset cancels out!
```

## Why This Paper Matters

### It Makes In-Memory Training Practical

**Before (TTv2):**
```
Theoretical: Beautiful, fast, elegant
Practical: Unusable (requires 2% programming accuracy)
Industry adoption: Blocked
```

**After (AGAD):**
```
Theoretical: Still fast and elegant
Practical: Robust to any errors
Industry adoption: Viable path forward
```

### It Simplifies Hardware Design

**TTv2 requirements:**
- Three crossbar arrays (A, R, W)
- Differential read circuitry
- Precise programming of R before training
- Asymmetric devices only

**AGAD requirements:**
- Two crossbar arrays (A, W)
- Simple direct read
- No pre-programming needed
- Symmetric OR asymmetric devices

**Chip area savings:** ~33% (eliminates R array)
**Circuit complexity:** Significantly reduced

### It Broadens Material Choices

```
Materials that work:
    TTv2: Only asymmetric (ReRAM with limited endurance)
    AGAD: Symmetric OR asymmetric
        → ReRAM
        → ECRAM (high endurance, low noise)
        → Capacitors (infinite endurance, but need refresh)
        → Other emerging materials
```

## Connection to Previous Papers

### vs Paper 1 (TaOx/HfOx memristors, 2017)
| Aspect | Paper 1 | This Paper |
|--------|---------|------------|
| On-chip training? | Yes (first demo) | Yes (improved algorithms) |
| Main contribution | Proof of concept | Practical algorithms |
| Key limitation | Needed very symmetric devices | Handles asymmetric devices |
| Reference handling | Not addressed | **Solved the reference problem** |

### vs Paper 2 (Photonics, 2020)
| Aspect | Paper 2 | This Paper |
|--------|---------|------------|
| Medium | Light (photons) | Electrons (memristors) |
| Training | External | **In-memory** |
| Speed | Fastest (ps) | Fast (ns) |
| Maturity | Early research | Closer to production |

### vs Paper 3 (HfOxSy/HfS2, 2025)
| Aspect | Paper 3 | This Paper |
|--------|---------|------------|
| Focus | Better devices | Better algorithms |
| Training | External (simulated) | **In-memory (actual)** |
| Device innovation | 2D materials | Algorithm handles any devices |
| Key metric | <1% accuracy loss | <1% accuracy loss + fast training |

## The Grand Unified Picture Emerging

```
Paper 1 (2017): "We can train on memristor hardware!"
                Problem: Needs very ideal devices

Paper 2 (2020): "What if we use light instead?"
                Problem: No on-chip training yet

Paper 3 (2025): "We made better memristor devices!"
                Problem: Still doing external training

This Paper (2024): "Here are algorithms that work with real devices!"
                Solution: ✅ On-chip training
                         ✅ Works with non-ideal devices
                         ✅ Fast (O(N) not O(N²))
                         ✅ Robust to errors
                         ✅ Practical for deployment
```

## Practical Implementation Considerations

### Learning Rate Settings
The paper provides specific formulas for setting learning rates dynamically:

```python
# Learning rate for hidden matrix H:
λ_H = (λ · ns · n) / (γ_0 · δ_W · λ_A)

Where:
    λ = SGD learning rate (scheduled)
    ns = readout period
    n = matrix size
    γ_0 = accumulation length hyperparameter
    δ_W = device step size

# Learning rate for array A (dynamic):
λ_A = (η_0 · l_max · δ_A) / (μ_x · μ_d)

Where:
    l_max = maximum pulses per update
    δ_A = device step size for A
    μ_x, μ_d = running averages of input/gradient magnitudes
```

**Key insight:** The learning rates are **automatically normalized** based on recent gradient magnitudes, making hyperparameter tuning easier.

### Memory Requirements

**Per N×N weight matrix:**
```
TTv2:
    A: N² devices (analog)
    R: N² devices (analog)  ← Can eliminate with AGAD
    W: N² devices (analog)
    H: 8N² bytes (digital)
    Total analog: 3N² devices
    Total digital: 8N² bytes

AGAD:
    A: N² devices (analog)
    W: N² devices (analog)
    H: 8N² bytes (digital)
    P: 8N² bytes (digital, if β≠1)
    P^ref: 8N² bytes (digital)
    Total analog: 2N² devices  ← 33% less!
    Total digital: 24N² bytes
```

**Trade-off:** More digital memory, but:
- Digital memory is cheap and dense
- Analog crossbar area is expensive and limited
- Net win for AGAD

## Limitations and Future Work

**Current limitations:**
1. Still focuses on relatively small networks (235K parameters for largest test)
2. Device model is simulation-based (soft-bounds model)
3. No actual hardware demonstration yet (all simulation)
4. Energy estimates are projected, not measured

**Future directions:**
1. Scale to truly large networks (billions of parameters)
2. Hardware implementation on real memristor chips
3. Co-design of devices and algorithms
4. Integration with other optimizers (Adam, etc.)
5. Investigation of training-inference energy trade-offs

## Bottom Line: Why This Is Important

This paper provides **the missing piece** for practical analog in-memory training:

```
The Problem:
    "In-memory training is theoretically 50-175× faster than digital,
     but previous algorithms break with realistic device non-idealities"

The Solution:
    "AGAD algorithm is robust to ANY device errors while maintaining
     the same speed advantage"

The Impact:
    "This moves analog in-memory training from
     'interesting research idea' to 'potentially deployable technology'"
```

**Key achievements:**
1. ✅ Maintains O(N) complexity (not O(N²))
2. ✅ Robust to 25%+ reference errors (vs 2% for TTv2)
3. ✅ Simpler hardware (no reference array needed)
4. ✅ Works with any device material (symmetric or asymmetric)
5. ✅ Demonstrated on multiple DNNs with <1% accuracy loss
6. ✅ ~50-175× faster than digital gradient accumulation

**The paradigm shift:**
```
Old thinking:
    "We need perfect devices to do analog training"

New thinking:
    "We can design algorithms that work with imperfect devices"

Result:
    "Analog in-memory training becomes practically viable"
```

This is the algorithmic breakthrough that makes the hardware advances from Papers 1 and 3 actually usable in practice.
