# 2406.12774v1.pdf

Towards Exact Gradient-based Training on
Analog In-memory Computing
Zhaoxian Wu
Rensselaer Polytechnic Institute
Troy, NY 12180
wuz16@rpi.edu
Tayfun Gokmen
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
tgokmen@us.ibm.com
Malte J. Rasch
IBM T. J. Watson Research Center
Yorktown Heights, NY 10598
malte.rasch@googlemail.com
Tianyi Chen
Rensselaer Polytechnic Institute
Troy, NY 12180
chentianyi19@gmail.com
Abstract
Given the high economic and environmental costs of using large vision or language
models, analog in-memory accelerators present a promising solution for energy-
efficient AI. While inference on analog accelerators has been studied recently,
the training perspective is underexplored. Recent studies have shown that the
"workhorse" of digital AI training - stochastic gradient descent ( SGD) algorithm
converges inexactly when applied to model training on non-ideal devices. This
paper puts forth a theoretical foundation for gradient-based training on analog
devices. We begin by characterizing the non-convergent issue of SGD, which is
caused by the asymmetric updates on the analog devices. We then provide a lower
bound of the asymptotic error to show that there is a fundamental performance limit
of SGD-based analog training rather than an artifact of our analysis. To address this
issue, we study a heuristic analog algorithm called Tiki-Taka that has recently
exhibited superior empirical performance compared to SGD and rigorously show its
ability to exactly converge to a critical point and hence eliminates the asymptotic
error. The simulations verify the correctness of the analyses.
1 Introduction
Large vision or language models have recently achieved great success in various applications.
However, training large models from scratch requires prolonged durations and substantial energy
consumption, which is very costly. For example, it took $2.4 million to train LLaMA [1] and $4.6
million to train GPT-3 [2]. To overcome this issue, a promising technique is application-specific
hardware accelerators for neural networks, such as TPU [3], NPU [4], and NorthPole chip [5], just
to name a few. Within these accelerators, the memory and processing units are physically split,
which requires constant data movements between the memory and processing units. This slows
down computation and limits efficiency. In this context, we focus onanalog in-memory computing
(AIMC) accelerators with resistive crossbar arrays [6–9] to accelerate the ubiquitous matrix-vector
multiplications (MVMs), which contribute to a significant portion of digital computation in model
training and inference, e.g., about 80% in VGG16 model [10]. Very recently, the first analog AI chip
has been fabricated in IBM’s Albany Complex [11], and has achieved an accuracy of 92.81% on the
The work was supported by National Science Foundation (NSF) project 2134168, NSF CAREER project
2047177, and the IBM-Rensselaer Future of Computing Research Collaboration.
Preprint. Under review.
arXiv:2406.12774v1  [cs.LG]  18 Jun 2024

CIFAR10 dataset while enjoying 280× and 100× efficiency and throughput value compared with the
most-recent GPUs, demonstrating the transformative power of analog computing for AI [12].
In AIMC accelerators, the trainable matrix weights are represented by the conductance of the resistive
elements in an analog crossbar array [13, 14]. Unlike standard digital devices such as GPU or TPU,
trainable matrices, input, and output of MVMs in resistive crossbars are all analog signals, which
means that they are effectively continuous physical quantities and are not quantized. In resistive
crossbars, the fundamental physics (mainly Kirchhoff’s and Ohm’s laws) enable the devices to
accelerate MVMs in both forward and backward computations. However, the analog representation
of model weights requires updating weights in their unique way. To this end, an in-memory pulse
update has been developed in [ 15], which changes the weights by sending consecutive pulses to
implement gradient-based training algorithms like stochastic gradient descent (SGD), which reduces
the energy consumption and execution time.
While the AIMC architecture has potential advantages, the analog signal in resistive crossbar devices
is susceptible to noise and other non-ideal device characteristics [16], leading to training performance
that is often suboptimal when compared to digital counterparts. Despite the increasing number of
empirical studies on AIMC that aim to overcome this accuracy drop [17–20], there is still a lack of
theoretical studies on the performance and performance limits of SGD on AIMC accelerators.
1.1 Main results
The focus of this paper is fundamentally different from the vast majority of work in analog computing.
0 500 1000 1500 2000 2500
Iteration k
10 9
10 7
10 5
10 3
10 1
f(Wk) f *
=2e-01
=1e-01
=5e-02
=3e-02
Digital SGD
Analog SGD
Figure 1: Digital/Analog SGD under different learning rates.
We aim to build a rigorous theoretical
foundation of analog training, which
can uniquely characterize the perfor-
mance limit of using a hardware-
agnostic digital SGD algorithm for
analog training and establish the con-
vergence rate of gradient-based ana-
log training. Consider the following
standard model training problem
W ∗ := arg min
W ∈RD
f(W ) (1)
where f(·) : RD → R is the objective
function, W is all the trainable weight stored in an analog way and D is the model size.
In digital training, the workhorse algorithm for solving problem (1) is SGD, which iteratively updates
weight W via the following recursion
Digital SGD Wk+1 = Wk − α(∇f(Wk) + εk) (2)
where k is the iteration index, α is a positive learning rate and εk is the gradient noise with zero
mean at iteration k. In digital training, the noise usually comes from the mini-batch sampling. In
analog training, the noise also arises from the non-ideality of devices, including weight read noise,
input/output noise, quantization noise, digital/analog conversion noise, and even thermal noise [21].
In Figure 1, we first numerically show that the asymptotic performance of SGD running on the analog
devices that we term Analog SGD does not follow that of (2), and thus ask a natural question:
Q1) How to better characterize the training trajectory of SGD on analog devices?
Building upon the pulse update in [ 15, 22], in this paper, we propose the following discrete-time
mathematical model to characterize the trajectory of Analog SGD on analog computing devices
Analog SGD Wk+1 = Wk − α(∇f(Wk) + εk) − α
τ |∇f(Wk) + εk| ⊙ Wk (3)
where | · | and ⊙ represent the coordinate-wise absolute value and multiplication, respectively and τ
is a device-specific parameter. We will explain the underlying rationale of this model(3) in Section
2.2. But before that, compared to (2), the extra term of (3) comes from the asymmetric update of
analog devices. Following this dynamic, we prove that the Analog SGD only converges inexactly to a
critical point, with the asymptotic error depending on the non-ideality of the analog device. Since the
inexact convergence guarantee usually leads to an unfavorable result, it raises another question:
2

Algorithm Rate Asymptotic Error
Digital SGD [23] O
q
σ2
K

0
Analog SGD [Theorem 2] O
q
σ2
K
1
1−W 2max/τ 2

O(σ2SK)
Tiki-Taka [Theorem 4] O
q
σ2
K
1
1−33P 2max/τ 2

0
Lower bound [24] O
q
σ2
K

0
Table 1: Comparison between the convergence of digital and analog training algorithms: K repre-
sents the number of iterations, σ2 is the variance of stochastic gradients, W 2
max/τ 2 and P 2
max/τ 2
measure the saturation degree, and SK measures the non-ideality of analog devices (c.f. Theorem 2).
Asymptotic error refers to the error that does not vanish with K.
Q2) How to mitigate the asymptotic error induced by the asymmetric analog update?
To answer this, we revisit a heuristic algorithmTiki-Taka that has been used among experimentalists
[22], and establish the first exact convergenceresult of Tiki-Taka on a class of AIMC accelerators.
Our contributions. This paper makes the following contributions (see the comparison in Table 1):
C1) We demonstrate that Analog SGD does not follow the dynamic of SGD in (2). Leveraging
the underlying physics, we propose a discrete-time dynamic of the gradient-based algorithm
on analog devices, and show that it better characterizes the trajectory of Analog SGD.
C2) Based on the proposed dynamic, we establish the convergence of Analog SGD and argue
that the performance limit of Analog SGD is a combined effect of data noise and device
asymmetry. We prove the tightness of our result by showing a matching lower bound.
C3) To improve the performance limit of analog training, we study a heuristic algorithm
Tiki-Taka that serves as an alternative to Analog SGD. We show that Tiki-Taka exactly
converges to the critical point by reducing the effect of asymmetric bias and noise.
C4) To verify the validity of our discrete-time dynamic for analog training and the tightness of
our analysis, we provide simulations on both synthetic and real datasets to show that the
asymptotic error of Analog SGD does exist, and Tiki-Taka outperforms Analog SGD.
1.2 Prior art
Since the seminal work on pulse update-based Analog SGD [15], a series of gradient-based training
algorithms have been proposed to enable training on analog devices. Despite its potential energy
and speed advantage, Analog SGD suffers from asymmetric update and noise issues, leading to large
errors in training. To overcome the asymmetric issue, a new algorithm so-termed Tiki-Taka (TT-v1)
introduces an auxiliary array to estimate the moving average of gradients, whose weight is then
transferred to the main array periodically [22] . However, the weight transfer process between arrays
still suffers from noise. To deal with this issue, TT-v2 [25] introduces an extra digital array to filter
out the high-frequency noise. Enabled by these approaches, researchers successfully trained a model
on the realistic prototype analog devices and reached comparable performance on a real dataset [20].
Another major hurdle comes from noisy analog signals, which can perturb the gradient computed
by analog devices. As an alternative, a hybrid scheme that accelerates the forward and backward
pass in-memory and computes gradient in the digital domain has been proposed in [17, 18], which
provides a more accurate but less efficient update. In addition, the successful training by TT-v1 or
TT-v2 relies on the zero-shifting technique [26], which corrects the symmetric point of devices as
zero. However, the correction is inaccurate because it also involves analog signals. To deal with
this issue, Chopped-TTv2 (c-TTv2) and analog gradient accumulation with dynamic reference have
been proposed in [19]. Because of these efforts, analog training has empirically shown great promise
in achieving a similar level of accuracy as digital training, with reduced energy consumption and
training time. Despite its good performance, it is still mysterious about when and why they work.
3

2 The Physics of Analog Training
Unlike digital devices, analog devices represent the trainable weights by the conductance of the base
materials, which have undergone offset and scaling due to physical laws. This difference leads to
entirely different training dynamics on analog devices, which will be discussed in this section.
2.1 Revisit SGD theory and its failure in modeling analog training
This section shows that the convergence theory developed for digital SGD fails to characterize the
analog training. Before that, we introduce standard assumptions for analyzing digital training.
Assumption 1 (L-smoothness). The objective f(W ) is L-smooth, i.e., for any W, W ′ ∈ RD, it holds
∥∇f(W ) − ∇f(W ′)∥ ≤ L∥W − W ′∥. (4)
Assumption 2 (Lower bounded). f(W ) is lower bounded by f ∗, i.e. f(W ) ≥ f ∗, ∀W ∈ RD.
Assumption 3 (Noise mean and variance). The noise εk is independently identically distributed
(i.i.d.) for k = 0, 1, . . . , K, and has zero mean and bounded variance, i.e., E[ε] = 0, E[∥ε∥2] ≤ σ2.
In non-convex optimization, instead of finding a global minimum, it is usually more common to find
a critical point, which refers to W ∗ satisfying ∇f(W ∗) = 0. Under Assumptions 1–3, if Wk follows
the digital SGD dynamic (2), the convergence of SGD is well-understood [23, Theorem 4.8]
1
K
K−1X
k=0
∥∇f(Wk)∥2 ≤ 2(f(W0) − f ∗)
αK + ασ2L. (5)
This result implies that the impact of noise can be controlled by the learning rate α, i.e., after
K ≥ Ω(1/α2) iterations, the error is O(α). By forcing α → 0, the error will reduce to zero.
Analog SGD violates the SGD theory. The dynamic of digital SGD exactly follows (2) up to the
machine’s precision. To verify whether analog training adheres to the same dynamic, we conduct
a numerical simulation on a least-squares problem. We compare Analog SGD implemented by an
analog devices simulator, IBM Analog Hardware Acceleration Kit (AIHWK IT) [27], and digital
SGD implemented by PYTORCH . The same level of noise obeying Gaussian noise is injected
into each algorithm. Beginning from a large learning rate α = 0.2, we reduce the learning rate
by half each time and observe the convergences. As Figure 1 illustrates, digital SGD behaves
as what (5) predicts: it converges with a smaller error when a small learning rate is chosen. On
the contrary, Analog SGD converges with a much larger error, which does not decrease as the
learning rate decreases. This result demonstrates the discrepancy between the theory of digital
SGD and the performance of Analog SGD. More details are deferred to Appendix I.
2.2 Training dynamic on analog devices
Compared to digital devices, the key feature of analog devices is analog signal. The input and
output of analog arrays are analog signals, which are prone to be perturbed by noise, including read
noise and input/output noise [21, 28]. Moreover, real training typically involves the utilization of
mini-batch samples, which also introduces noise. Besides the data noise, another notable feature of
analog accelerators is that the model weight is represented by material conductance.
Pulse update. To change the weights in analog devices, one needs to send an electrical pulse to
the resistive element, and the conductance will change by a small value, which is referred to as
pulse update [15]. To apply an update ∆w to the weight w, using the pulse update needs to send a
series of pulses to the resistive element, the number of which is proportional to the update magnitude
|∆w|. Since the increments responding to each pulse are small typically, we can regard the change in
conductance as a continuous process. Consequently, common operations involving significant weight
changes, like copying the weight, are expensive in analog accelerators. In contrast, gradient-based
algorithms typically update small amounts at each iteration, rendering the pulse update extremely
efficient. Figure 2 presents the weight change on AIMC devices with pulse number.
Asymmetric update. Even though the pulse update is performed efficiently inside AIMC accelerators,
it suffers from a phenomenon that we refer to as asymmetric update. This means that if we apply
4

+ -
number of pulses
weight      (conductance)
+ -
number of pulses
weight      (conductance)
Figure 2: The weight’s change with the number of pulses. Positive and negative pulses are sent
continuously on the left and right half, respectively. Beginning from w, the weight after applying
update ∆w to it is w+ or w− if ∆w ≥ 0 or ∆w < 0, respectively. The response factors q+(w)
and q−(w) are approximately the slope of the curve at w. (Left) Ideal device. q+(w) = q−(w) ≡
1. Every point is symmetric point. (Right) Asymmetric Linear Device (ALD). q+(w) = 1 −
(w − w⋄)/τ, q−(w) = 1 + (w − w⋄)/τ. The symmetric point w⋄ satisfies q+(w⋄) = q−(w⋄).
the change ∆w > 0 and ∆w < 0 on the same weight w, the amount of weight change will be
different. Considering the weight Wk at time k and the expected update ∆W ∈ RD, we express the
asymmetric update as Wk+1 = U(Wk, ∆W ) with the update function U : R × R → R defined by1
U(w, ∆w) :=
w + ∆w · q+(w), ∆w ≥ 0,
w + ∆w · q−(w), ∆w < 0, (6)
where q+(·) and q−(·) : R → R+ are up and down response factors, respectively. The response
factors measure the ideality of analog tiles. In the ideal situation, q+(w) = q−(w) ≡ 1 (see Figure
2, left), and analog algorithms have the same numerical behavior of the digital ones. Defining the
symmetric and asymmetric components as F (w) := 1
2(q−(w) + q+(w)) and G(w) := 1
2(q−(w) −
q+(w)), the update in (6) can be expressed in a more compact form U(w, ∆w) = w + ∆w · F (w) −
|∆w| · G(w). For simplicity, assume that all the coordinates of W use the same update rule, and the
analog update can be written as
Wk+1 = Wk + ∆W ⊙ F (Wk) − |∆W | ⊙ G(Wk) (7)
where | · | and ⊙ represent the coordinate-wise absolute value and multiplication, respectively. Note
that in (7), the ideal weight update ∆W is algorithm-specific: ∆W is the gradient in Analog SGD
while it is an auxiliary weight (c.f. in Section 4) in Tiki-Taka.
Remark 1 (Physical constraint). It is attempting to scale the ∆w by q+(w) or q−(w) to cancel
the effect of asymmetric update in (6) dynamically. However, this is impractical since it is hard to
implement the reading and scaling at the same time on analog tiles [22].
Symmetric point. The asymmetric update makes up and down responses different almost everywhere,
i.e. q+(w) ̸= q−(w) for almost any w. If a point w⋄ satisfies q+(w⋄) = q−(w⋄) and G(w) = 0 ,
w⋄ is called a symmetric point. With loss of generality, the response factor is defined so that
F (w⋄) = 1. Therefore, near the symmetric point, the update ∆w can be accurately applied on w,
i.e., U(w⋄, ∆w) ≈ w⋄ + ∆w. If all the coordinates of matrix Wk hover around the symmetric point,
the analog devices can exhibit performance that resembles the digital ones. In the next section, we
will show that the weight is biased toward its symmetric point.
Asymmetric linear device. Although our unified formulation (7) can capture the response behaviors
of different materials, this paper mainly focuses on the behaviors of the asymmetric linear device
(ALD), similar to the setting in [19]. ALD has a positive parameter τ which reflects the degree of
asymmetry and its response factors are written as linear functionsq+(w) = 1−(w − w⋄)/τ, q−(w) =
1 + (w − w⋄)/τ. Consequently, ALD has F (w) = 1, G(w) = (w − w⋄)/τ, and symmetric point
1This paper adopts w to represent the element of the weight matrix Wk without specifying its index. This
makes the formulations more concise and uses the fact that analog devices update all coordinates in parallel.
The notation U(Wk, ∆W ) on matrices Wk and ∆W denote the coordinate-wise operation on Wk and ∆W , i.e.
[U(Wk, ∆W )]i := U([Wk]i, [∆w]i), ∀i ∈ I .
5

w⋄; see Figure 2, right. Even though ALD is a simplified device model, it is representative enough to
reveal the key properties and difficulties of gradient-based training algorithms on analog devices. If
not otherwise specified, w⋄ is always 0 for simplicity. In summary, the update of ALD is expressed as
wk+1 = wk + ∆w − 1
τ |∆w| · wk or Wk+1 = Wk + ∆W − 1
τ |∆W | ⊙ Wk (8)
where the first equation is the update of one coordinate while the second one stacks all the elements
together. Replacing ∆W with noisy gradient α(∇f(Wk) + εk) reaches the dynamic (3).
2.3 Saturation, fast reset, and bounded weight
Based on the ALD dynamic (8), we study the properties of analog training, which serve as the
stepping stone of our analyses. Recall that the asymmetric update leads to different magnitudes of
increase and decrease. The following lemma characterizes the difference between two directions.
Lemma 1 (Saturation and fast reset). For ALD with a general w⋄, the following statements are valid
(Saturation) If sign(∆w) = sign(wk), it holds that |wk+1 − wk| = |1 − |wk − w⋄|/τ | · |∆w|.
(Fast Reset) If sign(∆w) = −sign(wk), it holds that |wk+1 − wk| = |1 + |wk − w⋄|/τ | · |∆w|.
The proof is deferred to Section E.1. Remarkably, Lemma 1 holds for any algorithm with any update
∆w, and it reveals the impact of the asymmetric update. In principle, Lemma 1 can be written as
|wk+1 − wk| = |1 ± |wk − w⋄|/τ | · |∆w|, where the symmetric point w⋄ = 0 is omitted.
When wk is not at the symmetric point w⋄ = 0, the update is scaled by a factor. When wk lies around
the symmetric point, |wk+1 − wk| ≈ | ∆w|, where all updates are applied to the weight and hence
the analog devices closely mimic the performance of digital devices. When wk moves away from w⋄,
i.e. sign(∆w) = sign(wk), the update becomes none. When wk gets closer to ±τ, nearly no update
can be applied. This phenomenon is called saturation; see also [22]. On the contrary, wk changes
faster when it moves toward w⋄, which is referred to as fast reset.
Because of the saturation property, the weight on the analog devices is intrinsically bounded, which
will be helpful in the later analysis. The following theorem discusses the bounded property.
Theorem 1 (Bounded weight). Denote ∥W ∥∞ as the ℓ∞ norm of W . Given ∥W0∥∞ ≤ τ and for
any sequence {∆Wk : k ∈ N}, which satisfies ∥∆Wk∥∞ ≤ τ, it holds that ∥Wk∥∞ ≤ τ, ∀k ∈ N.
The proof is deferred to Section E.2. Theorem 1 claims that the weight is guaranteed to be bounded,
even without explicit projection, which makes analog training different from its digital counterpart.
Similar to Lemma 1, Theorem 1 does not depend on any specific analog training algorithm.
3 Performance Limits of Analog Stochastic Gradient Descent
After modeling the dynamic of analog training, we next discuss the convergence of Analog SGD. As
shown by Lemma 1, the update is slow near the boundary of the active region. The weight is expected
to stay within a smaller region to avoid saturation, which necessitates the following assumption.
Assumption 4 (Bounded saturation). There exists a positive constant Wmax < τ such that the weight
Wk is bounded in ℓ∞ norm, i.e., ∥Wk∥∞ ≤ Wmax. The ratio Wmax/τ is the saturation degree.
Assumption 4 requires that Wk is bounded inside a small region, which is a mild assumption in
real training. For example, one can apply a clipping operation on wk to ensure the assumption. In
Appendix E.2, we show that Assumption 4 provably holds under the strongly convex assumption.
It is worth pointing out that Assumption 4 implicitly assumes there are critical points in the box
{W : ∥W ∥∞ ≤ τ }. Otherwise, the gradient will push the weight to the bound of the box and it
becomes possible that Wmax < ∥Wk∥∞ ≤ τ.
Intuitively, without the asymmetric bias, the weight is stable near the critical point. In contrast,
with asymmetric bias, the noisy gradient that pushes wk toward its symmetric point is amplified
by fast reset, while the one that drags wk away from 0 is suppressed by saturation (c.f. Lemma 1).
Consequently, the weight wk is attracted by 0, which prevents the stability of Analog SGD around
the critical point. We characterize the convergence of Analog SGD in the following theorem.
6

Theorem 2 (Convergence of Analog SGD). Under Assumption 1-4, if the learning rate is set as
α =
q
f(W0)−f ∗
σ2LK and K is sufficiently large such that α ≤ 1
L, it holds that
1
K
K−1X
k=0
E[∥∇f(Wk)∥2] ≤ O
 r
(f(W0) − f ∗)σ2L
K
1
1 − W 2max/τ 2
!
+ 4σ2SK (9)
where SK denotes the amplification factor given by SK := 1
K
PK
k=0
∥Wk∥2
∞/τ 2
1−∥Wk∥2∞/τ 2 ≤ W 2
max/τ 2
1−W 2max/τ 2 .
The proof of Theorem 2 is deferred to Appendix G.1. Theorem 2 suggests that the average squared
gradient norm is upper bounded by the sum of two terms: the first term vanishes at a rate of
O(
p
σ2/K) which also appears in the SGD’s convergence bound(5); the second term contributes to
the asymptotic error of Analog SGD, which does not vanish with the total number of iterations K;
that is, lim supK→∞
1
K
PK−1
k=0 E[∥∇f(Wk)∥2] ≤ 4σ2S∞ exist.
Impact of saturation/asymmetric update. The saturation degree Wmax/τ affects both con-
vergence rate and asymptotic error. The ratio is small if Wk remains close to the symmetric
point or τ is sufficiently large. The exact expression of SK depends on the specific noise dis-
tribution, and thus is difficult to reach. However, SK reflects the saturation degree near the
critical point W ∗ when Wk converges to a neighborhood of W ∗. Intuitively, Wk ≈ W ∗ implies
SK ≈ ∥W ∗∥2
∞/τ 2
1−∥W ∗∥2∞/τ 2 . Therefore, if the critical point is near the symmetric point, the asymptotic
error SK could be small. The asymmetric update has a negative impact on both rate and error. It
slows down the convergence of SGD by a factor 1/(1 − W 2
max/τ 2), and, meanwhile, a smaller τ
increases the asymptotic error.
To demonstrate the asymptotic error in Theorem 2 is not artificial, we provide a lower bound next.
Theorem 3 (Lower bound of the error of Analog SGD). There is an instance which satisfies Assump-
tion 1-4 such that Analog SGD generates a sequence {Wk : k = 0, 1, · · · , K} which satisfies
1
K
K−1X
k=0
E[∥∇f(Wk)∥2] = 4σ2SK + Θ(α)
α=Θ

1√
K

= Ω

4σ2SK + 1√
K

. (10)
The proof of Theorem 3 is deferred to Appendix G.2. Theorem 3 implies that4σ2SK in the right-hand
side (RHS) of (9) can not be improved and therefore the lower bound of the asymptotic error. It
demonstrates that the presence of asymptotic error is intrinsic and not an artifact of the convergence
analysis. The nonzero asymptotic error also reveals the fundamental performance limits of using
Analog SGD for analog training, pointing out a new venue for algorithmic development.
4 Eliminating Asymptotic Error of Analog Training: Tiki-Taka
Building upon our understanding on the modeling of gradient-based analog training in Section 2 and
the asymptotic error of Analog SGD in Section 3, this section will be devoted to understanding means
to overcome such asymptotic error in analog training.
We will focus on our study on a heuristic algorithm Tiki-Taka that has shown great promise in
practice [22]. The key idea of Tiki-Taka is to maintain an auxiliary array to estimate the true
gradient. To be specific, Tiki-Taka introduces another analog device, Pk, besides the main one, Wk.
At the initialization phase, Pk is initialized as 0. At iteration k, the stochastic gradient is computed
using the main device Wk and is first applied to Pk. Since Pk is also an analog device, the change of
Pk still follows the dynamic (8) by replacing Wk with Pk and ∆W with ∇f(Wk) + εk, that is
Pk+1 = Pk + β(∇f(Wk) + εk) − β
τ |∇f(Wk) + εk| ⊙ Pk (TT-P)
where β is a learning rate. After that, the value Pk+1 is read and transferred to the main array Wk via
Wk+1 = Wk − αPk+1 − α
τ |Pk+1| ⊙ Wk. (TT-W)
7

Tiki-Taka performs recursion (TT-P) and (TT-W) alternatively until it converges. Empirically,
Tiki-Taka outperforms Analog SGD in terms of the final accuracy. However, Tiki-Taka is a
heuristic algorithm, and there are no convergence guarantees so far. In this section, we demonstrate
that the improvement of Tiki-Taka stems from its ability to eliminate asymptotic errors.
Stability of Tiki-Taka. As explained in Section 3, the gradient noise contributes to the asymp-
totic error of Analog SGD. To eliminate the error, the idea under Tiki-Taka is to reduce the
noise impact. To see how Tiki-Taka reduces the noise, consider the case where Wk is already a
critical point and Pk is initialized as 0, i.e. ∇f(Wk) = Pk = 0. After one iteration, the weight
Wk drifts because of the noise. For Analog SGD, the expected drift is
E[Wk+1] − Wk = −αE[εk] − α
τ E[|εk|] ⊙ Wk = − α
τ E[|εk|] ⊙ Wk ∝ α. (11)
In contrast, Tiki-Taka updates the auxiliary array by Pk+1 = Pk + βεk − β
τ |εk| ⊙ Pk = βεk,
which implies E[Pk+1] = 0 and E[|Pk+1|] = βE[|εk|]. After the transfer, its expected drift is
E[Wk+1] − Wk = −αE[Pk+1] − α
τ E[|Pk+1|] ⊙ Wk = − αβ
τ E[|εk|] ⊙ Wk ∝ αβ. (12)
Comparing (12) with (11), it can be observed that Tiki-Taka improves the expected drift from O(α)
to O(αβ). With sufficiently small β, Tiki-Taka controls the drift and makes the weight stay at the
critical point. For a more generic scenarios, Pk ̸= 0. However, it is worth noting that
E[Pk+1] = E[Pk + βεk − β
τ |εk| ⊙ Pk] = (1 − β
τ E[|εk|]) ⊙ Pk (13)
which propels Pk back to 0 when E[|εk|] ̸= 0. By controlling the drift, Tiki-Taka manages to stay
near a critical point.
Note that from (13), the stability of Pk relies on the presence of noise, i.e. E[|εk|] ̸= 0. In addition to
the upper bound on the noise in Assumption 3, a lower bound for the noise is also assumed.
Assumption 5 (Coordinate-wise i.i.d. and non-zero noise). For any k ≥ 0 and i, j ∈ I , [εk]i and
[εk]j are i.i.d. from a distribution Dc which ensures E[εk]i∼Dc[εk] = 0. Furthermore, there exists
σ > 0 and c > 0 such that E[εk]i∼Dc[[εk]2
i ] ≤ σ2/D and E[εk]i∼Dc[|g + [εk]i|] ≥ cσ for any g ∈ R.
Intuitively, Assumption 5 requires the non-zero noise, which is mild since the random sampling and
the physical properties of analog devices always introduce noise. The factor D in the denominator
makes it consistent with Assumption 3. We discuss this assumption in more detail in Appendix D.2.
Theorem 4 (Convergence of Tiki-Taka). Suppose Assumption 1-5 hold and the learning rate is set
as α = O(1/
√
σ2K), β = 8αL. It holds for Tiki-Taka that the expected infinity norm Pk is upper
bounded by E[∥Pk+1∥2
∞] ≤ P 2
max := 41L2τ 4D
c2σ2 . Furthermore, if σ2 and D are sufficiently large so
that 33P 2
max/τ 2 < 1 it is valid that
1
K
K−1X
k=0
E[∥∇f(Wk)∥2] ≤ O
 r
(f(W0) − f ∗)σ2L
K
1
1 − 33P 2max/τ 2
!
. (14)
The proof of Theorem 4 is deferred to Appendix H. Theorem 4 first provides the upper bound for
the maximum magnitude of Pk that decreases as the variance σ2 increases or τ decreases. This
observation is consistent with (13), which implies the Pk tends to zero when β
τ E[|εk|] ̸= 0. Ensuring
stability during the training requires the noise to be sufficiently large to render the saturation degree of
Pmax/τ sufficiently small. In addition, the condition33P 2
max/τ 2 < 1 requires the D to be sufficiently
large, which is easy to meet when training large models.
Convergence rate. Theorem 4 claims that Tiki-Taka converges at the rate O(
q
σ2L
K
1
1−33P 2max/τ 2 ).
Therefore, we reach the conclusion that lim supK→inf
1
K
PK−1
k=0 E[∥∇f(Wk)∥2] = 0 and
Tiki-Taka eliminates the asymptotic error of Analog SGD. Furthermore, Tiki-Taka improves
the factor 1/(1 − W 2
max/τ 2) in Analog SGD’s convergence (c.f.(9)) to 1/(1 − 33P 2
max/τ 2), wherein
W 2
max/τ 2 and P 2
max/τ 2 are the saturation degrees in fact. Notice that Pk tends to 0 as indicated
8

0 1000 2000
Gradient Computation (k)
10 4
10 3
10 2
f(Wk) f *
= 1.0
Digital SGD Dynamic Analog SGD Dynamic Analog SGD Simulation
0 1000 2000
Gradient Computation (k)
= 1.5
0 1000 2000
Gradient Computation (k)
= 2.0
0 1000 2000
Gradient Computation (k)
= 2.5
Figure 3: The convergence of digital SGD dynamic (2), analog dynamic (3) (proposed) and
Analog SGD implemented by AIHWK IT (real behavior) under different τ.
by (13) while Wmax does not because 0 is usually not a critical point. Therefore, it usually has
P 2
max/τ 2 ≪ W 2
max/τ 2 in practice, implying faster convergence. The convergence matches the lower
bound O(
p
σ2L/K) for general stochastic non-convex smooth optimization [24] up to a constant.
5 Numerical Simulations
In this section, we verify the main theoretical results by simulations on both synthetic datasets and real
datasets. We use the PYTORCH to generate the curves for SGD in the simulation and use open source
toolkit AIHWK IT [27] to simulate the behaviors of Analog SGD; see github.com/IBM/aihwkit.
Each simulation is repeated three times, and the mean and standard deviation are reported. More
details can be referred to in Appendix I.
0 200 400
Number of Gradient Computation (k)
10 5
10 4
10 3
10 2
10 1
f(Wk) f *
 = 2
= 3
= 5
= 10
= 20
= 50
SGD
0 50 100 150 200
Number of Gradient Computation (k)
10 2
10 1
f(Wk) f *
2 = 0.001
2 = 0.01
2 = 0.1
2 = 1.0
0 10 20 30 40 50
Number of Gradient Computation (k)
10 2
100
102
104
f(Wk) f *
Figure 4: (Left) The convergence of Analog SGD under different τ. Reducing τ leads to a decrease
in asymptotic error. When τ is sufficiently large, Analog SGD tends to have a similar performance to
digital SGD. (Middle) The convergence of Analog SGD on noise devices under different σ2. (Right)
Analog SGDs that are initialized to different places converge to the same error.
5.1 Verification of the analog training dynamic
To verify that the proposed dynamic (3) characterizes analog training better than SGD dynamic (2),
we conduct a numerical simulation on a least-squares task, and compare Analog SGD implemented
by AIHWK IT, the digital and analog dynamics given by (2) and (3), respectively; see Figure 3. The
results show that the proposed dynamic provides an accurate approximation of Analog SGD.
5.2 Ablation study on the asymptotic training error
We verify some critical claims about the asymptotic error of Analog SGD on a least-squares task.
Impact of τ. To verify Theorem 2 that the error diminishes with a larger τ, we assign a range of
value τ and plot the convergence ofAnalog SGD. The result is reported on the Left of Figure 4. When
τ is small, the asymmetric bias introduces a notable gap between Analog and Digital SGD. As τ
increases, the gap diminishes. The result demonstrates the asymptotic error decreases as τ increases.
Impact of σ2. To verify the asymptotic error is proportional to σ2, we inject noise with different
variances. The result is reported in the middle of Figure 4. The result illustrates that the asymptotic
error increases as the noise increases.
9

0 20 40 60 80
Epoch
20
40
60
80
100T est Accuracy
= 0.7
= 0.78
= 0.8
Analog SGD
Tiki-T aka
Digital SGD
78 80
97.0
97.5
0 20 40 60 80
Epoch
20
40
60
80
100T est Accuracy
= 0.6
= 0.7
= 0.8
Analog SGD
Tiki-T aka
Digital SGD
78 80
98.75
99.00
99.25
τ = 0.70 τ = 0.78 τ = 0.80 τ = 0.6 τ = 0.7 τ = 0.8
D SGD 98.15 ± 0.04 99.16 ± 0.08
A SGD 11.35 ± 0.00 97.71 ± 0.63 97.78 ± 0.36 88.99 ± 0.89 92.15 ± 0.48 93.45 ± 0.46
TT 97.64 ± 0.17 97.62 ± 0.05 97.50 ± 0.11 98.84 ± 0.07 98.87 ± 0.04 98.86 ± 0.07
Figure 5: The test accuracy curves and tables for the model training. “D SGD", “A SGD”, and “TT”
represent Digital SGD, Analog SGD and Tiki-Taka, respectively; (Left) FCN. (Right) CNN.
Resnet18 Resnet34 Resnet50
Digital SGD 93.03 93.44 95.92
Analog SGD 93.58 93.58 95.51
Tiki-Taka 93.74 95.15 95.54
Table 2: The test accuracy of Resnet training on CI-
FAR10 dataset after 100 epochs.
Impact of the initialization . To demon-
strate the asymptotic error is not artificial,
we perform Analog SGD from different ini-
tializations. The result is reported on the
right of Figure 4. The result illustrates that
Analog SGD converges to a similar loca-
tion regardless of the initialization. The
smooth convergence curve ensures the error comes from bias instead of limited machine precision.
Therefore, the asymptotic error is intrinsic and independent of the initialization.
5.3 Analog training performance on real dataset
We also train vision models to perform image classification tasks on real datasets.
MNIST FCN/CNN. We train Fully-connected network (FCN) and convolution neural network (CNN)
models on MNIST dataset and see the performance of Analog SGD and Tiki-Taka under various τ.
The results are reported in Figure 5. By reducing the variance, Tiki-Taka outperforms Analog SGD
and reaches comparable accuracy with digital SGD. On both of the architectures, the accuracy of
Tiki-Taka drops by < 1%. In the FCN training, Analog SGD achieves acceptable accuracy on
τ = 0 .78 and τ = 0 .80 but converges much more slowly. In the CNN training, the accuracy of
Analog SGD always drops by > 6%.
CIFAR10 Resnet. We also train three Resnet models with different sizes on CIFAR10 dataset.
The last layer is replaced by a fully-connected layer mapped onto an analog device with parameter
τ = 0.8. The results are shown in Table 2. In this task, Analog SGD does not suffer from significant
accuracy drop but is still worth that Tiki-Taka, A surprising observation for analog training is that
both Analog SGD and Tiki-Taka outperform Digital SGD. We conjecture it happens because the
noise introduced by analog devices makes the network more robust to outlier data.
6 Conclusions and Limitations
This paper points out that Analog SGD does not follow the dynamic of digital SGD and hence, we
propose a better dynamic to formulate the analog training. Based on this dynamic, we studies the
convergence of two gradient-based analog training algorithms, Analog SGD and Tiki-Taka. The
theoretical results demonstrate that Analog SGD suffers from asymptotic error, which comes from
the noise and asymmetric update. To overcome this issue, we show that Tiki-Taka is able to stay in
the critical point without suffering from an asymptotic error. Numerical simulations demonstrate the
existence of Analog SGD’s asymptotic error and the efficacy ofTiki-Taka. One limitation of this
work is that the current analysis is device-specific that applies to asymmetric linear device. While it
is an interesting and popular analog device, it is also important to extend our convergence analysis to
more general analog devices and develop other device-specific analog algorithms in future work.
10

References
[1] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timo-
thée Lacroix, Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. Llama: Open
and efficient foundation language models. arXiv preprint arXiv:2302.13971, 2023.
[2] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal,
Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are
few-shot learners. Advances in neural information processing systems, 33:1877–1901, 2020.
[3] Norm Jouppi, George Kurian, Sheng Li, Peter Ma, Rahul Nagarajan, Lifeng Nai, Nishant
Patil, Suvinay Subramanian, Andy Swing, Brian Towles, et al. TPU v4: An optically reconfig-
urable supercomputer for machine learning with hardware support for embeddings. In Annual
International Symposium on Computer Architecture, pages 1–14, 2023.
[4] Hadi Esmaeilzadeh, Adrian Sampson, Luis Ceze, and Doug Burger. Neural acceleration
for general-purpose approximate programs. In 2012 45th annual IEEE/ACM international
symposium on microarchitecture, pages 449–460. IEEE, 2012.
[5] Dharmendra S Modha, Filipp Akopyan, Alexander Andreopoulos, Rathinakumar Appuswamy,
John V Arthur, Andrew S Cassidy, Pallab Datta, Michael V DeBole, Steven K Esser, Car-
los Ortega Otero, et al. Neural inference at the frontier of energy, space, and time. Science,
382(6668):329–335, 2023.
[6] An Chen. A comprehensive crossbar array model with solutions for line resistance and nonlinear
device characteristics. IEEE Transactions on Electron Devices, 60(4):1318–1326, 2013.
[7] Abu Sebastian, Manuel Le Gallo, Riduan Khaddam-Aljameh, and Evangelos Eleftheriou.
Memory devices and applications for in-memory computing. Nature Nanotechnology, 15:529–
544, 2020.
[8] Wilfried Haensch, Tayfun Gokmen, and Ruchir Puri. The next generation of deep learning
hardware: Analog computing. Proceedings of the IEEE, 107(1):108–122, 2019.
[9] Vivienne Sze, Yu-Hsin Chen, Tien-Ju Yang, and Joel S Emer. Efficient processing of deep
neural networks: A tutorial and survey. Proceedings of the IEEE, 105(12):2295–2329, 2017.
[10] Zihan Zhang, Jianfei Jiang, Yongxin Zhu, Qin Wang, Zhigang Mao, and Naifeng Jing. A
universal RRAM-based DNN accelerator with programmable crossbars beyond MVM operator.
IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 41(7):2094–
2106, 2021.
[11] Abu Sebastian, Manuel Le Gallo, and Vijay Narayanan. IBM research’s latest
analog AI chip for deep learning inference. https://research.ibm.com/blog/
analog-ai-chip-inference , 2023.
[12] Stefano Ambrogio, Pritish Narayanan, Hsinyu Tsai, Robert M Shelby, Irem Boybat, Carmelo
Nolfo, Severin Sidler, Massimo Giordano, Martina Bodini, Nathan CP Farinha, et al. Equivalent-
accuracy accelerated neural-network training using analogue memory. Nature, 558(7708):60,
2018.
[13] Geoffrey W Burr, Robert M Shelby, Abu Sebastian, Sangbum Kim, Seyoung Kim, Severin Sidler,
Kumar Virwani, Masatoshi Ishii, Pritish Narayanan, Alessandro Fumarola, et al. Neuromorphic
computing using non-volatile memory. Advances in Physics: X, 2(1):89–124, 2017.
[14] J Joshua Yang, Dmitri B Strukov, and Duncan R Stewart. Memristive devices for computing.
Nature nanotechnology, 8(1):13, 2013.
[15] Tayfun Gokmen and Yurii Vlasov. Acceleration of deep neural network training with resistive
cross-point devices: Design considerations. Frontiers in neuroscience, 10:333, 2016.
[16] Shubham Jain et al. Neural network accelerator design with resistive crossbars: Opportunities
and challenges. IBM Journal of Research and Development, 63(6):10–1, 2019.
11

[17] S. R. Nandakumar, Manuel Le Gallo, Irem Boybat, Bipin Rajendran, Abu Sebastian, and
Evangelos Eleftheriou. Mixed-precision architecture based on computational memory for
training deep neural networks. In IEEE International Symposium on Circuits and Systems ,
pages 1–5, 2018.
[18] S. R. Nandakumar, Manuel Le Gallo, Christophe Piveteau, Vinay Joshi, Giovanni Mariani, Irem
Boybat, Geethan Karunaratne, Riduan Khaddam-Aljameh, Urs Egger, Anastasios Petropoulos,
Theodore Antonakopoulos, Bipin Rajendran, Abu Sebastian, and Evangelos Eleftheriou. Mixed-
precision deep learning based on computational memory. Frontiers in Neuroscience, 14, 2020.
[19] Malte J Rasch, Fabio Carta, Omebayode Fagbohungbe, and Tayfun Gokmen. Fast offset
corrected in-memory training. arXiv preprint arXiv:2303.04721, 2023.
[20] Nanbo Gong, Malte Rasch, Soon-Cheon Seo, Arthur Gasasira, Paul Solomon, Valeria Bragaglia,
Steven Consiglio, Hisashi Higuchi, Chanro Park, Kevin Brew, et al. Deep learning acceleration
in 14nm CMOS compatible ReRAM array: device, material and algorithm co-optimization. In
IEEE International Electron Devices Meeting, 2022.
[21] Sapan Agarwal, Steven J Plimpton, David R Hughart, Alexander H Hsia, Isaac Richter,
Jonathan A Cox, Conrad D James, and Matthew J Marinella. Resistive memory device require-
ments for a neural algorithm accelerator. In International Joint Conference on Neural Networks,
pages 929–938. IEEE, 2016.
[22] Tayfun Gokmen and Wilfried Haensch. Algorithm for training neural networks on resistive
device arrays. Frontiers in Neuroscience, 14, 2020.
[23] Léon Bottou, Frank E Curtis, and Jorge Nocedal. Optimization methods for large-scale machine
learning. SIAM review, 60(2):223–311, 2018.
[24] Yossi Arjevani, Yair Carmon, John C Duchi, Dylan J Foster, Nathan Srebro, and Blake Wood-
worth. Lower bounds for non-convex stochastic optimization. Mathematical Programming,
199(1-2):165–214, 2023.
[25] Tayfun Gokmen. Enabling training of neural networks on noisy hardware. Frontiers in Artificial
Intelligence, 4:1–14, 2021.
[26] Murat Onen, Tayfun Gokmen, Teodor K Todorov, Tomasz Nowicki, Jesús A Del Alamo,
John Rozen, Wilfried Haensch, and Seyoung Kim. Neural network training with asymmetric
crosspoint elements. Frontiers in artificial intelligence, 5, 2022.
[27] Malte J Rasch, Diego Moreda, Tayfun Gokmen, Manuel Le Gallo, Fabio Carta, Cindy Goldberg,
Kaoutar El Maghraoui, Abu Sebastian, and Vijay Narayanan. A flexible and fast PyTorch toolkit
for simulating training and inference on analog crossbar arrays. IEEE International Conference
on Artificial Intelligence Circuits and Systems, pages 1–4, 2021.
[28] Malte J Rasch, Charles Mackin, Manuel Le Gallo, An Chen, Andrea Fasoli, Frédéric Odermatt,
Ning Li, SR Nandakumar, Pritish Narayanan, Hsinyu Tsai, et al. Hardware-aware training for
large-scale and diverse deep learning inference workloads using in-memory computing-based
accelerators. Nature Communications, 14(1):5282, 2023.
[29] Weier Wan, Rajkumar Kubendran, Clemens Schaefer, Sukru Burc Eryilmaz, Wenqiang Zhang,
Dabin Wu, Stephen Deiss, Priyanka Raina, He Qian, Bin Gao, et al. A compute-in-memory
chip based on resistive random-access memory. Nature, 608(7923):504–512, 2022.
[30] Riduan Khaddam-Aljameh, Milos Stanisavljevic, J Fornt Mas, Geethan Karunaratne, Matthias
Braendli, Femg Liu, Abhairaj Singh, Silvia M Müller, Urs Egger, Anastasios Petropoulos, et al.
HERMES core–a 14nm CMOS and PCM-based in-memory compute core using an array of
300ps/LSB linearized CCO-based adcs and local digital processing. In 2021 Symposium on
VLSI Circuits, pages 1–2. IEEE, 2021.
[31] Chengxin Xue, Yencheng Chiu, Tawei Liu, Tsungyuan Huang, Jesyu Liu, Tingwei Chang,
Huiyao Kao, Jinghong Wang, Shihying Wei, Chunying Lee, et al. A CMOS-integrated compute-
in-memory macro based on resistive random-access memory for AI edge devices. Nature
Electronics, 4(1):81–90, 2021.
12

[32] Laura Fick, Skylar Skrzyniarz, Malav Parikh, Michael B Henry, and David Fick. Analog matrix
processor for edge AI real-time video analytics. In IEEE International Solid-State Circuits
Conference, volume 65, pages 260–262. IEEE, 2022.
[33] Pritish Narayanan, Stefano Ambrogio, A Okazaki, Kohji Hosokawa, Hsinyu Tsai, A Nomura,
T Yasuda, C Mackin, SC Lewis, A Friz, et al. Fully on-chip MAC at 14nm enabled by accurate
row-wise programming of PCM-based weights and parallel vector-transport in duration-format.
In Symposium on VLSI Technology, pages 1–2. IEEE, 2021.
[34] Peng Yao, Huaqiang Wu, Bin Gao, Jianshi Tang, Qingtian Zhang, Wenqiang Zhang, J Joshua
Yang, and He Qian. Fully hardware-implemented memristor convolutional neural network.
Nature, 577(7792):641–646, 2020.
[35] Benjamin Scellier, Maxence Ernoult, Jack Kendall, and Suhas Kumar. Energy-based learning
algorithms for analog computing: a comparative study. In Advances in Neural Information
Processing Systems, 2023.
[36] Hadjer Benmeziane, Corey Lammie, Irem Boybat, Malte Rasch, Manuel Le Gallo, Hsinyu Tsai,
Ramachandran Muralidhar, Smail Niar, Ouarnoughi Hamza, Vijay Narayanan, et al. Analognas:
A neural network design framework for accurate inference with analog in-memory computing.
In IEEE International Conference on Edge Computing and Communications, pages 233–244.
IEEE, 2023.
[37] Sebastian U Stich. Unified optimal analysis of the (stochastic) gradient method. arXiv preprint
arXiv:1907.04232, 2019.
[38] Ahmed Khaled and Peter Richtárik. Better theory for SGD in the nonconvex world.Transactions
on Machine Learning Research, 2022.
[39] Yury Demidovich, Grigory Malinovsky, Igor Sokolov, and Peter Richtárik. A guide through the
zoo of biased sgd. arXiv preprint arXiv:2305.16296, 2023.
[40] Tianbao Yang, Qihang Lin, and Zhe Li. Unified convergence analysis of stochastic momentum
methods for convex and non-convex optimization. arXiv preprint arXiv:1604.03257, 2016.
[41] Yanli Liu, Yuan Gao, and Wotao Yin. An improved analysis of stochastic gradient descent with
momentum. Advances in Neural Information Processing Systems, 33:18261–18271, 2020.
[42] Kun Yuan, Bicheng Ying, and Ali H Sayed. On the influence of momentum acceleration on
online learning. The Journal of Machine Learning Research, 17(1):6602–6667, 2016.
[43] Vien Mai and Mikael Johansson. Convergence of a stochastic gradient method with momentum
for non-smooth non-convex optimization. In International conference on machine learning,
pages 6630–6639. PMLR, 2020.
[44] Nanbo Gong, T Idé, S Kim, Irem Boybat, Abu Sebastian, V Narayanan, and Takashi Ando.
Signal and noise extraction from analog memory elements for neuromorphic computing. Nature
communications, 9(1):2102, 2018.
[45] Geoffrey W. Burr, Matthew J. BrightSky, Abu Sebastian, Huai-Yu Cheng, Jau-Yi Wu, Sangbum
Kim, Norma E. Sosa, Nikolaos Papandreou, Hsiang-Lan Lung, Haralampos Pozidis, Evangelos
Eleftheriou, and Chung H. Lam. Recent Progress in Phase-Change Memory Technology. IEEE
Journal on Emerging and Selected Topics in Circuits and Systems, 6(2):146–162, 2016.
[46] Manuel Le Gallo and Abu Sebastian. An overview of phase-change memory device physics.
Journal of Physics D: Applied Physics, 53(21):213002, 2020.
[47] Jun-Woo Jang, Sangsu Park, Geoffrey W. Burr, Hyunsang Hwang, and Yoon-Ha Jeong. Opti-
mization of conductance change in Pr1−xCaxMnO3-based synaptic devices for neuromorphic
systems. IEEE Electron Device Letters, 36(5):457–459, 2015.
[48] Jun-Woo Jang, Sangsu Park, Yoon-Ha Jeong, and Hyunsang Hwang. ReRAM-based synaptic
device for neuromorphic computing. In IEEE International Symposium on Circuits and Systems,
pages 1054–1057, 2014.
13

[49] Seokjae Lim, Myounghoon Kwak, and Hyunsang Hwang. Improved synaptic behavior of
CBRAM using internal voltage divider for neuromorphic systems. IEEE Transactions on
Electron Devices, 65(9):3976–3981, 2018.
[50] Elliot J Fuller, Scott T Keene, Armantas Melianas, Zhongrui Wang, Sapan Agarwal, Yiyang
Li, Yaakov Tuchman, Conrad D James, Matthew J Marinella, J Joshua Yang, Alberto Salleo,
and A Alec Talin. Parallel programming of an ionic floating-gate memory array for scalable
neuromorphic computing. Science, 364(6440):570–574, 2019.
[51] Murat Onen, Nicolas Emond, Baoming Wang, Difei Zhang, Frances M Ross, Ju Li, Bilge Yildiz,
and Jesús A Del Alamo. Nanosecond protonic programmable resistors for analog deep learning.
Science, 377(6605):539–543, 2022.
[52] Yulong Li, Seyoung Kim, Xiao Sun, Paul Solomon, Tayfun Gokmen, Hsinyu Tsai, Siyu
Koswatta, Zhibin Ren, Renee Mo, Chun Chen Yeh, et al. Capacitor-based cross-point array for
analog neural network with record symmetry and linearity. In 2018 IEEE Symposium on VLSI
Technology, pages 25–26. IEEE, 2018.
[53] Seyoung Kim, Teodor Todorov, Murat Onen, Tayfun Gokmen, Douglas Bishop, Paul Solomon,
Ko-Tao Lee, Matt Copel, Damon B Farmer, John A Ott, et al. Metal-oxide based, CMOS-
compatible ECRAM for deep learning accelerator. In IEEE International Electron Devices
Meeting, pages 35–7. IEEE, 2019.
[54] Stefano Fusi and LF Abbott. Limits on the memory storage capacity of bounded synapses.
Nature neuroscience, 10(4):485–493, 2007.
[55] Jacopo Frascaroli, Stefano Brivio, Erika Covi, and Sabina Spiga. Evidence of soft bound
behaviour in analogue memristive devices for neuromorphic computing. Scientific reports,
8(1):1–12, 2018.
[56] Paiyu Chen, Binbin Lin, I-Ting Wang, Tuohung Hou, Jieping Ye, Sarma Vrudhula, Jae-sun Seo,
Yu Cao, and Shimeng Yu. Mitigating effects of non-ideal synaptic device characteristics for
on-chip learning. In IEEE/ACM International Conference on Computer-Aided Design, pages
194–199. IEEE, 2015.
[57] Lisha Chen, Heshan Devaka Fernando, Yiming Ying, and Tianyi Chen. Three-way trade-off in
multi-objective learning: Optimization, generalization and conflict-avoidance. In Advances in
Neural Information Processing Systems, 2023.
[58] Andreas Winkelbauer. Moments and absolute moments of the normal distribution. arXiv
preprint arXiv:1209.4340, 2012.
[59] Milton Abramowitz and Irene A Stegun. Handbook of mathematical functions with formulas,
graphs, and mathematical tables, volume 55. US Government printing office, 1948.
[60] NIST Digital Library of Mathematical Functions. https://dlmf.nist.gov/, Release 1.1.12
of 2023-12-15. F. W. J. Olver, A. B. Olde Daalhuis, D. W. Lozier, B. I. Schneider, R. F. Boisvert,
C. W. Clark, B. R. Miller, B. V . Saunders, H. S. Cohl, and M. A. McClain, eds.
[61] Tayfun Gokmen, Murat Onen, and Wilfried Haensch. Training deep convolutional neural
networks with resistive cross-point devices. Frontiers in neuroscience, 11:538, 2017.
14

Supplementary Material for “Towards Exact Gradient-based
Training on Analog In-memory Computing”
Table of Contents
A Literature Review 15
B Implementation of Asymmetric Updated by Pulse Update 16
C Examples of Analog Devices 17
C.1 Example 1: Linear step device (Soft bounds devices) . . . . . . . . . . . . . . . 17
C.2 Example 2: Power step device . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
C.3 Example 3: Exponential step device . . . . . . . . . . . . . . . . . . . . . . . . 17
D Verification of Assumptions 18
D.1 Verification of Assumption 4 under Strongly Convex Objective . . . . . . . . . . 18
D.2 Verification of Assumption 5: Non-zero Property of Gaussian Noise . . . . . . . 21
E Proof of Analog Training Properties 22
E.1 Proof of Lemma 1: Saturation and Fast Reset . . . . . . . . . . . . . . . . . . . 22
E.2 Proof of Theorem 1: Bounded Weight . . . . . . . . . . . . . . . . . . . . . . . 22
F Proof of Analog Gradient Descent Convergence 22
G Proof of Analog Stochastic Gradient Descent Convergence 23
G.1 Proof of Theorem 2: Convergence of Analog SGD . . . . . . . . . . . . . . . . 23
G.2 Proof of Theorem 3: Lower Bound of Analog SGD . . . . . . . . . . . . . . . . 25
H Proof of Tiki-Taka Convergence 29
H.1 Proof of Tracking Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 29
H.2 Proof of Weight Decay Lemma . . . . . . . . . . . . . . . . . . . . . . . . . . 32
H.3 Proof of Bounded Pk . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33
H.4 Proof of Tiki-Taka Descent Lemma . . . . . . . . . . . . . . . . . . . . . . . 34
H.5 Proof of Theorem 4: Convergence of Tiki-Taka . . . . . . . . . . . . . . . . . 35
I Simulation Details and Additional Results 37
I.1 Least squares problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 37
I.2 Classification problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
I.3 Verification of bounded saturation . . . . . . . . . . . . . . . . . . . . . . . . . 38
A Literature Review
This section briefly reviews literature that is related to this paper, as complementary to Section 1.
Inference and other applications of analog devices. Before designing hardware for training, a
series of AIMC prototypes focus on accelerating the inference phase [29–33, 12, 34]. The state-of-
the-art result shows that the analog inference is capable of reaching comparable accuracy with digital
inference [28]. Besides analog model training and inference, researchers also manage to exploit the
advantages of analog devices to facilitate the machine learning task. For example, energy-based
learning algorithms have been studied in [35], and the neural architecture search on analog devices
has been studied in [36].
Convergence analysis of gradient-based algorithms. In the digital domain, a series of literature has
discussed the convergence of gradient-based algorithms. As the basis of neural model training, much
work focuses on the convergence of SGD [37–39, 23] and its variety stochastic gradient descent with
momentum (SGDM) [40–43]. Notice that the difference between digital SGD (2) and Analog SGD
15

(3) lies on the asymmetric bias term, which implies Analog SGD can be regarded as a digital SGD
with bias. From this perspective, the convergence of Analog SGD (c.f. Theorem 2) is similar to that
of the digital bias-SGD counterparts. However, the results in this paper are still challenging and
non-trivial, especially in the convergence of Tiki-Taka, because the updates of both Wk and Pk
involve asymmetric bias.
Nowadays, gradient-based algorithms demonstrate their powerful capabilities in model training.
In fact, both SGD and SGDM have reached the theoretical lower bound of the convergence rate,p
σ2/K [24]. This lower bound is a generic result for any stochastic gradient-based algorithms,
which means that the convergence of gradient-based analog algorithms, including Analog SGD and
Tiki-Taka, is also subject to it.
B Implementation of Asymmetric Updated by Pulse Update
The proposed dynamic (3) of Analog SGD in Section 2 is inspired by the empirical studies in [22]
and [19]. However, the proposed dynamic (3) is only an approximation because we can not directly
apply an arbitrary increment ∆w on the weights. Instead, we need to send a series of pulses to analog
devices, which leads to the change on the weight. In this section, we introduce the implementation of
analog update U(·, ·) in (6) by pulse update and analyze the error of pulse update implementation.
Implementation. Similar to U(·, ·), we introduce the update function Up(·, ·) : R × {+, −} → R
for the pulsed update, defined by
Up(w, s) := w + ∆wmin · qs(w) =
w + ∆wmin · q+(w), s = +,
w − ∆wmin · q−(w), s = −, (15)
where ∆wmin > 0 is the response step size determined by devices. Since ∆wmin is the minimum
change of weight, it is sometimes called the resolution or granularity of the devices. Given the initial
weight w, the updated weight after receiving one pulse is Up(Wk, s) where the update sign s = + if
∆w ≥ 0 and s = − otherwise.
The response step size ∆wmin is usually known by physical measurement. In order to approximate
U(w, ∆w), analog device first computes the pulse series length (bit length) by
BL :=
 |∆w|
∆wmin

, (16)
so that
| BL ∆wmin − |∆w|| ≤ ∆wmin or |s BL ∆wmin − ∆w| ≤ ∆wmin. (17)
After that, a total of BL pulses are sent to the analog device, forcing the weight to become
U(w, ∆w) ≈ Up ◦ Up ◦ · · · ◦ Up| {z }
× BL
(w, s) = UBL
p (w, s). (18)
In this way, we implement the analog update U(w, ∆w).
Error analysis. Directly expending the (18) yields
UBL
p (w, s) = w + s∆wmin
BL −1X
t=0
qs(w + t∆wmin · sign(∆w)) (19)
= w + s∆wmin
BL −1X
t=0

qs(w) + t∆wmin · q′
s(w) · sign(∆w)

+ o(∆wmin)
= w + s∆wmin BL qs(w) + (∆wmin)2 BL(BL −1)
2 q′
s(w) · sign(∆w) + o(∆wmin)
= w + ∆w · qs(w) + O(∆wmin) + O((∆w)2),
which indicates the difference between U(w, ∆w) and UBL
p (w, s) are higher order infinitesimal. As
a concrete example, the ∆wmin is set as 0.002 [44] or 0.0949 [20] for RRAM devices.
16

For asymmetric linear device (ALD, defined in Section 2), it holds that |q′
+(w)| = |q′
−(w)| = 1
τ and
q′′
+(w) = q′′
−(w) = 0. Consequently, the difference between U(w, ∆w) and UBL
p (w, s) is
|UBL
p (w, s) − U(w, ∆w)| ≤ ∆wmin
τ + (∆wmin)2 BL(BL −1)
2τ ≤ ∆wmin
τ + (∆w)2
2τ . (20)
Since ∆wmin and ∆w = O(α) are usually small, the error can usually be ignored.
Significance. The significance of the analysis in this section is that it explains how to estimate the
response factors q+(·) and q−(·) in (3). In (15), it shows that they are, in fact, the response factors
of the given devices, which can be measured by physic measures [44]. In Section C, we showcase
various capable base materials for analog devices and their response factors.
C Examples of Analog Devices
In AIMC accelerators, weights of models can be represented by the conductance of based materials,
such as PCM [45, 46], ReRAM [47, 48], CBRAM [49, 50], or ECRAM [51]. In this section, we
showcase a spectrum of alternative devices for analog training.
C.1 Example 1: Linear step device (Soft bounds devices)
A large range of devices, including ReRAM [44, 20], Capacitor [52], ECRAM [52, 51], EcRamMO
[53], have linear response factor
q+(w) = 1 − w − w⋄
τmax
, q −(w) = 1 − w − w⋄
τmin
, (21)
where τmin < 0 < τmax and w⋄ are constants, whose meaning will be explained later. With these
response factors, the function F (·) and G(·) can be written as
F (w) = 1 − 1
2( 1
τmax
+ 1
τmin
)(w − w⋄), (22)
G(w) = − 1
2( 1
τmax
− 1
τmin
)(w − w⋄). (23)
ALD used in the paper is a special linear step device with τmax = τ and τmin = −τ.
C.2 Example 2: Power step device
For power step device, q+(w) and q−(w) are power function with respect to W , which models
implements synapse model [54, 55].
q+(w) =
 τmax − w
τmax − τmin
γ+
, q −(w) =
 w − τmin
τmax − τmin
γ−
(24)
where γ+ and γ− are parameters determined by materials.
C.3 Example 3: Exponential step device
In some ReRAM and CMOS-like devices [56, 21], the response factors are captured by an exponential
function, i.e. Exponential update step or CMOS-like update behavior.
q+(w) = 1 − exp

−γ+ · τmax − w
τmax − τmin

, (25)
q−(w) = 1 − exp

−γ− · w − τmin
τmax − τmin

. (26)
In Figure 6, the curve of response factors and F (·) and G(·) are illustrated.
17

4
 2
 0 2 4
w
0.0
0.5
1.0
1.5
2.0
2.5Response Factor
Linear Step Device
4
 2
 0 2 4
w
0.0
0.5
1.0
Power Step Device
4
 2
 0 2 4
w
0.00
0.25
0.50
0.75
1.00
Exponential Step Device
max
min
q+ (w)
q (w)
4
 2
 0 2 4
w
1.0
0.5
0.0
0.5
1.0
1.5
Sym/Asym components 4
 2
 0 2 4
w
0.4
0.2
0.0
0.2
0.4
4
 2
 0 2 4
w
1.0
0.5
0.0
0.5
1.0
F(w)
F(w)
G(w)
max
min
Figure 6: Response factors for different devices (without zero-shifting).
D Verification of Assumptions
D.1 Verification of Assumption 4 under Strongly Convex Objective
This section proves the weight Wk is strongly bounded when the loss function is strongly convex.
We prove the result for both Analog GD (εk = 0) and Analog SGD with εk possessing some special
structure. Before proving this, we introduce strongly convex assumption.
Assumption 6 (µ-strongly convex). The objective f(W ) is µ-strongly convex, i.e.
∥∇f(W ) − ∇f(W ′)∥ ≥ µ∥W − W ′∥, ∀W, W ′ ∈ RD. (27)
Assumption 6 ensures the critical point W ∗ (and hence optimal point) is unique.
D.1.1 Bounded Saturation of Analog GD
The following theorem provides a sufficient condition under which the Assumption 4 holds.
Theorem 5 (Bounded saturation of Analog GD). Suppose Assumptions 1 and 6 hold and define
Wmax := L
µ ∥W0 − W ∗∥ + ∥W ∗∥∞. (28)
The sequence {Wk} generated by Analog GD satisfies ∥Wk∥∞ ≤ Wmax.
Theorem 5 claims that if both L
µ ∥W0 − W ∗∥ and ∥W ∗∥∞ are sufficiently small, which means the
optimal point W ∗ is located inside the active region and W0 is properly initialized, the saturation
degree ∥Wk∥∞/τ can be sufficiently small as well, even though no projection operation is applied.
Proof of Theorem 5. Given f(·) is L-smooth (Assumption 1), we know from (57) that
f(Wk+1) ≤ f(Wk) − α
2

1 − ∥Wk∥2
∞
τ 2

∥∇f(Wk)∥2 ≤ f(Wk) (29)
where the second inequality comes from the bounded weight (Theorem 1). Consequently, we reach
the conclusion that f(Wk) ≤ f(W0), ∀k ∈ N. The strongly convex assumption (Assumption 6)
claims that
∥Wk − W ∗∥ ≤ 1
µ(f(Wk) − f ∗) ≤ 1
µ(f(W0) − f ∗) ≤ L
µ ∥W0 − W ∗∥. (30)
Therefore, triangle inequality ensures that
∥Wk∥∞ ≤ ∥Wk − W ∗∥∞ + ∥W ∗∥∞ (31)
≤ ∥Wk − W ∗∥ + ∥W ∗∥∞
≤ L
µ ∥W0 − W ∗∥ + ∥W ∗∥∞ (32)
which completes the proof.
18

Remark 2 (Bounded weight of digital GD) . Theorem 5 also holds for digital GD, which can be
achieved by forcing τ in inequality (29) to infinite.
In high-dimensional cases (D > 1), Theorem 5 does not exclude the situation ∥Wk∥∞ > ∥W ∗∥∞,
which is unfavorable in the example of Analog SGD’s lower bound (c.f. Section G.2). To facilitate
the proof of the lower bound, we consider the scalar case (D = 1). To distinguish between vectors
and scalars, we use the notation wk (and w∗) instead of Wk (and W ∗) in the next theorem.
Theorem 6 (Bounded saturation of Analog GD with scaler weight). Suppose Assumptions 1 and 6
hold and wk ∈ R is a scalar. If the learning rate α ≤ 1
2L, the following statements hold true
(Case 1) If w0 ≥ w∗, w0 ≥ wk ≥ (1 − αµ(1 − ∥wk∥∞
τ ))(wk − w∗) + w∗ ≥ wk+1 ≥ w∗;
(Case 2) If w0 < w ∗, w0 ≤ wk ≤ (1 − αµ(1 − ∥wk∥∞
τ ))(wk − w∗) + w∗ ≤ wk+1 ≤ w∗.
Consequently, the sequence {wk} generated by Analog GD satisfies the following relation |wk| ≤
wmax := min{|w0|, |w∗|}.
Theorem 6 asserts that {wk} converges monotonically from w0 to w∗. If |w0| ≤ | w∗|, we have
wmax = |w∗|, which improves the bound in Theorem 5.
Proof of Theorem 6. Expanding the objective f(·) in a Taylor series around w yields that2
f(w∗) = f(w) + ⟨∇f(w), w − w∗⟩ + ˜µ(w)∥w − w∗∥2, (33)
where ˜µ(w) depends on w and satisfies µ ≤ ˜µ(w) ≤ L because of the smoothness (Assumption 1)
and strong convexity (Assumption 6) assumptions. Consequently, the gradient on w can be written as
f(w) = ˜µ(w)(w − w∗) (34)
with which the gradient descent has the following property
wk − α∇f(wk) − w∗ =(1 − α˜µ(wk))(wk − w∗). (35)
Using this inequality and manipulating ∥wk+1 − w∗∥2 as
∥wk+1 − w∗∥2 (36)
= ∥wk − α∇f(wk) − α
τ |∇f(wk)| ⊙ wk − w∗∥2
= ∥wk − α∇f(wk) − w∗∥2 + 2α
τ ⟨wk − α∇f(wk) − w∗, |∇f(wk)| ⊙ wk∥⟩
+ α2
τ 2 ∥|∇f(wk)| ⊙ wk∥2
(a)
≤

1 − 2α˜µ(wk) + α2˜µ(wk)2 + 2α˜µ(wk)(1 − α˜µ(wk)) ∥wk∥∞
τ + α2˜µ(wk)2∥wk∥2
∞
τ 2

∥wk − w∗∥2
=

1 − 2α˜µ(wk)(1 − ∥wk∥∞
τ ) + α2˜µ(wk)2(1 − 2 ∥wk∥∞
τ + ∥wk∥2
∞
τ 2 )

∥wk − w∗∥2
=

1 − α˜µ(wk)(1 − ∥wk∥∞
τ )
2
∥wk − w∗∥2
where the (a) uses Cauchy’s inequality. Noticing Theorem 1 claims ∥wk∥∞
τ ≤ 1 and the learning rate
is chosen as α˜µ(wk) ≤ αL ≤ 1, we have ∥wk+1 − w∗∥2 ≤ ∥wk − w∗∥2.
In addition, Lemma 1 claims that
∥wk+1 − wk∥ ≤ ∥ 1 + wk/τ ∥∥α∇f(wk)∥ ≤ 2αL∥wk − w∗∥ ≤ ∥ wk − w∗∥ (37)
where the second inequality holds because Theorem 1 and smoothness assumption (Assumption
1 guarantee ∥1 + wk/τ ∥ ≤ 2 and ∥∇f(wk)∥ ≤ ∥ wk − w∗∥, respectively. This fact implies that
2Notice that the ℓ2-norm ∥ · ∥ and ℓ∞-norm ∥ · ∥∞ reduce to absolute value, when w is a scalar. Similarly,
inner product ⟨·, ·⟩ reduces to multiplication. We adopt the same notations for both vector and scalar weights to
make the notations consistent and easy to read.
19

wk+1 − w∗ has the same sign with wk − w∗. Consequently, inequality (36) and (37) show that in
case 1, if wk ≥ w∗, we have
wk ≥ (1 − αµ(1 − ∥wk∥∞
τ ))(wk − w∗) + w∗ (38)
≥ wk+1 ≥ w∗.
Keeping using this inequality reaches the result. Case 2 can be proved by the similar method.
D.1.2 Bounded Saturation of Analog SGD
The following theorem provides a sufficient condition under which the Assumption 4 holds. In the
section, we show that if the noise has special structures, the saturation degree ∥Wk∥∞/τ is bounded.
Assumption 7 (Random sample noise) . The noise can be written as ε = ∇f(W ; ξ) − ∇f(W ),
where ξ is a random variable sampled from an underline data distribution Df , f(W ; ξ) is a function
of W and ξ, and the gradient ∇f(W ; ξ) is taken over W .
Assumption 7 holds if the noise comes from the random sampling from a series of functions.
Theorem 7 (Bounded saturation of Analog SGD). Suppose Assumption 7 holds and each f(W ; ξ) is
L-smooth (Assumptions 1) and µ-strongly convex (Assumptions 6) with respect to W . Denote the
minimum of f(W ; ξ) as W ∗
ξ . If the supremum supξ{∥W ∗
ξ ∥∞} exists, define
Wmax := L
µ sup
ξ
{∥W0 − W ∗
ξ ∥} + sup
ξ
{∥W ∗
ξ ∥∞}. (39)
The sequence {Wk} generated by Analog SGD satisfies ∥Wk∥∞ ≤ Wmax uniformly.
Given the strong convexity with respect to W , f(W ; ξ) has unique minimum, and hence W ∗
ξ is well-
defined. Similarly, the supremum supξ{∥W0 − W ∗
ξ ∥} exists since supξ{∥W ∗
ξ ∥∞} exists. Theorem
7 claims that the saturation degree is bounded given the noise comes from random sampling over
a series of smooth and strongly convex functions. Similar to Theorem 5, Theorem 7 also holds for
digital GD by forcing τ in inequality (29) to infinite. The proof of Theorem 7 is inspired by that of
[57, Lemma 1].
Proof. According to Theorem 5, it holds that
∥Wk − W ∗
ξ ∥ ≤ L
µ ∥W0 − W ∗
ξ ∥. (40)
Therefore, triangle inequality ensures that
∥Wk∥∞ ≤ sup
ξ
{∥Wk − W ∗
ξ ∥∞} + sup
ξ
{∥W ∗
ξ ∥∞} (41)
≤ sup
ξ
{∥Wk − W ∗
ξ ∥} + sup
ξ
{∥W ∗
ξ ∥∞}
≤ L
µ sup
ξ
{∥W0 − W ∗
ξ ∥} + sup
ξ
{∥W ∗
ξ ∥∞}
which completes the proof.
Similar to Theorem 6, we can improve the bound when the weight is scalar (D = 1). The notation
wk (and w∗) instead of Wk (and W ∗) are adopted here to indicate the scalar situation.
Theorem 8 (Bounded saturation of Analog GD). Suppose Assumption 7 holds and for any ξ, f(w; ξ)
is L-smooth (Assumptions 1) and µ-strongly convex (Assumptions 6) with respect tow. Denote the
minimum of f(w; ξ) as w∗
ξ. If the supremum supξ{∥w∗
ξ ∥} exists, it holds for any k ∈ N that
min

w0, inf
ξ
{w∗
ξ }

≤ wk ≤ max
(
w0, sup
ξ
{w∗
ξ }
)
. (42)
20

Proof of Theorem 8. The theorem is proved by induction. The statement holds trivially at k = 0.
Suppose (42) holds for k. If Wk ≥ w∗, Theorem 6 guarantees that wk ≥ wk+1 ≥ w∗
ξ and hence
min

w0, inf
ξ
{w∗
ξ }

≤ w∗
ξ ≤ wk+1 ≤ wk ≤ max
(
w0, sup
ξ
{w∗
ξ }
)
. (43)
On the contrary, if wk ≤ w∗, Theorem 6 guarantees that wk ≤ wk+1 ≤ w∗
ξ and hence
min

w0, inf
ξ
{w∗
ξ }

≤ wk ≤ wk+1 ≤ w∗
ξ ≤ max
(
w0, sup
ξ
{w∗
ξ }
)
(44)
which implies wk+1 still satisfies (42). Now the conclusion is reached.
D.2 Verification of Assumption 5: Non-zero Property of Gaussian Noise
This section verifies Assumption 5. In principle, Assumption 5 requires the expectation that noise is
non-zero. To see that, note that when the probability density function of D is non-zero at only one
point, c equals zero. On the contrary, large c is at the other side of the spectrum, which appears when
the probability density function is relatively “uniformly distributed” throughout the probability space.
In analog acceleration devices, it is mild because noise is introduced during computation.
Generally, the estimation of c can be challenging because it involves the computation of the raw
momentum Eε[|g + ε|] of the noise. Scarifying the rigorousness a bit but providing an intuitive result,
we get the parameter c by considering an approximation of this raw momentum and deducing a bound
for this approximation.
In this section, we demonstrate that for Gaussian noise ε ∼ N (0, σ2), c can be selected as
q
2
π . For
any g, it holds that
Eε[∥1 − β
τ |g + ε|∥2
∞] = Eε[(1 − β
τ |g + ε|)2] (45)
= 1 − 2β
τ Eε[|g + ε|] + β2
τ 2 Eε[(g + ε)2]
= 1 − 2β
τ Eε[|g + ε|] + β2
τ 2 (g2 + σ2).
Because g + ε is Gaussian random variable with mean g and variance σ2, the second term in the RHS
has closed form [58] given by
Eε[|g + ε|] = σ
r
2
π 1F1

−1
2; 1
2; − g2
2σ2

. (46)
In the equation above, 1F1(·; ·; ·) is used to denote Kummer’s confluent hypergeometric functions. It
has been shown [59, 13.1.5] that when z ≤ 0, 1F1(·; ·; ·) has the asymptotic property
1F1(a; b; z) = Γ(b)
Γ(b − a)(−z)−a(1 + O(|z|−1)), (47)
where Γ(·) is Gamma function. When z is a small, 1F1(a; b; z) has the following approximation for
any a, b ∈ R [60, Sec 11.2]
1F1(a; b; z) ∼ 1 + z. (48)
Let z = − g2
2σ2 . On the one hand when σ ∼ 0, z is large and (47) with a = − 1
2 and b = 1
2 asserts that
Eε[|g + ε|] ∼ σ
r
2
π · Γ(1/2)
Γ(1)
r
g2
2σ2 (1 + O(2σ2
g2 )) =
r
2
π σ · √π(1 +
r
g2
2σ2 ). (49)
On the other hand, when σ → ∞, z is small and (48) provides that
Eε[|g + ε|] ∼ σ
r
2
π · (1 + g2
2σ2 ) =
r
2
π σ · (1 + g2
2σ2 ). (50)
In conclusion, choosing c =
q
2
π yields Eε[|g + ε|] ≿
q
2
π σ. Here ≿ is used to indicate the lower
bound holds for an approximation of the left-hand side (LHS).
21

E Proof of Analog Training Properties
E.1 Proof of Lemma 1: Saturation and Fast Reset
Lemma 1 (Saturation and fast reset). For ALD with a general w⋄, the following statements are valid
(Saturation) If sign(∆w) = sign(wk), it holds that |wk+1 − wk| = |1 − |wk − w⋄|/τ | · |∆w|.
(Fast Reset) If sign(∆w) = −sign(wk), it holds that |wk+1 − wk| = |1 + |wk − w⋄|/τ | · |∆w|.
Proof of Lemma 1. The proof can be completed by simply extending the LHS of
|wk+1 − wk| =
∆w − 1
τ |∆w|(wk − w⋄)
 (51)
=
|∆w|(sign(∆w) − 1
τ (wk − w⋄))

= |sign(∆w) − 1
τ (wk − w⋄)| · |∆w|
=
1 − sign(wk − w⋄)
sign(∆w)
|wk − w⋄|
τ
 · |∆w|.
Using the fact that w⋄ = 0 completes the proof.
E.2 Proof of Theorem 1: Bounded Weight
This section proves that the weight Wk is bounded.
Theorem 1 (Bounded weight). Denote ∥W ∥∞ as the ℓ∞ norm of W . Given ∥W0∥∞ ≤ τ and for
any sequence {∆Wk : k ∈ N}, which satisfies ∥∆Wk∥∞ ≤ τ, it holds that ∥Wk∥∞ ≤ τ, ∀k ∈ N.
Proof of Theorem 1. Without causing confusion, omit the superscriptk and denote the i-th coordinate
of Wk and ∆Wk as wi and gi, respectively. It holds that ∥Wk∥2
∞ < τ and |gij| ≤ τ
∥Wk+1 − W ⋄∥2
∞ = ∥Wk − W ⋄ + ∆Wk − 1
τ |∆Wk| ⊙ (Wk − W ⋄)∥2
∞ (52)
= max
i∈I
(wi − w⋄ − gi − 1
τ |gi|(wi − w⋄))2
= max
i∈I

(1 − 1
τ |gi|)2(wi − w⋄)2 − 2gi(1 − 1
τ |gi|)(wi − w⋄) + g2
i

≤ max
i∈I

(1 − 1
τ |gi|)2τ 2 + 2|gi| (1 − 1
τ |gi|)τ + g2
i

= max
i∈I

(1 − 1
τ |gi|) τ + |gi|
2
= τ 2.
Taking square root on both sides completes the proof.
F Proof of Analog Gradient Descent Convergence
In Section 3, we show that the asymptotic error is proportional to the noise varianceσ2. The following
theorem prove that the Analog GD converges to a critical point.
Theorem 9 (Convergence of Analog GD). Under Assumption 1, 2 and 4, if the learning rate is set as
α ≤ 1
L, it holds that
1
K
K−1X
k=0
∥∇f(Wk)∥2 ≤ 2(f(W0) − f ∗)L
K
1
1 − W 2max/τ 2 . (53)
A surprising conclusion of Theorem 9 is that the asymmetric update actually will not introduce any
asymptotic error in training. It also indicates that the asymptotic error comes from the interaction
between asymmetric update and noise from another perspective.
22

Proof of Theorem 9. The L-smooth assumption (Assumption 1) implies that
f(Wk+1) ≤ f(Wk) + ⟨∇f(Wk), Wk+1 − Wk⟩ + L
2 ∥Wk+1 − Wk∥2 (54)
= f(Wk) − α
2 ∥∇f(Wk)∥2 − ( 1
2α − L
2 )∥Wk+1 − Wk∥2
+ 1
2α ∥Wk+1 − Wk + α∇f(Wk)∥2
≤ f(Wk) − α
2 ∥∇f(Wk)∥2 + 1
2α ∥Wk+1 − Wk + α∇f(Wk)∥2
where the equality comes from
⟨∇f(Wk), Wk+1 − Wk⟩ = − α
2 ∥∇f(Wk)∥2 − 1
2α ∥Wk+1 − Wk∥2 (55)
+ 1
2α ∥Wk+1 − Wk + α∇f(Wk)∥2.
The third term in the RHS of (54) can be bounded by
1
2α ∥Wk+1 − Wk + α∇f(Wk)∥2 = α
2τ 2 ∥|∇f(Wk)| ⊙ Wk∥2 (56)
≤ α
2τ 2 ∥∇f(Wk)∥2 ∥Wk∥2
∞.
Substituting (56) back into (54) and cooperating with Assumption 4 yield
f(Wk+1) ≤ f(Wk) − α
2

1 − ∥Wk∥2
∞
τ 2

∥∇f(Wk)∥2 (57)
≤ f(Wk) − α
2

1 − W 2
max
τ 2

∥∇f(Wk)∥2.
Rearranging and averaging (57) for k from 0 to K − 1 deduce that
1
K
K−1X
k=0
∥∇f(Wk)∥2 ≤ 2(f(W0) − f(Wk))
αK(1 − W 2max/τ 2) . (58)
Noticing Assumption 2 claims that f(Wk) ≥ f ∗, we complete the proof.
G Proof of Analog Stochastic Gradient Descent Convergence
G.1 Proof of Theorem 2: Convergence of Analog SGD
This section provides the convergence guarantee of Analog SGD under non-convex assumption on
asymmetric linear devices.
Theorem 2 (Convergence of Analog SGD). Under Assumption 1-4, if the learning rate is set as
α =
q
f(W0)−f ∗
σ2LK and K is sufficiently large such that α ≤ 1
L, it holds that
1
K
K−1X
k=0
E[∥∇f(Wk)∥2] ≤ O
 r
(f(W0) − f ∗)σ2L
K
1
1 − W 2max/τ 2
!
+ 4σ2SK (9)
where SK denotes the amplification factor given by SK := 1
K
PK
k=0
∥Wk∥2
∞/τ 2
1−∥Wk∥2∞/τ 2 ≤ W 2
max/τ 2
1−W 2max/τ 2 .
Proof of Theorem 2. The L-smooth assumption (Assumption 1) implies that
Eεk[f(Wk+1)] ≤ f(Wk) + Eεk[⟨∇f(Wk), Wk+1 − Wk⟩] + L
2 Eεk[∥Wk+1 − Wk∥2] (59)
≤ f(Wk) − α
2 ∥∇f(Wk)∥2 − ( 1
2α − L)Eεk[∥Wk+1 − Wk + αεk∥2]
23

+ α2LEεk[∥εk∥2] + 1
2α ∥Wk+1 − Wk + α(∇f(Wk) + εk)∥2
where the second inequality comes from the assumption that noise has expectation 0 (Assumption 3)
Eεk[⟨∇f(Wk), Wk+1 − Wk⟩] = Eεk[⟨∇f(Wk), Wk+1 − Wk + αεk⟩]
= − α
2 ∥∇f(Wk)∥2 − 1
2α Eεk[∥Wk+1 − Wk + αεk∥2]
+ 1
2α Eεk[∥Wk+1 − Wk + α(∇f(Wk) + εk)∥2]
and the following inequality
L
2 Eεk[∥Wk+1 − Wk∥2] ≤ LEεk[∥Wk+1 − Wk + αεk∥2] + α2LEεk[∥εk∥2]. (60)
With the learning rate α ≤ 1
2L and bounded variance of noise (Assumption 3), (59) becomes
Eεk[f(Wk+1)] ≤ f(Wk) − α
2 ∥∇f(Wk)∥2 + α2Lσ2 + 1
2α ∥Wk+1 − Wk + α(∇f(Wk) + εk)∥2.
(61)
Cooperated with Assumption 3, the last term in the RHS of (59) can be bounded by
1
2α Eεk[∥Wk+1 − Wk + α∇f(Wk) + αεk∥2] (62)
= α
2τ 2 Eεk[∥|∇f(Wk) + εk| ⊙ Wk∥2]
≤ α
2τ 2 Eεk[∥∇f(Wk) + εk∥2] ∥Wk∥2
∞
≤ α
2τ 2 ∥∇f(Wk)∥2∥Wk∥2
∞ + ασ2
2τ 2 ∥Wk∥2
∞
where the last inequality comes from Assumption 3
Eεk[∥∇f(Wk) + εk∥2] = ∥∇f(Wk)∥2 + Eεk[∥εk∥2] ≤ ∥∇f(Wk)∥2 + σ2. (63)
Plugging (62) back into (59) yields
α
2

1 − ∥Wk∥2
∞
τ 2

∥∇f(Wk)∥2 ≤ Eεk[f(Wk) − f(Wk+1)] + α2Lσ2 + 2ασ2∥Wk∥2
∞
τ 2 . (64)
Assumption 4 ensures that 1 − ∥Wk∥2
∞/τ 2 ≥ 1 − Wmax/τ 2 > 0, which enables dividing both side
of (64) by 1 − ∥Wk∥2
∞/τ 2. Taking expectation over all εk, εk−1, · · · , ε0 and averaging for k from 0
to K − 1 deduce that
1
K
KX
k=0
E[∥∇f(Wk)∥2] (65)
≤ 2(f(W0) − E[f(Wk+1)])
αK(1 − W 2max/τ 2) + 2αLσ2
1 − W 2max/τ 2 + 1
K
KX
k=0
4σ2∥Wk∥2
∞/τ 2
1 − ∥Wk∥2∞/τ 2
≤ 2(f(W0) − f ∗)
αK(1 − W 2max/τ 2) + 2αLσ2
1 − W 2max/τ 2 + 1
K
KX
k=0
4σ2∥Wk∥2
∞/τ 2
1 − ∥Wk∥2∞/τ 2
= 4
r
(f(W0) − f ∗)σ2L
K
1
1 − W 2max/τ 2 + 4σ2SK
where the last equality chooses the learning rate as α =
q
f(W0)−f ∗
σ2LK . The proof is completed.
24

G.2 Proof of Theorem 3: Lower Bound of Analog SGD
This section provides the lower bound of Analog GD under non-convex assumption on noisy asym-
metric linear devices.
Theorem 3 (Lower bound of the error of Analog SGD). There is an instance which satisfies Assump-
tion 1-4 such that Analog SGD generates a sequence {Wk : k = 0, 1, · · · , K} which satisfies
1
K
K−1X
k=0
E[∥∇f(Wk)∥2] = 4σ2SK + Θ(α)
α=Θ

1√
K

= Ω

4σ2SK + 1√
K

. (10)
The proof is completed based on the following example.
(Example) Consider an example where all the coordinates are identical, i.e. Wk = wk1 for some
wk ∈ R where 1 ∈ RD is the all-one vector. Define W ∗ = w∗1 where w∗ ∈ R is a constant scalar
and a quadratic function f(W ) := L
2 ∥W − W ∗∥2 whose minimum is W ∗. Initialize the weight
on W0 = W ∗ and choose the learning rate α ≤ min{ 1
2L , 1
µ+6σ/(τ
√
D) }. Furthermore, consider the
noise defined as follows,
εk = ξk1 (66)
where random variable ξk ∈ R is sampled by
ξk =



ξ+
k := σ√
D
q
1−pk
pk
, w.p. pk,
ξ−
k := − σ√
D
q
pk
1−pk
, w.p. 1 − pk,
with pk = 1
2

1 − wk
τ

. (67)
As a reminder, it is always valid that |wk| = ∥Wk∥∞ ≤ τ (c.f. Theorem 5) and 0 ≤ pk ≤ 1.
Therefore, the noise distribution is well-defined. Furthermore, without loss of generality3, we assume
|w∗| ≤ τ
4 and σ ≤ τ L
√
D
4
√
3 .
Since all the coordinates are identical, Analog SGD can be regarded to train a scalar weight at each
coordinate. Furthermore, with the definition
f(w; ξk) := L
2 (w − w∗ + ξk
L )2, whose minimum is w∗
ξk = w∗ − ξk
L , (68)
the noise ξk satisfies Assumption 7. Therefore, even though w∗
ξk are time varying, we can regard wk
as the initial point and only consider wk+1. By this way, the conditions of Theorem 6 and Theorem 8
are met, and their claim for wk+1 is valid.
To prove the lower bound, we next introduce several necessary lemmas to facilitate the proof of
Theorem 3. The first one is used to verify Assumption 4.
Lemma 2. Suppose |w∗| ≤ τ
4 and σ ≤ τ L
√
D
4
√
3 . Define Wmax := τ
2 . The sequence {Wk : k ∈ N}
generated by Analog SGD (3) on the example above satisfies
∥Wk∥∞ ≤ Wmax. (69)
Proof of Lemma 2. The statement can be proved by induction. When k = 0,
∥W0∥∞ = |w∗| ≤ τ
4 < Wmax. (70)
To prove (69) for k + 1, Theorem 8 asserts that
min

W0, inf
[ε]i
{w∗
[ε]i }

≤ [Wk+1]i ≤ max
(
W0, sup
[ε]i
{w∗
[ε]i }
)
(71)
or equivalently
w∗ − σ
L
√
D
r1 − pk
pk
≤ [Wk+1]i ≤ w∗ + σ
L
√
D
r pk
1 − pk
. (72)
3These requirements are necessitated by Lemma 2 and can be relaxed to |w∗| ≤ c∗τ for any constant c∗ < 1.
In that situation, Lemma 2 remains valid, although Wmax differs.
25

According to the triangle inequality, ∥Wk+1∥∞ is still bounded by
∥Wk+1∥∞ = max
i
{|[Wk+1]i|} ≤ | w∗| + σ
L
√
D
max
r1 − pk
pk
,
r pk
1 − pk

(73)
= |w∗| + σ
L
√
D
s
1 + ∥Wk∥∞/τ
1 − ∥Wk∥∞/τ
≤|w∗| + σ
L
√
D
s
1 + Wmax/τ
1 − Wmax/τ
≤ τ
4 + τ
4
√
3
s
1 + (2τ)/τ
1 − (2τ)/τ = τ
2
= Wmax.
Therefore, (69) is confirmed for k + 1 and the proof of Lemma 2 is completed.
Noise ξk defined by (67) is time-varying since Wk is changing. The following lemma provides an
upper bound for the variation of noise with respect to Wk.
Lemma 3. Suppose ∥Wk∥∞ ≤ Wmax ≤ τ
2 . The following statements are always valid
|ξ+
k+1 − ξ+
k | ≤ 3σ
τ
√
D
|wk+1 − wk|, (74a)
|ξ−
k+1 − ξ−
k | ≤ 3σ
τ
√
D
|wk+1 − wk|. (74b)
Proof of Lemma 3. Define the functions
g+(x) :=
r
1 + x
1 − x , g −(x) :=
r
1 − x
1 + x , (75)
with which ξ+
k and ξ−
k can be written as
ξ+
k = σ√
D
g+
 wk
τ

, ξ −
k = σ√
D
g−
 wk
τ

. (76)
The derivatives of g+(x) and g−(x) is
∇g+(x) = 1
2
 
1p
(1 + x)(1 − x)
+
s
1 + x
(1 − x)3
!
, (77)
∇g−(x) = − 1
2
 
1p
(1 + x)(1 − x)
+
s
1 − x
(1 + x)3
!
, (78)
whose norms are upper bounded by
|∇g+(x)| ≤ 1
2
 2√
3 + 2
√
3

≤ 3, (79)
|∇g−(x)| ≤ 1
2
 2√
3 + 2
√
3

≤ 3. (80)
Therefore, both g+(x) and g−(x) are Lipschitz continuous and hence
|ξ+
k+1 − ξ+
k | = σ√
D
g+
 wk+1
τ

− g+
 wk
τ
 ≤ 3σ
τ
√
D
|wk+1 − wk|, (81)
|ξ−
k+1 − ξ−
k | = σ√
D
g−
 wk+1
τ

− g−
 wk
τ
 ≤ 3σ
τ
√
D
|wk+1 − wk|. (82)
The proof of Lemma 3 is then completed.
26

In the proof of Theorem 3, the most tricky part in the recursion(3) is the gradient wrapped by absolute
value |∇f(Wk) + εk|. Fortunately, the expectation E[|∇f(Wk) + εk|] can be expressed explicitly in
the example above, which is demonstrated by the following lemma.
Lemma 4. Suppose the learning rate is chosen such that α ≤ min{ 1
2L , 1
µ+6σ/(τ
√
D) } and (74) are
valid, it holds that
E[|∇f(Wk) + εk|] = 2σ√
D
p
pk(1 − pk) · 1 + (2pk − 1)∇f(Wk). (83)
Proof of Lemma 4. The proof of Lemma 4 closely relies on the following inequality
w∗ − ξ+
k
L ≤ [Wk]i ≤ w∗ + ξ−
k
L (84)
or equivalently, the gradient[∇f(Wk)]i = L([Wk]i − w∗) satisfies
ξ+
k ≤ [∇f(Wk)]i ≤ ξ−
k , ∀i ∈ I . (85)
By (85), we claim the signs of [∇f(Wk)]i + ξ+
k and [∇f(Wk)]i + ξ−
k never change during the
training and thus the absolute value can be removed
|∇f(Wk) + ξ+
k 1| = ξ+
k 1 + ∇f(Wk), (86)
|∇f(Wk) − ξ−
k 1| = ξ−
k 1 − ∇f(Wk). (87)
Accordingly, E[|∇f(Wk) + εk|] can be written as
E[|∇f(Wk) + εk|] (88)
= pk
 
ξ+
k 1 + ∇f(Wk)

+ (1 − pk)
 
ξ−
k 1 − ∇f(Wk)

= pk
 σ√
D
r1 − pk
pk
· 1 + ∇f(Wk)

+ (1 − pk)
 σ√
D
r pk
1 − pk
· 1 − ∇f(Wk)

= 2σ√
D
p
pk(1 − pk) · 1 + (2pk − 1)∇f(Wk)
which is the result. Therefore, the rest of proof shows (84) is valid.
Verification of(84). The statement can be proved by induction. When k = 0, (84) holds naturally
since W0 = W ∗.
To prove (84) for k + 1, we consider εk = ξ+
k 1 and εk = ξ−
k 1 separately, and the conclusion holds
for both cases.
Case 1: εk = ξ+
k 1 = σ√
D
q
1−pk
pk
· 1. From the induction assumption, it is valid that
[Wk]i ≥ w∗ − σ
L
√
D
r1 − pk
pk
= w∗
[ε]i . (89)
Therefore, Theorem 6 asserts that [Wk+1]i ≤ [Wk]i and hence we have pk+1 ≥ pk, ξ+
k+1 ≤ ξ+
k and
ξ−
k+1 ≤ ξ−
k . Consequently, the second inequality of (84) can be reached by Theorem 8
[Wk+1]i ≤ w∗ − ξ−
k
L ≤ w∗ − ξ−
k+1
L . (90)
To obtain the other inequality, notice that
w∗ − ξ+
k+1
L = [Wk+1]i + (w∗ − ξ+
k+1
L − [Wk+1]i)
| {z }
≤0
+ 1
L (ξ+
k − ξ+
k+1)
| {z }
≥0
(91)
(a)
= [ Wk+1]i + (1 − αL(1 − [Wk]i
τ ))(w∗ − ξ+
k+1
L − [Wk]i) + 1
L(ξ+
k − ξ+
k+1)
(b)
≤ [Wk+1]i + (1 − αL(1 − [Wk]i
τ ))(w∗ − ξ+
k+1
L − [Wk]i) + 6ασ
τ
√
D
([Wk]i − (w∗ − ξ+
k+1
L ))
27

= [Wk+1]i − (1 − αL(1 − [Wk]i
τ ) − 6ασ
τ
√
D
)([Wk]i − (w∗ − ξ+
k+1
L ))
where (a) comes from Theorem 6, (b) comes from Lemma 3 and inequality (37).
ξ+
k − ξ+
k+1 = |ξ+
k − ξ+
k+1| ≤ 3
τ
√
D
|[Wk+1]i − [Wk]i| ≤ 6ασ
τ
√
D
|[Wk]i − (w∗ − ξ+
k+1
L )|. (92)
The choice of learning rate implies that
1 − αL(1 − [Wk]i
τ ) − 6ασ
τ
√
D
≥ 1 − α(L + 6σ
τ
√
D
) ≥ 0 (93)
from which and (91) we reach that
w∗ − ξ+
k+1
L ≤ [Wk+1]i. (94)
Combining (94) and (90) reaches (84) in case 1.
Case 2: εk = ξ−
k 1 = − σ√
D
q
pk
1−pk
· 1. Noticing the permutation invariant between pk and 1 − pk
in the noise definition (67), we find that the proof for case 2 is similar to that of case 1. Therefore, the
proof of case 2 is omitted here.
In conclusion, (84) still holds for k + 1 and (84) is verified. Now the proof of Lemma 4 is completed.
After providing the necessary lemmas, we begin to prove Theorem 3.
Proof of Theorem 3. Consider the example constructed above. Before deriving the lower bound, we
demonstrate Assumption 1–4 hold. It is obvious that ∇f(W ) = L(W − W ∗) satisfies Assumption 1
and f(W ) ≥ f ∗ := 0 satisfies Assumption 2. In addition, Assumption 3 could be verified by noticing
(66) impplies Eεk[εk] = 0 and Eεk[∥εk∥2] ≤ σ2. Assumption 4 is verified by Lemma 2.
Now we can derive the lower bound. Utilizing Lemma 4 and manipulating the recursion (3), we have
the following result
Eεk[Wk+1 − W ∗] (95)
= Eεk[Wk − α∇f(Wk) − αεk − α
τ |∇f(Wk) + εk| ⊙ Wk − W ∗]
= (1 − αL)(Wk − W ∗) − α
τ Eεk[|∇f(Wk) + εk|] ⊙ Wk
= (1 − αL)(Wk − W ∗) − 2ασ
τ
√
D
p
pk(1 − pk) · Wk − α
τ (2pk − 1)∇f(Wk) ⊙ Wk
Multiplying the both size of (95) by L and pluging in the equation ∇f(Wk) = L(Wk − W ∗) yield
Eεk[∇f(Wk+1)] (96)
= (1 − αL)∇f(Wk) − 2αLσ
τ
√
D
p
pk(1 − pk) · Wk − αL
τ (2pk − 1)∇f(Wk) ⊙ Wk
=

1 − αL − αL(2pk − 1) wk
τ

∇f(Wk) − 2αLσ
τ
√
D
p
pk(1 − pk) · Wk.
where the last equality uses Wk = wk1. Recall the probability pk = 1
2(1 − wk
τ ), we have
1 − pk = 1
2

1 + wk
τ

,
p
pk(1 − pk) = 1
2
r
1 − w2
k
τ 2 , 2pk − 1 = − wk
τ . (97)
Substitute them back into (96)
Eεk[∇f(Wk+1)] =

1 − αL + αL |wk|2
τ 2

∇f(Wk) − 2αLσ
τ
√
D
r
1 − |wk|2
τ 2 · Wk (98)
28

=

1 − αL + αL ∥Wk∥2
∞
τ 2

∇f(Wk) − 2αLσ
τ
√
D
r
1 − ∥Wk∥2∞
τ 2 · Wk
= ∇f(Wk) − αL

1 − ∥Wk∥2
∞
τ 2

∇f(Wk) − 2αLσ
τ
√
D
r
1 − ∥Wk∥2∞
τ 2 · Wk
where the first equality utilizes |wk|2 = ∥wk1∥2
∞ = ∥Wk∥2
∞. Reorganizing (98), we obtain
∇f(Wk) = − 2σp
1 − ∥Wk∥2∞/τ 2
Wk/τ√
D
+ ∇f(Wk) − Eεk[∇f(Wk+1)]
1 − ∥Wk∥2∞/τ 2 .
It is worth noticing that the second term in the RHS of (99) is in the order of O(α)

∇f(Wk) − Eεk[∇f(Wk+1)]
1 − ∥Wk∥2∞/τ 2
 = L

Eεk[Wk − Wk+1]
1 − ∥Wk∥2∞/τ 2

(a)
≤ αL ∥Eεk[∇f(Wk) + εk]∥
(1 − ∥Wk∥∞/τ)(1 − ∥Wk∥2∞/τ 2)
= αL ∥∇f(Wk)∥
(1 − ∥Wk∥∞/τ)(1 − ∥Wk∥2∞/τ 2)
(b)
≤ αL2(Wmax + W ∗)D
(1 − Wmax/τ)(1 − W 2max/τ 2)
= O(α)
where (a) is Lemma 1 and (b) comes from ∥∇f(Wk)∥ = L∥Wk − W ∗∥ ≤ L(∥Wk∥ + ∥W ∗∥) ≤
L
√
D(∥Wk∥∞ + |w∗|). Therefore, the gradient can be rewritten as
∇f(Wk) = − 2σp
1 − ∥Wk∥2∞/τ 2
Wk/τ√
D
+ O(α). (99)
Taking the square norm of the gradient and averaging for k from 0 to K − 1 we obtain
1
K
KX
k=0
∥∇f(Wk)∥2 = 4σ2 1
K
KX
k=0
∥Wk∥2
∞/τ 2
1 − ∥Wk∥2∞/τ 2 = 4σ2SK + O(α) (100)
where the following equality is applied

Wk/τ√
D

2
=

∥Wk∥∞1√
D

2
= ∥Wk∥2
∞/τ 2. (101)
The proof of Theorem 3 is thus completed.
H Proof of Tiki-Taka Convergence
This section provides the convergence guarantee of theTiki-Taka under the non-convex assumption.
Theorem 4 (Convergence of Tiki-Taka). Suppose Assumption 1-5 hold and the learning rate is set
as α = O(1/
√
σ2K), β = 8αL. It holds for Tiki-Taka that the expected infinity norm Pk is upper
bounded by E[∥Pk+1∥2
∞] ≤ P 2
max := 41L2τ 4D
c2σ2 . Furthermore, if σ2 and D are sufficiently large so
that 33P 2
max/τ 2 < 1 it is valid that
1
K
K−1X
k=0
E[∥∇f(Wk)∥2] ≤ O
 r
(f(W0) − f ∗)σ2L
K
1
1 − 33P 2max/τ 2
!
. (14)
H.1 Proof of Tracking Lemma
Before the proving Theorem 4, we first introduce two useful lemmas.
29

Lemma 5 (Tracking Lemma). Under Assumption 1 and 3, it holds that
Eεk+1[∥Pk+2 − τ
√
D
σ ∇f(Wk+1)∥2] (102)
≤

1 − βσ
τ
√
D

∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + 4L2τ 3D3/2
βσ3 ∥Wk+1 − Wk∥2 + 4βσ
τ
√
D
∥Pk+1∥2
+ 4β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk)∥2 + 2β2σ2.
To make Pk+2 sufficiently close to the true gradient, ∥Pk+1∥2 should be small.
Proof of Lemma 5. According to the update rule of Pk, we have
Eεk+1[∥Pk+2 − τ
√
D
σ ∇f(Wk+1)∥2] (103)
= Eεk+1[∥Pk+1 + β∇f(Wk+1) + βεk+1 − β
τ |∇f(Wk+1) + εk+1| ⊙ Pk+1 − τ
√
D
σ ∇f(Wk+1)∥2]
= Eεk+1[∥(1 − βσ
τ
√
D
)(Pk+1 − τ
√
D
σ ∇f(Wk+1)) + βσ
τ
√
D
Pk+1
− β
τ |∇f(Wk+1) + εk+1| ⊙ Pk+1 + βεk+1∥2]
≤ 1
1 − u Eεk+1[∥(1 − βσ
τ
√
D
)(Pk+1 − τ
√
D
σ ∇f(Wk+1)) + βεk+1∥2]
+ β2
uτ 2 Eεk+1[∥(σ1/
√
D − |∇f(Wk+1) + εk+1|) ⊙ Pk+1∥2].
With Assumption 3, the first term of (103) can be bounded via the same technique as (63), that is
1
1 − u Eεk+1[∥(1 − βσ
τ
√
D
)(Pk+1 − τ
√
D
σ ∇f(Wk+1)) + βεk+1∥2] (104)
≤
(1 − βσ
τ
√
D )2
1 − u ∥Pk+1 − τ
√
D
σ ∇f(Wk+1)∥2 + β2
1 − u σ2
≤

1 − 2βσ
τ
√
D
+ u

∥Pk+1 − τ
√
D
σ ∇f(Wk+1)∥2 + β2
1 − u σ2
where the last inequality holds because
(1 − βσ
τ
√
D
)2 ≤ (1 − u)

1 − 2βσ
τ
√
D
+ u

. (105)
The third term in the RHS of (103) can be manipulated by the variance decomposition and conse-
quently can be bounded by
β2
uτ 2 Eεk+1[∥(σ1/
√
D − |∇f(Wk+1) + εk+1|) ⊙ Pk+1∥2] (106)
= β2
uτ 2 Eεk+1
"X
i∈I
(σ/
√
D − |[∇f(Wk+1) + εk+1]i|)2[Pk+1]2
i
#
= β2
uτ 2 Eεk+1
"X
i∈I
(σ2/D − 2σ|[∇f(Wk+1) + εk+1]i| + [∇f(Wk+1) + εk+1]2
i )[Pk+1]2
i
#
= β2
uτ 2 Eεk+1
"X
i∈I
(σ2/D − 2σ|[∇f(Wk+1) + εk+1]i|)[Pk+1]2
i
#
+ β2
uτ 2 Eεk+1
"X
i∈I
([∇f(Wk+1) + εk+1]2
i )[Pk+1]2
i
#
30

≤ β2
uτ 2 (σ2/D − 2cσ2 + σ2/D)∥Pk+1∥2 + β2
uτ 2 ∥Pk+1∥2
∞∥∇f(Wk+1)∥2
≤ 2β2σ2
uτ 2D ∥Pk+1∥2 + β2
uτ 2 ∥Pk+1∥2
∞∥∇f(Wk+1)∥2.
Combining (104) and (106) with (103) provides that
Eεk+1[∥Pk+2 − τ
√
D
σ ∇f(Wk+1)∥2] (107)
≤

1 − 2βσ
τ
√
D
+ u

∥Pk+1 − τ
√
D
σ ∇f(Wk+1)∥2 + β2
1 − u σ2 + 2β2σ2
uτ 2D ∥Pk+1∥2
+ β2
uτ 2 ∥Pk+1∥2
∞∥∇f(Wk+1)∥2
(a)
≤

1 − 3βσ
2τ
√
D

∥Pk+1 − τ
√
D
σ ∇f(Wk+1)∥2 + 2β2σ2 + 4βσ
τ
√
D
∥Pk+1∥2
+ 2β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk+1)∥2
≤

1 − 3βσ
2τ
√
D

1 + βσ
2τ
√
D

∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2
+

1 − 3βσ
2τ
√
D
 
1 + 2τ
√
D
βσ
!
τ 2D
σ2 ∥∇f(Wk+1) − ∇f(Wk)∥2 + 2β2σ2
+ 4βσ
τ
√
D
∥Pk+1∥2 + 2β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk+1)∥2
(b)
≤

1 − βσ
τ
√
D

∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + 2L2τ 3D3/2
βσ3 ∥Wk+1 − Wk∥2 + 4βσ
τ
√
D
∥Pk+1∥2
+ 2β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk+1)∥2 + 2β2σ2,
(c)
≤

1 − βσ
τ
√
D

∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + 4L2τ 3D3/2
βσ3 ∥Wk+1 − Wk∥2 + 4βσ
τ
√
D
∥Pk+1∥2
+ 4β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk)∥2 + 2β2σ2.
In the inequality above, (a) sets u = β/2; (b) holds because

1 − 3βσ
2τ
√
D

1 + βσ
2τ
√
D

= 1 − βσ
τ
√
D
− 3βσ
2τ
√
D
βσ
2τ
√
D
≤ 1 − βσ
τ
√
D
(108)
and

1 − 3βσ
2τ
√
D
 
1 + 2τ
√
D
βσ
!
= 2τ
√
D
βσ

1 − 3βσ
2τ
√
D

1 + βσ
2τ
√
D

≤ 2τ
√
D
βσ ; (109)
and (c) comes from
2β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk+1)∥2 (110)
≤ 4β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk)∥2 + 4β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk+1) − ∇f(Wk)∥2
≤ 4β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk)∥2 + 4βL2τ
√
D
σ ∥Wk+1 − Wk∥2
≤ 4β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk)∥2 + 2L2τ 3D3/2
βσ3 ∥Wk+1 − Wk∥2.
where the last inequality holds because β ≤ 2τ 2D
βσ 2 Now we get the claim.
31

H.2 Proof of Weight Decay Lemma
Lemma 6 (Weight Decay Lemma). Suppose Assumption 1, 3 and 5 hold. If β satisfies
β ≤ min
 cτ
2(4L2τ 2 + σ2) , 2
3cσ

, (111)
it holds that
Eεk+1[∥Pk+2∥2] ≤ (1 − βcσ
2τ )∥Pk+1∥2 + 4βτ
cσ ∥∇f(Wk)∥2 + 4L2βτ
cσ ∥Wk+1 − Wk∥2 + 3β2σ2.
Proof of Lemma 6. The proof begins from the manipulation of the expected square norm of Pk+2.
Eεk+1[∥Pk+2∥2] (112)
= Eεk+1[∥Pk+1 + β(∇f(Wk+1) + εk+1) − β
τ |∇f(Wk+1) + εk+1| ⊙ Pk+1∥2]
= Eεk+1[∥(1 − β
τ |∇f(Wk+1) + εk+1|) ⊙ Pk+1 + β(∇f(Wk+1) + εk+1)∥2]
= Eεk+1[∥(1 − β
τ |∇f(Wk+1) + εk+1|) ⊙ Pk+1∥2] + β2Eεk+1[∥∇f(Wk+1) + εk+1∥2]
+ 2Eεk+1

(1 − β
τ |∇f(Wk+1) + εk+1|) ⊙ Pk+1, β(∇f(Wk+1) + εk+1)

.
For the sake of simplicity, below we use [gk+1]i := [∇f(Wk+1)]i to denote the i-th coordinate of
the gradient. According to Assumption 5, the first term in the RHS of (112) can be bounded by
Eεk+1[∥(1 − β
τ |∇f(Wk+1) + εk+1|) ⊙ Pk+1∥2] (113)
= Eεk+1
"X
i∈I
(1 − β
τ |[gk+1]i + [εk+1]i|)2[Pk+1]2
i
#
=
X
i∈I

1 − 2β
τ E[εk+1]i[|[gk+1]i + [εk+1]i|] + β2
τ 2 E[εk+1]i[([gk+1]i + [εk+1]i)2]

[Pk+1]2
i
≤
X
i∈I

1 − 2βcσ
τ + β2(4L2τ 2 + σ2)
τ 2

[Pk+1]2
i
≤

1 − 3βcσ
2τ

∥Pk+1∥2,
where the last inequality holds because of the selection of β. To bound the third term in the RHS of
(112), we use the unbiased property of Eεk+1[εk+1] = 0 (Assumption 3) and ∥Pk∥∞ ≤ τ (Theorem
1), which lead to
2Eεk+1

(1 − β
τ |∇f(Wk+1) + εk+1|) ⊙ Pk+1, β(∇f(Wk+1) + εk+1)

(114)
= 2βEεk+1 [⟨Pk+1, ∇f(Wk+1) + εk+1⟩]
− 2Eεk+1
 β
τ |∇f(Wk+1) + εk+1| ⊙ Pk+1, β(∇f(Wk+1) + εk+1)

= 2β ⟨Pk+1, ∇f(Wk+1)⟩ − 2β2
τ Eεk+1 [⟨|∇f(Wk+1) + εk+1|, (∇f(Wk+1) + εk+1) ⊙ Pk+1⟩]
≤ 2β ⟨Pk+1, ∇f(Wk+1)⟩ + 2β2Eεk+1

∥∇f(Wk+1) + εk+1∥2
.
Plugging (113) and (114) into (112) yields
Eεk+1[∥Pk+2∥2] ≤

1 − 3βcσ
2τ

∥Pk+1∥2 + 3β2Eεk+1[∥∇f(Wk+1) + εk+1∥2] (115)
+ 2β ⟨Pk+1, ∇f(Wk+1)⟩ .
32

Notice that the second term in theRHS of (115) can be upper bounded by Assumption 3 and inequality
(63), and the third term can be bounded by Young’s inequality
2β ⟨Pk+1, ∇f(Wk+1)⟩ ≤ βcσ
τ ∥Pk+1∥2 + βτ
cσ ∥∇f(Wk+1)∥2. (116)
Inequality (63), (116) and the condition β ≤ 2
3cσ lead to the result
Eεk+1[∥Pk+2∥2] ≤ (1 − βcσ
2τ )∥Pk+1∥2 + 2βτ
cσ ∥∇f(Wk+1)∥2 + 3β2σ2. (117)
Applying the inequality
∥∇f(Wk+1)∥2 ≤ 2∥∇f(Wk)∥2 + 2∥∇f(Wk+1) − ∇f(Wk)∥2 (118)
≤ 2∥∇f(Wk)∥2 + 2L2∥Wk+1 − Wk∥2
on (117) completes the proof.
H.3 Proof of Bounded Pk
Lemma 7 (Bounded Saturation of Pk). Suppose Assumption 1, 3 and 5 hold. If β satisfies
β ≤ min
 cτ
2(4L2τ 2 + σ2) , 2
3cσ , L2τ 3D
3cσ3

, (119)
it holds that
Eεk[∥Pk+2∥2
∞] ≤ (1 − βcσ
τ )∥Pk+1∥2
∞ + 41βL2τ 3D
cσ (120)
and further
E[∥Pk+1∥2
∞] ≤ P 2
max := 41L2τ 4D
c2σ2 . (121)
Proof of Lemma 7. Let i∗ := arg maxi |[Pk+2]i|. We expand the expected square norm of Pk+2 by
Eεk[∥Pk+2∥2
∞] (122)
= Eεk[∥Pk+1 + β(∇f(Wk) + εk) − β
τ |∇f(Wk) + εk| ⊙ Pk+1∥2
∞]
= Eεk[∥(1 − β
τ |∇f(Wk) + εk|) ⊙ Pk+1 + β(∇f(Wk) + εk)∥2
∞]
= Eεk[[(1 − β
τ |∇f(Wk) + εk|) ⊙ Pk+1]2
i∗] + β2Eεk[[∇f(Wk) + εk]2
i∗]
+ 2Eεk

[(1 − β
τ |∇f(Wk) + εk|) ⊙ Pk+1 ⊙ β(∇f(Wk) + εk)]i∗

.
For the sake of simplicity, below we use [gk]i := [∇f(Wk)]i to denote the i-th coordinate of the
gradient. According to Assumption 5, the first term in the RHS of (122) can be bounded by
Eεk[[(1 − β
τ |∇f(Wk) + εk|) ⊙ Pk+1]2
i∗] (123)
= Eεk

(1 − β
τ |[gk]i∗ + [εk]i∗ |)2[Pk+1]2
i∗

=

1 − 2β
τ E[εk]i[|[gk]i + [εk]i|] + β2
τ 2 E[εk]i[([gk]i + [εk]i)2]

[Pk+1]2
i∗
≤

1 − 2βcσ
τ + β2(4L2τ 2 + σ2)
τ 2

[Pk+1]2
i∗
≤

1 − 3βcσ
2τ

∥Pk+1∥2
∞
33

where the last inequality holds because of the selection of β. To bound the third term in the RHS
of (122), we use the unbiased property of E[εk]i∗ [[εk]i∗] = 0 of Assumption 5 and ∥Pk+1∥∞ ≤ τ in
Theorem 1, which lead to
2Eεk

[(1 − β
τ |∇f(Wk) + εk|) ⊙ Pk+1 ⊙ β(∇f(Wk) + εk)]i∗

(124)
= 2βEεk [[Pk+1 ⊙ ∇f(Wk) + εk]i∗]
− 2Eεk

[ β
τ |∇f(Wk) + εk| ⊙ Pk+1 ⊙ β(∇f(Wk) + εk)]i∗

= 2β[Pk+1 ⊙ ∇f(Wk)]i∗ − 2β2
τ Eεk [[|∇f(Wk) + εk| ⊙ Pk+1 ⊙ (∇f(Wk) + εk)]i∗]
≤ 2β[Pk+1 ⊙ ∇f(Wk)]i∗ + 2β2Eεk[[∇f(Wk) + εk]2
i∗].
Plugging (123) and (124) into (122) yields
Eεk[∥Pk+2∥2
∞] (125)
≤

1 − 3βcσ
2τ

∥Pk+1∥2
∞ + 3β2Eεk[[∇f(Wk) + εk]2
i∗] + 2β[Pk+1 ⊙ ∇f(Wk)]i∗ .
≤

1 − 3βcσ
2τ

∥Pk+1∥2
∞ + 3β2∥∇f(Wk)∥2
∞ + 2β[Pk+1 ⊙ ∇f(Wk)]i∗ + 3β2σ2
D
where the last inequality comes from Assumption 5 and variance decomposition (63). Notice that the
second term in the RHS of (125) can be upper bounded by Assumption 3 and inequality (63), and the
third term can be bounded by Young’s inequality
2β[Pk+1 ⊙ ∇f(Wk)]i∗ ≤ βcσ
2τ [Pk+1]2
i∗ + 8βτ
cσ [∇f(Wk)]2
i∗ ≤ βcσ
2τ ∥Pk+1∥2
∞ + 8βτ
cσ ∥∇f(Wk)∥2
∞.
(126)
Inequality (126) and the condition β ≤ 2
3cσ lead to the result
Eεk[∥Pk+2∥2
∞] ≤ (1 − βcσ
τ )∥Pk+1∥2
∞ + 10βτ
cσ ∥∇f(Wk)∥2
∞ + 3β2σ2
D (127)
≤ (1 − βcσ
τ )∥Pk+1∥2
∞ + 40βL2τ 3D
cσ + 3β2σ2
D
≤ (1 − βcσ
τ )∥Pk+1∥2
∞ + 41βL4τ 3D
cσ
where the last inequality holds because the parameter is chosen as β ≤ L2τ 3D
3cσ3 . Telescoping over 0 to
K − 1 and using the fact that ∥∇f(Wk)∥2
∞ ≤ ∥∇f(Wk)∥2 ≤ 4L2τ 2D, we achieve that
E[∥Pk∥2
∞] ≤ (1 − βcσ
τ )k∥P0∥2
∞ + 41βL2τ 3D
cσ ≤ 41L2τ 4D
c2σ2 .
where the last inequality holds due to the fact that P0 = 0. Now we complete the proof.
H.4 Proof of Tiki-Taka Descent Lemma
Lemma 8 (Descent Lemma). Under Assumption 1, it holds that
f(Wk+1) ≤ f(Wk) − ατ
√
D
2σ ∥∇f(Wk)∥2 −
 σ
2ατ
√
D
− L
2

∥Wk+1 − Wk∥2 (128)
+ ασ
τ
√
D
∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + α∥Pk+1∥2.
Proof of Lemma 8. The L-smooth assumption (Assumption 1) implies that
f(Wk+1) ≤ f(Wk) + ⟨∇f(Wk), Wk+1 − Wk⟩ + L
2 ∥Wk+1 − Wk∥2 (129)
34

= f(Wk) − ατ
√
D
2σ ∥∇f(Wk)∥2 −
 σ
2ατ
√
D
− L
2

∥Wk+1 − Wk∥2
+ σ
2ατ
√
D
∥Wk+1 − Wk + ατ
√
D
σ ∇f(Wk)∥2
where the equality comes from
⟨∇f(Wk), Wk+1 − Wk⟩ = σ
ατ
√
D
*
ατ
√
D
σ ∇f(Wk), Wk+1 − Wk
+
(130)
= − ατ
√
D
2σ ∥∇f(Wk)∥2 − σ
2ατ
√
D
∥Wk+1 − Wk∥2
+ σ
2ατ
√
D
∥Wk+1 − Wk + ατ
√
D
σ ∇f(Wk)∥2.
From the update rule of Tiki-Taka and bounded saturation assumption (c.f. Assumption 4), the
third term in the RHS of (129) can be bounded by
σ
2ατ
√
D
∥Wk+1 − Wk + ατ
√
D
σ ∇f(Wk)∥2 (131)
= ασ
2τ
√
D
∥Pk+1 − τ
√
D
σ ∇f(Wk) + 1
τ |Pk+1| ⊙ Wk∥2
≤ ασ
τ
√
D
∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + ασ
τ 3√
D
∥|Pk+1| ⊙ Wk∥2
≤ ασ
τ
√
D
∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + ασ
τ 3√
D
∥Pk+1∥2∥Wk∥2
∞
≤ ασ
τ
√
D
∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + α∥Pk+1∥2.
Substituting (131) back into (129) yields
f(Wk+1) ≤ f(Wk) − ατ
√
D
2σ ∥∇f(Wk)∥2 −
 σ
2ατ
√
D
− L
2

∥Wk+1 − Wk∥2 (132)
+ ασ
τ
√
D
∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + α∥Pk+1∥2
which completes the proof.
H.5 Proof of Theorem 4: Convergence of Tiki-Taka
With the lemmas above, we can begin the proof of Theorem 4.
Proof of Theorem 4. The statement that E[∥Pk+1∥2
∞] ≤ P 2
max := 41L2τ 4D
c2σ2 has been shown in
Lemma 7. Therefore, it suffices to prove the convergence rate.
The proof begins by defining a Lyapunov function
Vk := f(Wk) − f ∗ + σ
Lτ
√
D
∥Pk+1 − ∇f(Wk)∥2 + 5σ
Lcτ D∥Pk+1∥2 (133)
+ 8
Lcτ σ∥Pk+1∥2
∞∥∇f(Wk)∥2.
Notice that the last term of Vk has the iteration
Eεk+1[∥Pk+2∥2
∞∥∇f(Wk+1)∥2] (134)
(a)
≤ 1
1 − u Eεk+1[∥Pk+2∥2
∞∥∇f(Wk)∥2]
35

+ 1
u Eεk+1[∥Pk+2∥2
∞∥∇f(Wk+1) − ∇f(Wk)∥2]
(b)
≤ 1
1 − u

(1 − βcσ
τ )∥Pk+1∥2
∞
41βL2τ 3D
cσ

∥∇f(Wk)∥2 + L2τ 2
u ∥Wk+1 − Wk∥2
(c)
≤ (1 − βcσ
2τ )∥Pk+1∥2
∞∥∇f(Wk)∥2 + 82βL2τ 3D
cσ ∥∇f(Wk)∥2 + L2τ 3
βcσ ∥Wk+1 − Wk∥2
where (a) uses ∥x + y∥2 ≤ 1
1−u ∥x∥2 + 1
u ∥y∥2 for any vector x, y ∈ RD and scalar 0 < u < 1; (b)
follows Lemma 7, bounded weight (c.f. Theorem 1), and Assumption 1; (c) specifies u = βcσ
2τ and
the inequalities 1−βcσ/τ
1−u ≤ 1 − βcσ
τ + u and 1
1−u ≤ 2.
By inequality (134), Lemma 5, 6 and 8, we have
Eεk+1[Vk+1] (135)
= f(Wk+1) − f ∗ + σ
Lτ
√
D
Eεk+1[∥Pk+2 − ∇f(Wk+1)∥2] + 5σ
Lcτ DEεk+1[∥Pk+2∥2]
+ 8
Lcτ σEεk+1[∥Pk+2∥2
∞∥∇f(Wk+1)∥2]
≤ f(Wk) − f ∗ − ατ
√
D
2σ ∥∇f(Wk)∥2 −
 σ
2ατ
√
D
− L
2

∥Wk+1 − Wk∥2
+ ασ
τ
√
D
∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + α∥Pk+1∥2
+ σ2
2Lτ 2D
 
1 − βσ
2τ
√
D

∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 + 4L2τ 3D3/2
βσ3 ∥Wk+1 − Wk∥2
+ 2βσ
τ
√
D
∥Pk+1∥2 + 2β
√
D
τ σ ∥Pk+1∥2
∞∥∇f(Wk)∥2 + 2β2σ2
!
+ 2σ2
Lcτ 2D3/2

(1 − βcσ
2τ )∥Pk+1∥2 + 4βτ
cσ ∥∇f(Wk)∥2 + 4L2βτ
cσ ∥Wk+1 − Wk∥2 + 3β2σ2

+ 2
Lcτ 2√
D

(1 − βcσ
2τ )∥Pk+1∥2
∞∥∇f(Wk)∥2 + 82βL2τ 3D
cσ ∥∇f(Wk)∥2 + L2τ 3
βcσ ∥Wk+1 − Wk∥2

Rearranging inequality (135) above, we have
Eεk+1[Vk+1] − Vk (136)
≤ −
 
ατ
√
D
2σ − 8βσ
Lc2τ D3/2 − 164βLτ
√
D
c2σ
!
∥∇f(Wk)∥2
−
 
σ
2ατ
√
D
− L
2 − 2Lτ
√
D
βσ − 8Lβ
c2σ
√
D
− 2Lτ
βc2σ
√
D
!
∥Wk+1 − Wk∥2
−
 βσ3
4Lτ 3D3/2 − ασ
τ
√
D

∥Pk+1 − τ
√
D
σ ∇f(Wk)∥2 −
 βσ3
Lτ 3D3/2 − α

∥Pk+1∥2
+ β2σ2
L
 σ2
τ 2D + 16σ2
cτ 2D3/2

≤ − ατ
√
D
2σ

1 − 16βσ2
αLc2τ 2D2 − 164βL
αc2

∥∇f(Wk)∥2 + β2σ4
Lτ 2D

1 + 16
c
√
D

where the second inequality holds because the selection of α and β as follows implies the coefficients
of the third to fifth term of RHS are greater than 0
α ≤ σ
6Lτ
√
D
, 8αLτ 2D
σ2 ≤ β ≤ c2σ4
64αLτ 4D . (137)
36

By taking β = 8αLτ 2D
σ2 , we bound
Eεk+1[Vk+1] − Vk (138)
≤ − ατ
√
D
2σ

1 − 128
c2D − 1312L2τ 2D
c2σ2

∥∇f(Wk)∥2 + 8α2Lτ 2D

1 + 16
c
√
D

≤ − ατ
√
D
2σ

1 − 128
c2D − 32P 2
max
τ 2

∥∇f(Wk)∥2 + 8α2Lτ 2D

1 + 16
c
√
D

Taking expectation, averaging (138) overk from 0 to K − 1, and choosing the parameter α as
α = O

 1q
D(1 + 16/(c
√
D))
r
V0
σ2L2K

 (139)
deduce that
E
"
1
K
K−1X
k=0
∥∇f(Wk)∥2
#
(140)
≤ 2σ
 V0 − E[Vk+1]
ατ
√
DK
+ αLτ
√
D

1 + 16
c
√
D
 1
1 − 128/(c2D) − 32P 2max/τ 2
≤ 2σ
 V0
ατ
√
DK
+ αLτ
√
D

1 + 16
c
√
D
 1
1 − 128/(c2D) − 32P 2max/τ 2
= O


r
(f(W0) − f ∗)σ2L
K
q
1 + 16/(c
√
D)
1 − 128/(c2D) − 32P 2max/τ 2

 .
= O
 r
(f(W0) − f ∗)σ2L
K
1
1 − 33P 2max/τ 2
!
where the last inequality holds when D is sufficiently large. The proof is completed.
I Simulation Details and Additional Results
This section provides details about the experiments in Section 2.1 and 5. The analog training
algorithms, including Analog SGD and Tiki-Taka, are provided by the open-source simulation
toolkit AIHWK IT [27], which has Apache-2.0 license; see github.com/IBM/aihwkit. We use
Softbound device provided by AIHWK IT to simulate the asymmetric linear device (ALD), by setting
its upper and lower bound as τ. The digital algorithm, including SGD, and dataset used in this
paper, including MNIST and CIFAR10, are provided by PYTORCH , which has BSD license; see
https://github.com/pytorch/pytorch. Compute Resources. We conduct our experiments
on an NVIDIA RTX 3090 GPU, which has 24GB memory. The simulations take from 30 minutes to
one hour, depending on the size of the model and the dataset.
Statistical Significance. The simulation reported in Figure 5 is repeated three times. The randomness
originates from the data shuffling, random initialization, and random noise in the analog device
simulator. The mean and standard deviation are calculated using statistics library.
I.1 Least squares problem
In Section 2.1 and 5.1, we considers the least squares problem on a synthetic dataset and a ground
truth W ∗ ∈ RD, whose elements are sampled from a Gaussian distribution with mean 0 and variance
σ2
data. Consider a matrix A ∈ RDout×D of size D = 40 and Dout = 100 whose elements are sampled
from a Gaussian distribution with variance σ2
A. The label b ∈ RDout is generated by b = AW ∗ where
W ∗ are sampled from a standard Gaussian distribution with σ2
W ∗.
The problem can be formulated by
min
W ∈RD
f(W ) := 1
2 ∥AW − b∥2 = 1
2 ∥A(W − W ∗)∥2. (141)
37

1.0
 0.5
 0.0 0.5 1.0
Weight Values
0
2
4
6
8Frequently
Digital SGD
Analog GD
Tiki-T aka
0.0 0.2 0.4 0.6 0.8 1.0
Saturation
0
5
10
15
20Frequently
Analog GD
Tiki-T aka
Figure 7: (Left) Distribution on the model weight, i.e. {[WK]i : i ∈ I} . (Right) Saturation
distribution after training, i.e. {|[WK]i|/τ : i ∈ I} .
During the training, the noise with variance level σ2
g is injected into the gradient. For digital SGD
and the proposed Analog SGD dynamic, we add a Gaussian noise with mean 0 and variance σ2
g to the
gradient. For Analog SGD in AIHWK IT, we add a Gaussian noise with mean 0 and variance σ2
g/s2
A
into W when computing the gradient. The constant sA is defined as the mean of the singular value of
A, which is introduced since the noise on W is amplified by A⊤A, whose impact is approximately
proportional to A’s singular value.
In Figure 1, the response step size ∆wmin=1e-4 while τ = 3. The maximum bit length is 800. The
variance are set as σ2
data = 0.32, σ2
A = 1, σ2
W ∗ = 0.452, σ2
g = (0.01/D)2.
In Figure 3, the learning rate is set as α =3e-2. The response step size ∆wmin is set so that the
number state is always 300, i.e. 2τ /∆wmin = 300, when the τ is changing. The maximum bit length
is 300. The variance are set as σ2
data = 0.32, σ2
A = 0.52, σ2
W ∗ = 0.3, σ2
g = 0.1.
I.2 Classification problem
We conduct training simulations of image classification tasks on a series of real datasets. The
gradient-based analog training algorithms, including Analog SGD and Tiki-Taka, are implemented
by AIHWK IT. In the real implementation of Tiki-Taka, only a few columns or rows of Pk are
transferred per time to Wk to balance the communication and computation. In our simulations, we
transfer 1 column every time. The response step size is ∆wmin =1e-3.
The other setting follows the standard settings of AIHWK IT, including output noise (0.5 % of the
quantization bin width), quantization and clipping (output range set 20, output noise 0.1, and input
and output quantization to 8 bits). Noise and bound management techniques are used in [ 61]. A
learnable scaling factor is set after each analog layer, which is updated using digital SGD.
3-FC / MNIST. Following the setting in [15], we train a model with 3 fully connected layers. The
hidden sizes are 256 and 128 and the activation functions are Sigmoid. The learning rates are α = 0.1
for SGD, α = 0.05, β = 0.01 for Analog SGD or Tiki-Taka. The batch size is 10 for all algorithms.
CNN / MNIST. We train a convolution neural network, which contains 2-convolutional layers,
2-max-pooling layers, and 2-fully connected layers. The learning rates are set as α = 0.1 for digital
SGD, α = 0.05, β = 0.01 for Analog SGD or Tiki-Taka. The batch size is 8 for all algorithms.
Resnet / CIFAR10. We train different models from Resnet family, including Resnet18, 34, and 50,
The base model is pre-trained on ImageNet dataset. The learning rates are set as α = 0.15 for digital
SGD, α = 0.075, β = 0.01 for Analog SGD or Tiki-Taka. The batch size is 128 for all algorithms.
I.3 Verification of bounded saturation
To further justify the Assumption 4, we visualize the weight distribution of the trained 3-FC model;
see Figure 7. The results show that despite the absence of projection or saturation, the weights trained
by digital SGD are bounded, which means that it is always possible to find an τ to represent all the
weights in analog devices without being clipped. In the right of Figure 7, Wmax for Analog SGD
could be chosen as 0.2τ. Without constraint on the Wmax, Tiki-Taka has a large Wmax.
38
