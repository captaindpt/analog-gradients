# s41467-024-51221-z.pdf

Article https://doi.org/10.1038/s41467-024-51221-z
Fast and robust analog in-memory deep
neural network training
Malte J. Rasch 1,2 ,F a b i oC a r t a1, Omobayode Fagbohungbe1 &
Tayfun Gokmen 1
Analog in-memory computing is a promising future technology for efﬁciently
accelerating deep learning networks. While using in-memory computing to
accelerate the inference phase has been studied extensively, accelerating the
training phase has received less attention, despite its arguably much larger
compute demand to accelerate. While some analog in-memory training algo-
rithms have been suggested, they either invoke signiﬁcant amount of auxiliary
digital compute—accumulating the gradient in digitalﬂoating point precision,
limiting the potential speed-up—or suffer from the need for near perfectly
programming reference conductance values to establish an algorithmic zero
point. Here, we propose two improved algorithms for in-memory training, that
retain the same fast runtime complexity while resolving the requirement of a
precise zero point. We further investigate the limits of the algorithms in terms
of conductance noise, symmetry, retention, and endurance which narrow
down possible device material choices adequate for fast and robust in-memory
deep neural network training.
Analog in-memory computing AIMC is a promising future hardware
technology for accelerating deep-learning workloads. Great energy
efﬁciency is achieved by representing weight matrices in resistive
elements of crossbar arrays and using basic physical laws of electro-
statics (Kirchhoff’sa n dO h m’s laws) to compute ubiquitous matrix-
vector multiplications (MVMs) directly in memory in essentially con-
stant timeOð1Þ
1–5. Many recent AIMC prototype chip-building efforts to
date have been focused on accelerating the inference phase of deep
neural networks (DNNs) trained in digital
6–12. However, in terms of
compute requirements, the training phase is typically orders of mag-
nitude more expensive than the inference phase, and thus would in
principle have a greater need for efﬁcient hardware acceleration using
in-memory compute
13. However, accelerating the training phase using
AIMC has been challenging, in particular, because of the asymmetric
and non-ideal switching of the memory devices that fail to achieve the
high precision requirements of standard (SGD) algorithms designed
for FP (FP) DNN training (see e.g., ref. 14 for a discussion). Thus,
dedicated AIMC training algorithms are needed that can successfully
train DNNs with the promised AIMC speedup and ef ﬁciency despite
non-ideal device switching characteristics.
To accelerate DNN training in contrast to inference, the back-
propagation of the gradients in SGD, as well as weight gradient com-
putation and weight update itself, have to considered. While the
backward pass of an MVM is straightforwardly accelerated in AIMC by
transposing the inputs and outputs in constant timeOð1Þ, the gradient
accumulation and update onto we ights represented in the con-
ductances of the memory elements is much more challenging. Typical
device materials, such as Resistive Random Access Memory (ReRAM)
15,
Electro-Chemical Random Access Memory (ECRAM) 16,17,a sw e l la s
capacitors as weight elements18, show various degrees of asymmetry
when updating the conductance in one direction versus the other
direction, as well as a gradual saturation to a minimal or maximal
conductance value. Moreover, the device conductance can only ef ﬁ-
ciently be updated in small increments thus making some operations
such as a full reset to a common target conductance prohibitively
expensive. Finally, inherent device-to-device variations make it
Received: 18 December 2023
Accepted: 1 August 2024
Check for updates
1IBM Research, TJ Watson Research Center, Yorktown Heights, NY, USA.2Sony AI, Zürich, Switzerland. e-mail: malte.rasch@gmail.com;
tgokmen@us.ibm.com
Nature Communications|         (2024) 15:7133 1
1234567890():,;
1234567890():,;

challenging to implement many algorithmic ideas that instead inher-
ently assume translational invariance.
One way to get around these challenges is to sacriﬁce speed and
efﬁciency by simply computing the gradient and its accumulation in
digital memory and precision and only accelerate the forward and
backward pass using AIMC, as suggested by Nandakumar et al. 19,20.
However, given thatOðN2Þ digital operations are needed for updating a
weight matrix of sizeN × N, the update phase would not match with the
Oð1Þ character of the MVM in the forward and backward passes and
thus slow down the overall AIMC acceleration of DNN training.
Therefore, Gokmen et al.13 instead suggested to use coincidence
of voltage pulse trains to perform the outer-product and weight
update operations fully in-memory in a highly ef ﬁcient and fully par-
allel manner. This approach has great potential since also the update
phase can then be done in constant time Oð1Þ.U n f o r t u n a t e l y ,w h e n
computing the gradient and directly updating the weight in-memory
with this approach, a bi-directionally switching device of unrealistically
high symmetry and precision is needed
13,21,22. The main problem when
accumulating gradients over time using asymmetric devices with rea-
listic device-to-device variations is that each device will drift in general
towards a different conductance value even in the case when random
ﬂuctuations with zero mean are accumulated and therefore the net
update should be zero and identical for all devices.
However, realizing this issue, follow-up studies
23,24 more recently
suggested to use two additional, separate arrays of non-volatile
memory (NVM) devices to, respectively, accumulate the gradients
separately from the weights and represent predetermined reference
values. It turns out that a differential read of the devices used for the
accumulated gradients and those programmed with the reference
values can statically correct for the effect of the device-to-device var-
iations on the gradient accumulation. Indeed, when additionally
introducing a low-pass digital ﬁltering stage, the requirements of the
number of reliable conductance states and on-device symmetry were
considerably relaxed
24. Furthermore, because only OðNÞ additional
digital operations are needed, the update pass retains very good run-
time complexity and is this efﬁciently accelerated using AIMC.
While this Tiki-Taka version 2 (TTv2) algorithm
24 was also
demonstrated recently in hardware and in simulation using realistic
ReRAM on small tasks 25, several challenges remain in practice. First,
implementing the circuitry for a differential read results in a more
complicated unit cell design as well as signiﬁcant additional chip area
cost for the additional reference devices. Second, the estimation of the
reference conductance values and the programming of the resulting
values has to be done prior to the start of the training, which takes
additional time and effort
26. Finally and most importantly, as we will
show here, even a small deviation of the programmed reference values
from the theoretical values on the order a few percent leads to sig-
niﬁcant accuracy drops during training, thus severely limiting this
approach in practice where much larger programming errors and
l i m i t e dr e t e n t i o na r ec o m m o ni s s u e s .I n d e e d ,e v e ni nt h es t u d y
demonstrating the TTv2 algorithm
25, reference values were repre-
sented in digital values due to test hardware limitations. Moreover,
even if the programming would be perfect, retention of the exact
values over long training times might become problematic. Together,
these issues make the use of the TTv2 algorithm challenging in
practice.
Here, we ﬁrst make a simple improvement to the TTv2 algorithm
to better handle any offsets inﬂicted by an erroneous reference value.
We propose to use the chopper technique
27 in the gradient accumu-
lation to remove any remaining offsets in the reference by periodic or
random sign changes. This Chopped-TTv2 (c-TTv2) algorithm relaxes
the requirement of the reference errors to smaller than about 25%
without signi ﬁcantly altering the runtime in comparison to TTv2.
Secondly, we introduce an altogether different algorithm, Analog
Gradient Accumulation with Dynamic reference (AGAD), that
establishes reference values on-the- ﬂy using a modest amount of
additional digital computing. In this case, the reference values are an
estimate of the recent past of the transient conductance dynamics and
thus independent of any device measurement or device model
assumption. We ﬁnd that both c-TTv2 and AGAD train benchmark
DNNs to state-of-the-art accuracy. In addition, AGAD also greatly
simpliﬁes the hardware design as it does not need a separate con-
ductance array for any reference values, nor any differential read cir-
cuitry. We also show that AGAD broadens the choice of device
materials since both symmetric as well as asymmetric device char-
acteristics can be used, in contrast to TTv2 and c-TTv2, which depend
on devices showing asymmetry. By estimating the expected perfor-
mance, we show that the both proposed algorithms retain the fast
runtime of TTv2, showing two orders of magnitude runtime
improvement to the alternative approach using digital instead of in-
memory gradient accumulation
20.
Finally, we also introduce a dynamic way to set the learning rate to
optimize the gradient accumulations in diverse DNNs, signi ﬁcantly
easing the search for hyper-parameters in practice.
Results
I nt h ef o l l o w i n g ,w ep r e s e n tﬁrst simple toy examples to illustrate and
compare the mechanism of the proposed training algorithms
Chopped-TTv2 (c-TTv2) (Supplementary Alg. 2) and Analog Gradient
Accumulation with Dynamic reference (AGAD) (Supplementary Alg. 3)
to the baseline Tiki-Taka version 2 (TTv2) algorithm (see Fig. 1;t h e
proposed algorithms are described in detail in the “Methods” section
“Fast and robust in-memory training”). Then, we use them to simulate
the training of DNNs with different material and reference offsets
settings. For simulations, we use the PyTorch-based
28 open source
toolkit (AIHWKit)29, where we have implemented the proposed algo-
rithms (see also Supplementary Fig. 4). Finally, we investigate the
projected performance numbers, as well as on-chip memory, and
digital compute, and device material requirements.
Gradient update mechanisms
All here proposed AIMC learning algorithms share the feature that they
use a dedicated array of conductances (that is /C21A) to compute the
gradient accumulation in-memory, while slowly transferring the
accumulated gradients onto the actual weight matrix, which is repre-
sented by another crossbar array of conductances (that is/C21W ) to enable
in-memory acceleration of the forward and backward passes as well.
To illustrate the mechanism of the proposed learning algorithms, we
ﬁrst investigate a simple case where activations are given byx = − X and
gradient inputs by d = αX +( 1 − α)Y where X , Y ∼ Nð0, 1Þ are Gaussian
random variables. Thus, in this case, the correlation of activations and
gradients is given by α and expected average update is only in one
direction Δ /C21w//C0α.
Let’s ﬁrst assume that the reference matrix /C21R used for the differ-
ential read of the accumulated gradients in TTv2 and c-TTv2 (see Fig.1)
is perfectly accurately set to the symmetry point (SP) of /C21A (as illu-
strated in Fig.2) so that no offset remains (see results in Fig.3A–C). For
simplicity, we plot here the conductance values in normalized units,
assuming that the SP is set arbitrarily to zero,/C21a
*/C17 0, and the maximal
and minimal conductance at 1 and − 1, respectively (see “Methods”
section “Device material model ” for details). Note that for TTv2
(Fig. 3A; see “Methods” section “Recap of the Tiki-Taka (version 2)
algorithm”) the trace of a selected matrix element /C21a is strongly biased
towards negative values, thus indicating correctly the direction of the
gradient. It, however, saturates at a certain level, caused by the char-
acteristics of the underlying device model (see Eq. (4)). Because of the
occasional reads (indicated with dot markers), the hidden weight
accumulates until threshold is reached at − 1 (green trace), in which
case the weight /C21w is updated by one pulse (orange trace). The shaded
blue area indicates the instantaneous accumulated gradient value of
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 2

ω = /C21a/C0/C21r. The area would be red if the value was positive, which would
cause the hidden weighth to update in the wrong direction if readout
at that moment.
In Fig. 3B, the behavior of the proposed c-TTv2 algorithm (see
“Methods” section “Chopped-TTv2 algorithm” for details) is shown for
the same inputs. In this algorithm, the gradients are accumulated with
changing signs in either positive or negative directions within a
chopper period. Here, for better illustration, aﬁxed chopper period is
chosen (gray dashed lines). Since the incoming gradient is constant
(negative), the modulation with the chopper sign causes an oscillation
in the accumulation of the gradient on /C21a. However, since the sign is
corrected for during readout, the hidden matrix is updated (mostly) in
the correct direction (blue areas are sign corrected). As we will see
below, this ﬂipping of signs will cancel out any offsets (which are
currently assumed to be 0). If the trace of/C21a has not returned to the SP
before the readouts, it would cause some transient updates of the
hidden weights in the wrong direction (red areas). The weight /C21w is
nevertheless correctly updated on average as the hidden weight
Fig. 2 | Setting the zero-reference point with differential read in Tiki-Taka
version 2 (TTv2) and Chopped-TTv2 (c-TTv2). A Example ReRAM-like device
response traces showing noise and variation in response to bi-directional pulses.
Here we assume that the device gradually saturates with consecutive up or down
pulses (see lower plot for pulse direction applied). Noise properties and update
step sizes can be adjusted in the soft-bounds model Eq. (4) to e.g., reﬂect typical
ReRAM (high noise), capacitor (medium noise, lower variation), or ECRAM (low
noise) traces. B Due to the asymmetry, consecutive (pairwise) up-down pulses
converge the conductance to a ﬁxed point where up and down pulses are on
average of the same size (symmetry point (SP), see Eq. (8)). Because of device-to-
device variation each device has an individual SP value (dashed lines).C When the
SP is estimated for each device of a crossbar array /C21A, it can be programmed on a
separate reference device/C21R. Assuming that the circuitry allows for a matrix-vector
multiplication with differential read, e.g.,y
i = P
j /C21aij/C0/C21rij
/C16/C17
xj , then individual device
responses are effectively set to zero when consecutive up-down pair pulses are
applied.
Fig. 1 | Illustration of gradient update computation steps.The general structure
of the gradient computation is shared for all improved learning algorithms dis-
cussed and is based on Tiki-Taka version 2 (TTv2) (see ref.24). For each input vector
x and backpropagated error vectord the weight gradient isﬁrst accumulated on a
crossbar array /C21A, using a parallel pulsed outer-product update with learning rateλA
(13; see Supplementary Alg. 1). Note that the matrices are here displayed in a
transposed fashion so that voltage inputsx are delivered from the left and d from
the bottom side. Then a single row of the accumulated gradient in /C21A is read out
intermittently everyns vector updates (looping through the rows over time), and
digital computation is used to arrive at a FP vector zk that is added to the digital
storage H with learning rate λH. Finally, the corresponding row of actual weight
matrix, which is represented by a second crossbar array /C21W , is updated when a
threshold is crossed, and the hidden matrixH is reset correspondingly. The newly
proposed algorithms differ in the digital computation to arrive at^x and zk.F o rt h e
TTv2 baseline algorithm, it is ^x/C17 x and zk/C17ð /C21A/C0/C21RÞ vk where the reference
crossbar array /C21R is programmed before DNN training and a fast differential analog
MVM is used for readout (using one-hot unit vectorvk). See“Methods” section“Fast
and robust in-memory training” and Supplementary Fig. 2 for more details on the
digital operations of the proposed algorithms.
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 3

averages out transients successfully. The rate of change of/C21w, however,
is somewhat impacted by the averaging of the transients.
We further propose the AGAD algorithm (Fig. 3C; see “Methods”
section “AGAD algorithm” for details) that uses an (average) value pref
of the recent accumulated gradient /C21a as the reference point (and not
the static SP programmed onto/C21R; see violet line in Fig.3C). The digital
reference value ofpref is changed only when the chopper sign changes
(dashed horizontal lines) and is computed by a leaky average of the
past conductance readouts (p; see red line in Fig. 3C). Because of this
on-the-ﬂy reference computation, this algorithm is not plagued with
the same transients. In fact, the increased dynamical range causes a
faster update of the hidden matrix and subsequently the weight /C21w.
Since in Fig. 3A the reference /C21R was set exactly to the SP of /C21A—as
required for TTv2—the zero point was perfectly set to the ﬁx-point of
the device dynamics
30. In this case, the baseline algorithm TTv2 indeed
works perfectly ﬁne and might be the algorithm of choice, because it
requires least amount of digital computing (as we discuss below).
However, in a more realistic setting when the reference matrix/C21R does
not exactly match the SP, that is programmed instead with an error
offset /C21R /C21a
*/C0μr and μr ≠ 0, the algorithm performs generally poorly.
This is shown in Fig. 3D, where the experiment of Fig. 3A is repeated,
however, now with an offset of μr = −0.8 (blue dashed line in Fig. 3D).
Note that the constant gradient pushes the accumulated gradient /C21a
away from the SP (here at zero) as expected, however, since the
algorithm does subtract the offset programmed on/C21R, the update onto
the hidden matrix is wrong. In fact, hidden weighth (green line) never
reaches the threshold and is net zero in this example instead of
becoming negative as expected (compare to the Fig. 3A).
On the other hand, because of the effect of the chopper sign
changes, even this large offset can be successfully removed with the
c-TTv2 algorithm (Fig.3E). Note that the hidden weighth as well as the
weight /C21w decreases correctly. However, due to the large offset,
noticeable oscillations (red areas) are stressing the accumulation onh,
thus reducing the speed and ﬁdelity of the gradient accumulation. In
case of the AGAD algorithm (Fig. 3 F), the dynamic reference point
computation perfectly compensates any wrong offset, making the
reference device conductance and the programming of the SP alto-
gether unnecessary.
Stochastic gradient descent on single linear layer
While investigating the case of constant gradient input is illustrative
for the accumulation behavior of the learning algorithms, in a more
realistic setting, the incoming gradient magnitude typically depends
on the past update of the weight matrix, thus closing a feedback loop
30.
Therefore, we next test how the algorithms perform when actually
implementing stochastic gradient descent. We ﬁrst consider training
to program a linear layer with outputf
iðxÞ = Pn
j =1 wij xj to a given target
weight matrix ^W .W ed e ﬁne the loss function as the mean squared
deviation from the expected output by using the target weight ^W ,
namely
LðxjW , ^WÞ = 1
2m
Xm
i
f iðxÞ/C0
Xn
j =1
^wij xj
/C16/C17 2
: ð1Þ
Naturally, when minimizing this loss (using SGD) and updatingW,
the deviation is minimized for W = ^W . This problem statement is
similar to the proposal to program target weights for AIMC inference31,
however, we here use our proposed gradient update algorithms to
perform the gradient accumulation in memory instead of using digital
computed gradients.
We set ^W to random values Nð0,0:3Þ and use x
j ∼ Nð0, 1Þ as
inputs. We evaluate the different algorithms by the achieved weight
error ϵ
2
w =hðwij/C0 ^wijÞ2i, that is the standard deviation (SD) of the
learned weights with the target weight. Figure4 shows the results for a
20 × 20 weight matrix after a set amount of inputs with ﬁxed learn-
ing rate.
Fig. 3 | Illustration of the gradient update mechanism of the algorithms
assuming constant (negative) gradient input. A–C Reference conductance (/C21R)
set to the symmetry point (SP) of /C21A without offset (/C21rij = /C21a*
ij ). D-F Reference con-
ductance set to the symmetry point (SP) with added offset (/C21rij = /C21a*
ij/C00:8). A, D Tiki-
Taka version 2 (TTv2) accumulates the gradient onto /C21aij (blue curve) which is
constantly updated in the direction of the net gradient. The hidden weights hij
(green curve) is updated intermittently with the readout of/C21aij (indicated with dots;
here every 25 updates). The weight (orange line) is updated once the threshold is
reached (dotted line). Note that the weight is updated correctly without offset (plot
(A) blue area indicates correctly signed updates), however, with reference offset
(plot (D) blue dashed line) the weight update breaks down.B, E Chopped-TTv2 (c-
TTv2) introduces a chopper (dashed gray lines) that switches the gradient
accumulation direction (here set to regular intervals). Note that weight is correctly
updated without offset (plot (B)), however, a reference offset (plot (E)) causes a
slowdown (but not breakdown) of the weight learning as the offset disturbs the
zero point in one chopper cycle but recovers every other cycle (red areas indicate
wrong sign of the gradient readout due to the offset).C, F (AGAD) introduces an on-
the-ﬂy reference estimation (p
ij; red line) that is copied to the current reference
(pref
ij , violet line) when the chopper changes. Note that in this case the reference is
dynamically adjusted so that weight update is correct without (plot (C)) as well as
with any offset (plot (F)). Parameter settings: 5 × 5 matrix size (onlyﬁrst element is
plotted), δ =0 . 0 5 ,σb = σ± = σd-to-d = σc-to-c =0 . 3 ,γ0 =2 0 0 ,λ =0 . 1 ,ns =5 , β =0 . 5 ,
ρ =0 . 1 ,lmax =5 , λA =1 ,a n dσr =0 .
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 4

We compare the two proposed algorithms (c-TTv2, and AGAD)
with the TTv2 baseline24, as well as with plain in-memory SGD, where
the gradient update is directly done on the weight /C21W (Supplementary
Alg. 1). Additionally, we explore the resilience to two-parameter var-
iations, (1) the magnitude of the offset (by varying the SD of the
reference offsetσ
r across devices), and (2) the number of device states
nstates (see Eq. ( 6)). As the number of states also scales the relative
amount of conductance noise in our model (see “Methods” section
“Device material model”), this variable can be seen as a choice of dif-
ferent device materials, where a low number of states corresponds to
e.g., ReRAM devices, and a high number of states corresponds to e.g.,
ECRAM devices.
As expected in case of no offset σ
r = 0 and in agreement with the
original study 24, the TTv2 algorithm works very well, vastly out-
performing in-memory SGD, in particular for small number of states
(e.g., ϵ
w ≈ 5% vs > 25.0%, respectively, for 20 states and the very same
target weight matrix; see Fig. 4A, B). However, reference offset varia-
tions σr > 0 critically affect the performance of TTv2. As soon asσr ≥ 0.1
(here corresponding to 5% of the weight range of 2), weight errors
increase signiﬁcantly (e.g., to ϵw ≈ 9% for 20 states). This poses chal-
lenges to the usefulness of TTv2 with current device materials because
weight programming errors are generally in the order of at least 5–10%
of the target conductance range for ReRAM (
6, see also Supplementary
Fig. 1B in ref. 32). Thus, the reference /C21R cannot be programmed
accurately enough with the SP of /C21A (see “Methods” section “Recap of
the Tiki-Taka (version 2) algorithm ”) to avoid a signi ﬁcant accuracy
degradation when training in-memory using the baseline TTv2.
Using the concept of choppers in the proposed algorithms c-TTv2
and AGAD, on the other hand, improves the resiliency to offsets dra-
matically (Fig.4C, D). The c-TTv2 algorithm maintains the same weight
error for large offsets when the number of states is small. Offsets in
case of larger number of states are less well corrected, consistent with
the existence of transient decays towards the SP that are the slower the
higher the number of states is (see Eq. (7)). In case of AGAD, reference
offsets simply do not matter, as the reference is dynamically computed
on-the-ﬂy( s e eF i g .4D). Moreover, in contrast to c-TTv2, AGAD works
equally well for higher number of states showing that transients are not
problematic here either.
DNN training experiments
Finally, we compare the different learning algorithms for actual DNN
training. For better comparison, we use largely the same DNNs that
were previously used to evaluate the earlier algorithms. These were a
three-layer fully connected DNN
13, LeNet convnet33 for image classiﬁ-
cation of the MNIST dataset 34, and a two-layer recurrent long short-
term memory (LSTM) network for text prediction of the War and Peace
novel
24,35. We again trained the DNNs with different reference offset
variations (see Fig.5; see Supplementary Methods Sec. C.1 for details)
Fig. 4 | Weight programming error using different learning algorithms.Stan-
dard deviation of the converged analog weights /C21W to the target weights is plotted
in color code. The value of the reference offset device-to-device variationσr is
increased horizontally, while vertically changing the number of material device
states n
states (see Eq. (6)). Less number of states, in general, corresponds to a noisier
conductance response (e.g., for typical ReRAM materials), higher number of states
corresponding to more ideal device conductance responses (e.g., ECRAM).A In-
memory SGD using stochastic pulse trains. B Baseline Tiki-Taka version 2 (TTv2).
C The proposed Chopped-TTv2 (c-TTv2) algorithm.D The proposed (AGAD)
algorithm. Simulation details: Parameter settings as in Fig.3 except thatσr and δ are
varied. Additionally, we setσb = 0 for /C21W only (to not confound results for not being
able to represent the target weight with/C21W )a n ds e tσ± =0 . 1( t oa v o i dal a r g ei m p a c t
of few failed devices on the weight error). The target matrix and inputs areﬁxed for
each case for better comparison. Averaged over three construction seeds of the
device variations.
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 5

with the same challenging device model (see example device response
traces for nstates = 20 in Supplementary Fig. 3). As suggested by
Gokmen24, accuracy for all algorithms could in principle be further
improved and weights could be extracted from the analog devices for
further deployment using stochastic weight averaging not con-
sidered here.
The results of Fig. 5 are very consistent across the three different
DNNs of various topologies (fully connected, convnet, and recurrent
network) and con ﬁrm the trends found in case of the weight pro-
gramming of one layer (compare to Fig. 4): If the offsets are perfectly
corrected for, all algorithms fare very similarly reaching close to FP
accuracy. However, as expected, the impact of a reference offset is
quite dramatic for TTv2, whereas c-TTv2 can largely correct for it until
it becomes too large. On the other hand, AGAD is not affected by the
offsets at all and typically shows best performance (Fig. 5B–D).
We found that even without offsets, both algorithms outperform
the state-of-the-art TTv2. However, this is largely due to the choice of
parameter settings which has larger writing rates onto the /C21A matrix
(l
max = 5). When using reduced rates (lmax = 1) for devices with smaller
number of states, all algorithms are fairly similar (see Supplemen-
tary Fig. 7A).
We further ﬁnd that the gradients are computed so well for the
proposed algorithms in spite of the offsets and transients on/C21A,t h a tt h e
second-order effect of not correcting for the SP of /C21W (as illustrated in
Fig. 2) is becoming prevalent. Indeed, the test error improves beyond
the FP test error for both c-TTv2 and AGAD when the SP of /C21W is
subtracted and thus corrected for (Fig. 5 closed symbols), but
increases somewhat if not (open symbols). AGAD shows better per-
formance over c-TTv2 for larger number of states (Fig. 5A).
Although these three benchmark networks have been used
extensively in previous studies on AIMC training algorithm evaluation,
they are relatively small in terms of free parameters (235 K, 80 K, and
77 K, respectively). Simulating every update pulse for each weight
element accurately in larger networks remains challenging due to
simulation time limitations, in particular when multiple training runs
are necessary for hyper-parameter tuning. However, to conﬁrm whe-
ther the general trend of the effect of a reference value offset on the
v a r i o u sa l g o r i t h m si sp r e s e r v e di nl a r g e rD N N s ,w ec o n d u c t e dab r i e f
training experiment on a vision transformer
36 for classifying the
CIFAR1037 image data set, which is signiﬁcantly larger (4.3M parameter;
see Supplementary Methods Sec. C.1.4 for details). Indeed, even with-
out hyper-parameter tuning, we found that when the reference offset is
not perfectly corrected for, the classiﬁcation error remains markedly
stable only for the proposed algorithmic improvements c-TTv2 and
AGAD but not for TTv2 (see Supplementary Methods Sec. C.1.4 and
Supplementary Fig. 6). This is very consistent with the observed trend
f o rt h es m a l l e rb e n c h m a r kD N N s( c o m p a r et oF i g .5B–D).
Device material requirements
The proposed AIMC training algorithms are in principle agnostic to the
choice of the device material, as long as the devices support incre-
mental bi-directional update. However, each algorithm has certain
requirements on device behavior to successfully converge in the DNN
training. The baseline TTv2 as well as the proposed c-TTv2 algorithm
Fig. 5 | DNN training with different analog learning algorithms. The symmetry
point (SP) of /C21W is either corrected for (closed symbols; compare to Fig. 2)o rn o t
(open symbols with dashed lines). All simulations are done for a ﬁxed amount of
epochs for comparability (see Supplementary Fig. 5 for example traces).A Con-
verged test error (in percent) with a three-layer fully connected DNN on the MNIST
dataset is shown for varying number of device statesn
states using a large reference
offset variation σr = 0.5. Note that the proposed algorithms, Chopped-TTv2
(c-TTv2) and (AGAD), greatly outperform the baseline Tiki-Taka version 2 (TTv2)
across all settings of nstates. Test errors for training in FP precision using standard
SGD are shown as comparison (FP; red dotted line).B–D Reference offset variation
σr versus test error withnstates = 20 for different DNNs. Note that independent of the
DNN the results are very similar to the weight programming task in Fig. 4:T T v 2
essentially does not allow for reference offsets, TTv2 is much more tolerable,
whereas AGAD is invariant against reference offset.A, B 3-layer fully connected
DNN on MNIST.C LeNet on MNIST.D 2-layer LSTM on the War & Peace dataset. See
Supplementary Methods Sec. C.1 for more details on the simulations.
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 6

indeed require asymmetric conductance response that is induced by
the gradual saturation of the update magnitude when approaching the
bounds at least for the/C21A devices (ie. the assumption of the soft-bounds
model Eq. (4) must be valid). This becomes evident when repeating the
same weight programming task of Fig. 4 but now varying the asym-
metry of the devices (see Fig. 6). The asymmetry is changed by
increasing the saturation bounds, but keeping the average update size
δ constant at the SP, which effectively increases the number of states
(see Eq. ( 6)) and causes a more symmetric (linear) pulse response
around the SP (see example responses in Fig.6A, e.g., blue curve versus
orange curve, where the latter has high symmetry in up and down
direction around zero). Note that the weight programming error
sharply improves with higher symmetry for in-memory SGD (see
Fig. 6B, red curve), however, the weight error decreases signi ﬁcantly
for higher symmetry for TTv2 and c-TTv2 (blue and orange curves,
respectively) showing that a certain amount of device asymmetry is
necessary for these algorithms. In contrast, the achieved weight error
of AGAD does not depend on the device asymmetry setting (Fig. 6;
green line), due to its dynamic reference computation. Thus, AGAD is
more widely applicable, supporting both asymmetric material choices
(such as ReRAM) as well as more symmetric devices, such as capacitors
or ECRAM.
Endurance. Another important feature of some NVM device materials
(especially ReRAM materials) is the often limited endurance: after
sending a very large number of voltage pulses the conductance
response diminishes or fails altogether
38. Since we propose to accu-
mulate the gradient using fast in-memory compute, high endurance is
critical. Indeed, if one counts the maximal number of pulses (positive
and negative) for any of the devices used for training a DNN up to
convergence (here LeNeT on MNIST; see Fig. 5)o n e ﬁnds values
between 0.5 to 4 pulses maximally per input sample for the /C21A devices
(depending on the device and hyper-parameter settings for AGAD).
However, different analog crossbar arrays /C21A, /C21R,a n d /C21W (see Fig. 1 and
Supplementary Fig. 2) serve very different functions and thus have
very different endurance requirements. For instance, if one counts the
number of pulses written onto /C21W for the same DNN training simula-
tions, one instead ﬁn d sv a l u e sb e t w e e n2⋅10
−4 and 4 ⋅10−4 pulses
maximally per input sample. Thus, the devices representing the weight
/C21W require 4 orders of magnitudes less number of pulses than those
used for the gradient accumulation/C21A. Given that a typical training data
set can have millions of examples and a fair number of epochs are
typically trained, the endurance of /C21A needs to be very high, whereas
endurance requirements for the device material used for /C21W and /C21R are
much less concerning.
Retention. Similarly, the retention requirements are vastly different
for /C21A, /C21R and /C21W .W eh e r ed eﬁne retention as the time the conductance
level stays nearby the target level without external inputs. For the
reference device/C21R, the retention requirements can be assessed by the
tolerable reference value offset. As seen from the simulations in
Fig. 4B, if the reference value would drift by more than 5% from the
programmed value (in percent of the conductance range, corre-
sponding to σ
r = 0.1), during the time of the training, the TTv2 algo-
rithm will not converge to the desired accuracy. However, for c-TTv2
the retention requirement on /C21R is signiﬁcantly relaxed as the /C21R could
drift up to 25% ( σ
r =0 . 5 ; F i g .4C) within the time needed for training.
However, in practice retention should be much higher, since the
writing of /C21R would need to be refreshed for the next DNN training
leading to inefﬁciencies. Since AGAD is independent of any offsets on/C21R
(Fig. 4D), the programmable reference device is not needed as dis-
cussed above.
The retention requirement for /C21W , on the other hand, is similar for
all algorithms and on the order of the duration for a full training run, as
these devices represent the converged DNN weights.
Interestingly, the retention required for/C21A is signiﬁcantly less than
the duration of the training. As shown in Supplementary Fig. 8, the
required retention duration for /C21A in AGAD is on the order of the
transfer period Nn
s,w h e r eN × N is the assumed matrix size, which in
typical cases corresponds to the time duration the learning algorithm
takes to process on the order of 100 to 1000 input samples. Since the
number of training examples is often on the order of many millions,
the retention requirement of /C21A is orders of magnitudes smaller than
the time it takes to train the DNN. However, because of chip design
considerations/C21R and /C21A likely need to be made of the same material and
the retention requirements for/C21R is considerably higher. Therefore, the
beneﬁt of reduced retention for /C21A can only be exploited for AGAD,
which does not need a programmable reference/C21R.I nt h i sc a s e ,/C21A could
be made of a high endurance but low retention material (or using an
appropriate capacitor).
Performance
In the following, we estimate the expected runtime performance for
the different algorithms as well as needed memory and bandwidth.
Fig. 6 | Varying device asymmetry. Different device materials show different
degrees of asymmetric conductance responses.A different device responds with
varying degrees of asymmetry (changingwmax and ﬁxing the step size). Colors of
the example pulse responses to 200 up and 200 down pulses indicate the asym-
metry device setting. B Weight errors (computed as in Fig. 4) achieved by the
various algorithms depend on the degree of device symmetry. Note that only AGAD
retains a very low error independent of the asymmetry setting (green line).
Asymmetry, typically very detrimental for direct SGD implementation (red line), is
necessary for TTv2 (blue line) as well as c-TTv2 (orange line). This is because the
latter algorithms hinge on the assumption that the conductance quickly returns to
the symmetry point (SP) and the time constant to reach the symmetry point (SP) for
random updates depends on the asymmetry (see Eq. (7)). Error bars indicate
standard errors over 3 construction seeds.
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 7

We focus on evaluating how much time the update pass (including
gradient accumulation) would take on average per input sample,
since other phases, namely forward and backward phases, are iden-
tically shared among all algorithms discussed here. Note that by
focusing on the update performance per input sample, we assume
that the convergence behavior for the different algorithms is not
vastly different in respect to the FP baseline. In other words, we
assume that a similar amount of training epochs are needed to reach
acceptable accuracy. We con ﬁrmed that the number of epochs nee-
ded for convergence is indeed on the same order of magnitude in
respect to the FP baseline in practice (see Supplementary Fig. 5 for
example traces for the data in Fig. 5A), validating our assumption in
ﬁrst-order approximation.
Table 1 lists the detailed runtime estimates and complexities for
the proposed algorithms (see “Methods” section for detailed deriva-
tions). As additional comparison, we have listed the Mixed-Precision
(MP) algorithm
20,w h e r et h eg r a d i e n ta c c u m u l a t i o ni sd o n ei nd i g i t a l l y
using a FP matrix. When an element of this gradient accumulation
matrix reaches a threshold, pulses are sent to the (full) analog weight
matrix /C21W . Thus, the number of FP operations is on the order of
Oð2N
2 + NÞ, as one multiplication and one addition is needed per
matrix element and input sample and additionally one of the input
vectors needs to be scaled with the learning rate. We assume for MP
that writing the full analog weight matrix is only done once per batchB,
so that the analog time needed per input sample is N/Bt
single-pulse for
programming N rows.
As a second baseline, we compare to in-memory SGD (as descri-
bed in “Methods” section “In-memory outer-product update”), which,
however, yielded poor accuracy results in Fig. 5.
When one assumes that a certain amount X of digital compute
throughput is available exclusively for a single analog crossbar array,
then we can estimate the average time (per input sample) the gradient
update step would take. For approximate numbers, we assume that a
single update pulse would take approximately 5 ns, a single MVM
about 40 ns
39, and that the memory operations (Table 1,r o w si nﬁrst
section) can be hidden behind the compute40. In Supplementary Fig. 9,
the average time for an update is plotted against the amount of
available compute. As seen from Table1, if one assumes a state-of-the-
art number of 175 billion FP operations per second (FLOPS) (that is 0.7
TFLOPS
40, shared among 4 crossbar arrays), the proposed algorithms
out-perform the alternative MP algorithm by a large margin, showing
the beneﬁts of AIMC for in-memory gradient update (about 50× faster,
even if one already assumes a batch size of 100, which favors the MP
algorithm). Moreover, computing the gradient in digital requires a
much higher memory throughput for MP (see row “Memory ops” in
Table1), which could be challenging to maintain. Since at most one row
(or column) is processed in digitally for our proposed algorithms per
input, memory bandwidth is not a bottleneck.
Note that for these numbers we have considered a conservative
setting of the hyper-parameters,n
s =2a n d lmax = 5. In fact, the runtime
of the algorithms TTv2, c-TTv2, and AGAD would all converge to the
limit of in-memory SGD with increasing values ofn
s,a st h e i ra d d i t i o n a l
compute all scale with 1
ns
(see Table 1 “FP ops” and “Analog ops”). We
ﬁnd that higher ns numbers are supported, however, accuracy drops
slightly ifns gets too high (depending on the matrix size), if at the same
time, the device number of states is limited (see Supplementary Fig. 7
for the effect of differentn
s settings during DNN training). Note that if
ns increases, the analog devices /C21A have to accumulate and hold the
information for more input samples before being read out. However,
as shown in Supplementary Fig. 7A, DNNs can also be trained with e.g.,
n
s =1 0 a n d lmax = 1 without accuracy loss with certain device char-
acteristics (here nstates = 20). With the same digital throughput
assumptions as above, the expected update time for AGAD in Table 1
would then further reduce to 17.1ns reaching an acceleration factor of
about 175× compared to MP (see Table1; see also Supplementary Fig. 9
for more parameter settings).
Finally, as detailed in the “Methods” section “AGAD algorithm”,
one could also set β = 1 in AGAD which would make the storing and
computing of P unnecessary, saving OðN2Þ storage and Oð3N=nsÞ
compute for the estimation of the leaky average. However, weﬁnd that
accuracy is generally improved when setting β < 1 depending on the
number of available states nstates (see Supplementary Fig. 7, red line
labeled AGAD with β =1 ) .
Discussion
We have introduced two learning algorithms for fast parallel in-
memory training using crossbar arrays. In this approach, the weight
update necessary for the stochastic gradient descent is directly done
in-memory using parallelly pulsed increments for adding the outer
product between the activations and backpropagated error signals to
the weights.
Note that this in-memory training approach is radically different
from hardware-aware training typically employed when using analog
crossbar arrays for DNN inference only (e.g.,
32,41,42). In the latter case,
the DNN weights are (re)-trained in software (using traditional digital
CPUs or GPUs) assuming generic noise sources to improve the noise
robustness. Theﬁnal weights are programmed once onto the analog AI
hardware accelerator which is then used in an inference application
without further training. In contrast, in our study the training of the
weights itself is done by the analog AI hardware accelerator in-memory
on the crossbar arrays, thus opening up the possibility for high energy
efﬁciency during the training of DNNs. Whether inference is then done
with the same hardware using the trained weights depends on the
application. While directly using the trained weights with the same
hardware for inference would be the most efﬁcient, other approaches
are possible as well. For instance, Gokmen
24 suggests extracting the
Table 1 | Complexity and estimated runtime performance of the weight update during DNN training
Algorithm TTv2 c-TTv2 AGAD in-memory SGD MP
Storage [byte] OðN2 + 2NÞ Oð3N2 + 2NÞ Oð2NÞ OðN2 + 2NBÞ
Input loads [bit] Oð16N + 2NlmaxÞ Oð16NÞ
Memory ops [bit] Oð16N=nsÞ Oð18N=nsÞ Oð50N=nsÞ Oð1Þ Oð16N2 =BÞ
FP ops. Oð2N + 2N=nsÞ Oð2N + 6N=nsÞ Oð2NÞ Oð2N2 + NÞ
Analog ops [time] ( lavg +1 /ns) tsingle-pulse+ tMVM/ns lavg tsingle-pulse N/Bt single-pulse
≈∑ Time est. [ns] 56.3 56.3 62.1 30.9 3024.5
Complexity and average runtime of the weight update with input vectorx and gradient input vectord of sizeN (and assuming part of a mini-batch of sizeB). Number comparisons, resetting, signﬂips,
and ceilings are not counted. Here only FP multiplication and additions are counted as operations. The integerns ≥ 1 is the period of transfer. Note thatOð1Þ load and store operations (e.g., counters
and constants) are omitted. An 8 bit FP number format is assumed for all digital numbers. For the time estimates, we assumeN = 512, and FP operations compute time is calculated assuming
throughput of 0.7 TFLOPS (70% utilization of 1 core of40 (1 GFLOPS, FP8) shared for 4 analog crossbars39)a n dns = 2. Memory access time estimates are assumed to be included in the throughput and
hidden with compute40. For the analog compute time, we assume lavg =5 , tMV M =4 0 ns39 and tsingle-pulse=5 ns. We further assume B = 100 for the total time estimate of MP. Stochastic number
generation (possibly in hardware circuitry) is assumed to be parallel and hidden among the other operations.
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 8

trained weights during in-memory training using stochastic weight
averaging in a highly efﬁcient way, so that they can then be used for any
other hardware during inference, including reduced precision digital
inference accelerators. Other analog inference hardware could be used
as well, however, an additional programming error penalty will be
introduced in this case. Nevertheless, given that realistic device noise is
naturally present during our proposed in-memory training, the
resulting weights are likely to be robust to any device noise in a way
similar to the conventional hardware-aware training approach in
software (see e.g., ref. 32).
For our algorithms, we found that the converged accuracy mat-
ches or exceeds the current state-of-the-art in-memory training algo-
rithm TTv2
24. Indeed, in cases where the TTv2 algorithm suffers severe
convergence issues, the proposed algorithms are considerably
improved. In particular, TTv2 suffers if the reference conductance is
not programmed very precisely (within few percent of the con-
ductance range), which hasn ’tb e e nc o n s i d e r e dd u r i n gi t s
conception
24. A precise writing of the reference is very dif ﬁcult to
achieve with current device materials rendering the application of
TTv2 unrealistic, in particular for larger-scale DNN training. Both pro-
posed algorithms, c-TTv2, and AGAD, relax this requirement
signiﬁcantly.
The computational complexity added to TTv2 for the proposed
algorithms is negligible for c-TTv2. While AGAD introduces slightly
more digital compute and storage, the overall runtime is nevertheless
expected to be still orders of magnitude faster than alternatives, where
the gradient matrix is computed in digital and therefore scales with
OðN
2Þ20. Indeed, when estimating the average gradient update time for
a 512 × 512 weight matrix in Table 1 with reasonable assumptions, we
ﬁnd 62.1ns for AGAD versus > 3000 ns when updating the gradient
matrix in digital instead. This large improvement is achieved because
the in-memory update pass uses only linear order of digital operations
(OðNÞ) with the proposed algorithms. Moreover, since the weight is
stored in analog memory, the forward and backward passes can be
accelerated as well. While the MVMs needed for the forward and
backward passes can be accelerated in-memory in constant time
(Oð1Þ), there are, however, typically other utility OðNÞ computations
done in digital besides the mere MVMs. For instance, rescaling of the
input and outputs for improving the AIMC MVM ﬁdelity (see e.g.,
ref. 32 for a discussion), or computing other layers such as af ﬁne
transforms of normalization layers, skip connections, activation func-
tions, that are all part of modern DNNs. Since these utility layers
commonly have at leastOðNÞ runtime complexity, the additionalOðNÞ
digital operations needed for the proposed updated passes will not
change the overall runtime complexity of the full training, which
includes forward, backward, and update passes
24.
We like to emphasize that the reduced number of digital opera-
tions necessary for our AIMC training algorithm, together with the
non-von-Neumann architecture and high energy ef ﬁciency of MVMs
and outer products on the analog crossbar arrays, translates into a
highly energy-efﬁcient approach for DNN training in comparison to
traditional digital ways of compute. While the energy ef ﬁciency per
digital operation has improved over time
43, the complexity of the
memory access and MVM compute still remains bounded by OðN2Þ
and is thus inherently worse than our AIMC approach. Indeed, even
more energy savings could result from co-designing DNNs for
deployment on AIMC architectures, as the scaling laws of the SGD
training is different compared to digital hardware. For instance, a large
and dense matrix multiplication is much less costly on AIMC than on
digital von Neumann hardware, potentially opening up opportunities
for designing novel energy-ef ﬁcient DNN architectures with high
accuracy tailored to AIMC in the spirit of
44.
We here have given a runtime estimate for the gradient update
only instead of a complete estimate of the time needed to train a DNN
on a given chip. A complete estimate has to take into account many
details of the mixed analog-digital chip architecture, as it needs to
consider not only the forward pass computations of all analog and
digital auxiliary layers (as recently shown for an energy estimate for
inference-only AIMC hardware
39), but also the backward pass, and
weight update computations that require intermediately storing of
results (see ref. 24 for a discussion). Therefore, a complete energy
estimate for a full DNN training run has to be based on a speciﬁcA I M C
chip architecture and is thus beyond the current study.
The hallmark of AGAD is to compute the reference value on-the-
ﬂy. Interestingly, even in the ﬁeld of analog ampli ﬁer design, it has
been previously proposed to dynamically compute the zero point
(auto-zero) in conjunction with the chopping technique. This combi-
nation as been shown to have superior performance in challenging
signal-processing application
45. This approach is qualitatively similar
to AGAD that employs both the chopper as well as an on-the- ﬂy
reference.
Note that the reference value computed for AGAD is different
from the reference value programmed onto the conductances /C21R in
case of TTv2 and c-TTv2. In the latter case, the symmetry point (SP) of
/C21A is used as reference together wit h a differential read of both con-
ductances. Consequently, TTv2 and c-TTv2 make in practice quite
restrictive assumptions on the device model, namely that an unique SP
exists, which is moreover stable over time. In contrast, AGAD subtracts
an estimate of the history of the transient conductance value that was
reached before the chopper signﬂipped in digital. This digitally stored
reference value is based on the transient conductance dynamics and
thus independent from any SP assumption. Using this transient on-the-
ﬂy reference value computation is made possible by the introduction
of the chopper that changes the sign and thus the direction of the
information accumulation. Given that the devices have limited con-
ductance range, incoming gradients therefore can use the full
dynamics range effectively.
The on-the-ﬂy reference value computation has several advan-
tages for AIMC DNN training. First, the lengthy estimation and pro-
gramming of the reference arrays/C21R prior to the DNN training run is not
necessary thus simplifying and improving the overall training process.
Second, the chip design is simpli ﬁed as the differential read of two
devices does not needed to be implemented in circuitry. Third, the unit
cell of the crossbar array is simpliﬁed because no individual and pro-
grammable reference for each element in the weight matrix is needed
at all, saving considerably in hardware complexity and chip area cost.
Finally, the AGAD algorithm greatly broadens the device material
choices. The on-the-ﬂy reference estimation enables the computation
on transients, meaning that the average conductance level becomes
irrelevant. This means that both symmetric or asymmetric devices can
be used similarly well for the gradient accumulation. This contrasts
with TTv2 and c-TTv2, which are designed speciﬁcally for and require
asymmetric device conductance responses. Enabling such broad
device material choice is important for future applicability of AIMC for
DNN training. For instance, very high endurance ReRAM (many mil-
lions of pulses) is beyond the current state-of-the-art for this material
choice, however, other material choices exist such as ECRAM, or
capacitors, that essentially have no endurance limit, but have a much
more symmetrical response. We also show that gradient accumulation
material only needs to show very short retention, thus further relaxing
the material requirements of AGAD. In conclusion, we show that both
c-TTv2 and AGAD push the boundary of in-memory training perfor-
mance, while considerably relaxing device material and chip design
requirements, opening a realistic path for accelerating DNN training
using analog in-memory computation.
Method
Analog matrix-vector multiplication
Using resistive crossbar arrays to compute an MVM in-memory has
been suggested early on
46, and multiple prototype chips where MVMs
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 9

of DNNs during inference are accelerated have been recently
described6–9,11,47. In these studies, the weights of a linear layer are stored
in a crossbar array of tunable conductances, inputs are encoded e.g., in
voltage pulses, and Ohm’sa n dK i r c h h o f f’s laws are used to multiply the
weights with the inputs and accumulate the products (Supplementary
Fig. 1A, see also e.g., ref. 1 for more details). In many designs, the
resulting currents or charges are converted back to digital by highly
parallel analog-to-digital converters (ADCs).
For fully in-memory analog training, as suggested in ref. 13,
additionally a transposed MVM has to be implemented for the back-
ward pass, which can be achieved by transposing inputs and outputs
accordingly (see Supplementary Fig. 1B).
Here, we simulate the non-linearity induced by an MVM in the
forward and backward following previous studies
13.W eu s et h es t a n -
dard forward and backward settings in the simulation package
(AIHWKIT)29, which includes output noise, input, and output quanti-
zation, as well as bound and noise management techniques as
described in
33 (see Supplementary Methods Sec. C.1 for the exact AIMC
MVM model settings).
However, we focus on the nonidealities induced by the incre-
mental update of the conductances (as detailed below) which are
typically much more challenging for AIMC training than the MVM
nonlinearities. For instance, it has recently been shown in simulation
that with realistic MVM assumptions many large-scale DNNs can be
deployed without signiﬁcant accuracy drop on AIMC inference hard-
ware when retrained properly
32.
In-memory outer-product update
While accelerating the forward and the backward pass of SGD using
AIMC is promising, for a full in-memory training solution, in-memory
gradient computation and weight update have to be considered for
acceleration as well.
For the gradient accumulation of and N × N weight matrix W of a
linear layer (i.e., computingy = Wx), the outer-product updateW ← W
+ λ dx
T needs to be computed. While this can be done in digital, pos-
sibly exploiting sparseness (e.g., MP, see ref.20), it would still require
on the order ofOðN2Þ digital operations, and doing so would thus limit
the overall acceleration factor obtainable for in-memory training. To
accelerate also the outer-product update to be performed in-memory
and fully parallel, Gokmen & Vlasov
13 suggested to use stochastic pulse
trains and their coincidence (as illustrated in Supplementary Fig. 1C).
The exact update algorithm has gone through a number of
improvements in recent years33,35, however, we here use a yet improved
version in Supplementary Alg. 1. In particular, we suggest to dynami-
c a l l ya d j u s tt h ep u l s et r a i n si nl e n g t hf o rb e t t e re fﬁciency. Note that we
assume in Supplementary Alg. 1 for simplicity of the formulation that a
mixture of negative or positive pulses across inputs x
i are possible,
while in practice, negative and positive pulses are sent sequentially in
two separate phases (setting allxi <0t o0i nt h e ﬁrst phase and allx >0
to zero in the second).
In the “Results” section, we will compare the performance of our
in-memory training algorithms in more detail, and they are partly
based on this outer product. Note that the Supplementary Alg. 1 takes
Oð2NÞ FP operations for each vector update (assuming a vector length
of N) to compute the absolute maximal values that is needed to scale
the probabilities. Then maximallyl
max pulses are given (in each of the
two sequential phases of negative and positive pulses), however, the
dynamical adjustment of the pulse train length (see Supplementary
Alg. 1) leads to only l
avg ≤ lmax pulses on average over input vectors.
Thus, assuming a pulse duration oftsingle-pulse, the runtime complexity
of digital compute of the output product update is Oð2NÞ and the
a v e r a g et i m ef o rt h ea n a l o gp a r ti s2tsingle-pulselavg.F o rt h ep u l s i n g ,
2Nlavg stochastic numbers are generated or 2 Nlmax pre-generated
pseudo-random pulse trains are loaded from memory, and therefore a
complexity of the memory loads is Oð2Nl
maxÞ bits. If one further
assumes that the input and output vectors, x and d need to by tran-
siently stored to compute the pulse probabilities (e.g., in 8-bit FP for-
mat), then the overall memory operations required is on the order
of Oð2Nl
max +1 6NÞ bits.
Previous studies13,33,35 have investigated the noise properties when
using Supplementary Alg. 1 to directly implement the gradient update
in-memory and it turns out that this would require very symmetric
switching characteristics of the memory device elements in particular
for large DNNs
22. Thus, the requirements of such an in-memory SGD
algorithm turns out to be too challenging in face of the asymmetry
observed in today ’s device materials, which we discuss in the next
section.
Device material model
When subject to a large enough voltage pulse, bi-directionally
switching device materials, such as ReRAM 15,E C R A M16,17,o r
capacitors18, show incremental conductance changes. In previous
studies48,49, it was shown that the soft-bounds model characterizes the
switching behavior of such materials qualitatively well. According to
that model, the conductance change g ← g + Δg
D to a single voltage
pulse in either up (D =+o rd o w n D = −) direction is given by
Δg + /C17 α+ gmax/C0g
/C0/C1
Δg/C0/C17 α/C0 gmin/C0g
/C0/C1 ð2Þ
where thus the induced conductance change gradually reduces
towards the conductance bounds. While here the conductance is
measured in physical units, it is more convenient for the following
discussion to (arbitrarily) normalize the conductances. For that, we
ﬁrst set g
half/C0range/C17hgmaxi/C0hgmini
2 where the average is taken over the
individual devices (that in general have individualgmin and gmax values
due to device-to-device variations). Then, we set the normalized
conductance value to /C21w/C17
g/C0hgmini
ghalf/C0range
/C01, so that for a device athgmini the
normalized value is /C21w =/C01, and hgmaxi corresponds to /C21w =1 , a n d
ﬁnallyhgmini +hgmaxi
2 corresponds to /C21w =0 .
Using this normalization, Eq. ( 2) becomes (assuming no device
variations for the moment, i.e., gmax =hgmaxi and gmin =hgmini)
Δ /C21w + /C17 α + 1/C0 /C21w
/C0/C1
Δ /C21w/C0/C17/C0 α/C0 1+ /C21w
/C0/C1 ð3Þ
which corresponds to the soft-bounds model in48 albeit with a different
conductance normalization (here shifted to the range of −1, …,1
instead of 0, …, 1 to ease of discussion of the algorithmic zero point).
We introduce device-to-device variations on the saturation levels
as well as on the slope parameter α and cycle-to-cycle update ﬂuc-
tuations to arrive at the full model
Δ /C21w + /C21wj θ
/C0/C1
/C17 α +
/C21wmax/C0/C21w
/C21wmax
+ σc/C0to/C0cξ
/C16/C17
Δ /C21w/C0 /C21wj θ
/C0/C1
/C17/C0α/C0
/C21wmin/C0/C21w
/C21wmin
+ σc/C0to/C0cξ
/C16/C17 ð4Þ
whereξare standard normal random numbers (drawn for each update)
to model the update ﬂuctuations of strength σc-to-c.H e r ew ec h o s et o
normalize the difference of the actual conductance to the bound by
the bound, that is e.g.,
/C21wmax/C0/C21w
/C21wmax
, so that the update size remains constant
for the same relative distance of /C21w towards the bounds when varying
solely /C21wmin or /C21wmax. Note that in simulations the normalized
conductance values are ensured to be clamped to the saturation
levels (between /C21w
min and /C21wmax) to avoid that the additive noise would
drive the conductance to not supported levels.
In Eq. ( 4), we use the placeholder θ for the hyper-parameters as
deﬁned in the following. To capture device-to-device variability, we
draw random variations during construction according to
/C21wmax =m a xð1+ σbξ1,0Þ and /C21wmin =m i nð/C01+ σbξ2,0Þ where ξi2
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 10

Nð0, 1Þ are random numbers that are different for each device but
ﬁxed during training.
The slope parameters are given by
α + /C17 δγ + ρðÞ
α/C0/C17 δγ /C0ρðÞ ð5Þ
where γ = eσd/C0to/C0d ξ3 ,a n dρ = σ±ξ4,s ot h a tσd-to-d is a hyper-parameter for
the variation of the slope across devices, and σ± a separate device-to-
device variation in the difference of the slope between up and down
direction. The material parameter δ determines the average update
response for one pulse when the weight is at /C21w =0 . W e d eﬁne the
number of device states (for a given ﬁxed setting of the incremental
update noise level σ
c-to-c) by the average weight range divided by δ,
that is
nstates = /C21wmax/C0 /C21wmin
δ : ð6Þ
We found in previous studies that this model of the device-to-
device variationsﬁts ReRAM (array) measurements reasonable well25,50.
Symmetry point. It can easily be seen that for the device model Eq. (4),
the conductance change in response to a positive voltage pulse linearly
depends on the current conductance value and decreases up to the
bound /C21w
max where it becomes zeros. Likewise, the conductance
change linearly decreases for negative updates down to the bound
/C21wmin, and thus the (normalized) conductance /C21w will saturate at /C21wmax
and /C21wmin. Because of this gradual saturating (soft-bounds) behavior,
there exists a conductance value at which the up and down con-
ductance change magnitudes are equal on average, which is called the
symmetry point (SP)
23,30 and denoted as /C21w*.
If one assumes that random up-down pulsing (without a bias in
either direction) is applied to the devices, the device will reach its SP
quickly. This can be easily seen in the case where a positive pulse
always follows a negative pulse. Then the weight change can be written
as (assuming for the moment σ
b = σc-to-c = σ± = σd-to-d =0 ) :
Δ /C21w ≈ Δ /C21w + /C21wj θ
/C0/C1
+ Δ /C21w/C0 /C21wj θ
/C0/C1
=/C02δ /C21w ð7Þ
which shows that for repeated pairs of up-down pulses the weight will
decay exponentially with (approximate) decay rate ofτ=2 δ to a ﬁxed
point at /C21w* =0 .
Solving Eq. (4)f o rt h eS P/C21w* by settingΔ /C21w/C0 /C21w*j θ
/C16/C17
= Δ /C21w + /C21w*j θ
/C16/C17
,
one ﬁnds for the non-degenerated case, i.e., /C21wmax > /C21wmin, α+ >0 , a n d
α− >0 ,
/C21w* = α + /C0α/C0
α +
/C21wmax
/C0 α/C0
/C21wmin
= 2ρ
γ + ρ
/C21wmax
/C0γ/C0ρ
/C21wmin
ð8Þ
Note that some of the AIMC training algorithms discussed in the fol-
lowing will use this SP as a reference value of the gradient
accumulation.
Recap of the Tiki-Taka (version 2) algorithm
In the TTv2 learning algorithm (see Fig. 1 for an illustration), three
tunable conductance elements for each weight matrix element are
required, namely the matrices /C21A, /C21R,a n d /C21W , where we write /C21X for a
weight matrix X that is thought of coded into the conductances of a
crossbar array, to distinguish between matrices that are in digital
memory. The ﬁrst two conductances,/C21A and /C21R,a r eu s e dt oa c c u m u l a t e
the gradient accumulation and storing the SP of /C21A, respectively, and
are read intermittently in fast differential manner/C21A/C0/C21R, whereas /C21W is
used as the representation of the weight W of a linear layer and thus
used in the forward and backward passes. On a functional level, the
algorithm is similar to modern SGD methods that introduce a
momentum term (such as ADAM
51), since also here the gradient isﬁrst
computed and accumulated in a leaky fashion onto a separate matrix
before being added to the weight matrix. However, the analog-friendly
TTv2 algorithm computes and transfers the accumulated gradients
asynchronously for each row (or column) to gain run-time advantages.
Furthermore, crucially, the device asymmetry of the memory element
causes an input-dependent decay of the recently accumulated gra-
dients as opposed to the usual constant decay rate of the momentum
term that is dif ﬁcult to ef ﬁciently implement in-memory (see also
discussion in refs. 24, 30).
While this TTv2 algorithm greatly improves the material speci ﬁ-
cations by introducing low pass ﬁltering of the recent gradients, it
hinges on the assumption that the device has a pre-deﬁned and stable
SP within its conductance range
30.T h eS Pi sd e ﬁned as the con-
ductance value, where a positive and a negative update will result on
average in the same net change of the conductance. Because of the
assumed device asymmetry, the SP acts as a stableﬁxp o i n tf o rr a n d o m
inputs, which causes the accumulated gradient on /C21A to automatically
decay near convergence (see “Methods” section “Symmetry point”).
However, to induce a decay towards zero algorithmically, it is essential
to identify the SP with the zero value for each device, which is achieved
by removing the offset using a reference array /C21R (as illustrated in
Fig. 2). The reference conductance/C21R is thus used to store the SP values
of its corresponding devices of/C21A and instead of directly reading/C21A,t h e
difference /C21A/C0/C21R is read, while only /C21A is updated during training.
Taken together, for TTv2 the reference array/C21R must be set to the
SP of a corresponding analog matrix /C21A prior to the DNN training. The
algorithm of how to program /C21R to the SP in practice is discussed in
ref. 26. It turns out, however, that the programming as well as the SP
estimation is in general subject to errors. To model this error, we set
(with Eq. (8)) the elements of /C21R to
/C21r
ij = /C21w*
ij + ξij ð9Þ
where ξij2 NðμR, σRÞ.T h u s ξij models the remaining error on the
reference device after SP subtraction.
In more mathematical detail, to lower the device requirements for
in-memory SGD, TTv2 computes the outer product update in a fast
manner in-memory and thus accumulate the recent past of the gra-
dients ( dx
T) onto a separate analog crossbar array /C21A,b u ts l o w l y
transfer the recent accumulated gradients by sequential vector reads
of /C21A onto the analog weight matrix /C21W , to counter-act the loss of the
information due to the device asymmetry on the gradient matrix/C21A.S o ,
each vector update, the following three sequential operations are in
principle done (see illustration in Fig. 1):
/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
/ λA dxT
parallel update Supplementary Alg: 1
/C21A
/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
λH /C21A/C0/C21RðÞ vk
every ns an MVM read
hk/C0/C0/C0/C0/C0/C0/C0/C0/C0!
/ hkbc 0 vT
k
write single pulses
/C21W
ð10Þ
The outer-product update onto the analog array /C21A is done using
the stochastic pulse trains and coincidences as described in Supple-
mentary Alg. 1 and is thus essentiallyOð1Þ. For the second step, a row of
/C21A can be read by computing an MVM in-memory by using the corre-
sponding one-hot unit vectorsv
k as input and is thus fast (Oð1Þ). Note
that instead of reading a row as described, one could similarly read out
ac o l u m no f/C21A instead by using the transposed read capability —as is
true for the other algorithms that are described below. To not confuse
the description, we will here explain only the case of rows with the
understanding that instead of rows columns could be processed
as well.
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 11

The resulting FP vectorzk =ð/C21A/C0/C21RÞ vk is multiplied with a learning
rate λH and then added onto the corresponding row of the digital FP
matrix H. The selected rowk could be random, or sequentially iterated
through all rows with wrapped boundaries. Each time a transfer is
made, the absolute vector values ∣hk∣are tested against a threshold
(typically set to 1), and single pulses are used to update the corre-
sponding row of the analog weight matrix /C21W when the threshold is
reached. Thereby the sign of h
ik) is respected (note that we use the
ﬂoor-towards-zero sign hk
/C4/C5
0). This writing of single pulses can be
done in Oð1Þ as only one column is written in parallel.
This TTv2 algorithm (as described in all details in Supplementary
Alg. 2 with ρ = 0) is our baseline comparison.
Overall performance. The average runtime complexity of the TTv2
algorithm per input sample is divided into digital operations (compute
and storage) and time for the analog operations. As detailed in the
“Methods” section “In-memory outer-product update”, the outer pro-
duct into /C21A needs Oð2NÞ digital operations. The additionalOðNÞ scaling
and OðNÞ additions needed for the transfer of the readout of /C21A to the
digital matrix H are only done every ns vector updates and skipped
otherwise, so that the average complexity of digital operations per
input vector sample is Oð2N=n
sÞ. Similarly, the writing onto /C21W is only
executed everyns inputs. Altogether, the average complexity of digital
operations for the full gradient update is thus Oð2Nð1+ 1
ns
ÞÞ.
The average analog runtime per input sample is
2ðlavg + 1
ns
Þ tsingle/C0pulse + 1
ns
tMVM, given that at most 2 pulses (positive
and negative phase) are sent for the write on/C21W and one read (forward
pass) of /C21A has to be performed (with timetMV M)e v e r yns input samples.
Note that although Oð8N2Þ bit memory is needed to store H,o n l y
Oð16N=nsÞ bit memory operations (load and store per input sample)
are needed in addition to those needed the outer product on /C21A (see
Methods section ‘In-memory outer-product update’), as only one row
is operated on for the transfer and writing, which could thus be pre-
fetched and cached efﬁciently.
Fast and robust in-memory training
We propose two algorithms based on TTv2, that improve the gradient
computation in case of any kinds of reference instability or residual
offsets. Both algorithms introduce a technique borrowed from
ampliﬁer circuit design, called chopper
27. A chopper is a well-known
technique to remove any offsets or residuals caused by the accumu-
lating system that are not present in the signal, by modulating the
signal with a random sign-change (the chopper) that is then corrected
for when reading from the accumulator.
Chopped-TTv2 algorithm. While using a reference matrix /C21R has the
advantage to subtract the SP from/C21A efﬁciently using a differential read,
the design choice comes with unique challenges. In particular, the
programming of /C21R m i g h tb ei n e x a c t ,o rt h eS Pm i g h tb ew r o n g l ye s t i -
mated or vary on a slow time scale. As shown in the “Results” section,
any residual offsetso
r/C17 /C21r/C0/C21a* would constantly accumulate onH and
be written onto /C21W thus biasing the weight matrix unwantedly. More-
over, the decay of /C21A to its SP is the slower the more states the device
has and input dependent (see Eq. ( 7)). While feedback from the loss
would eventually change the gradients and correct /C21W , the learning
dynamics might nevertheless be impacted.
For robustness to any remaining offsets and low-frequency noise
sources, we suggest here to improve the algorithm by introducing
choppers. Chopper stabilization is a common method for offset cor-
rection in ampliﬁer circuit design
27. We use choppers to modulate the
incoming signal before gradient accumulation, and subsequently
demodulate during the reading of the accumulated gradient.
In more detail, we introduce choppers cj ∈ { −1, 1} that ﬂip the
sign of each of the activationsxj before the gradient accumulation on/C21A,
that is cjxj (or in vector notation with element-wise product c ⊙ x).
When reading the k-th row of /C21A to be transferred onto H,w ea p p l y
the corresponding chopperck to recover the correct sign of the signal.
Thus, the overall structure of the update remains the same as illustrated
in Fig. 1,h o w e v e r ,i ti sn o ws e t^x/C17 c/C12x and z
k/C17 ck /C21A/C0/C21R
/C16/C17
vk .
In summary, the gradient update now becomes (compare also to
Supplementary Fig. 2)
/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
/ λA dðc/C12xÞT
parallel update Supplementary Alg: 1
/C21A
/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
ck λH /C21A/C0/C21RðÞ vk
every ns an MVM read
hk/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
/ hkbc 0 vT
k
write single pulses
/C21W
ð11Þ
The choppers are ﬂipped randomly with a probability ρ every
read cycle (see Supplementary Alg. 2 for the detailed algorithm). In
this manner, any low frequency component that is caused by the
asymmetry or any remaining offsets and transients on /C21A is not
modulated by the chopper and thus canceled out by the sign ﬂips.
We call this algorithm Chopped-TTv2 (c-TTv2) stochastic gradient
descent.
Overall performance. Since only sign changes are introduced the
c-TTv2 algorithms has largely the same runtime performance num-
bers as the baseline TTv2 (see “Methods” section “Recap of the Tiki-
Taka (version 2) algorithm”). Since applying andﬂipping a sign is very
fast, we omit these operations, however, the current signs must
be loaded and stored every n
s input samples, so that the average
number of memory operations per input sample increases by
2N/ns bits.
AGAD algorithm. While the chopper together with the low-pass ﬁl-
tering greatly improve the resilience to any remaining offsets (see
“Results” section), if offsets become too large simply low-passﬁltering
will not be effective enough.
Moreover, if the training was perfectly inert to any offsetso
r then
the differential read could be replaced by a direct read of /C21A (using
constant reference conductance to balance the currents), which would
signiﬁcantly reduce the chip design complexity and the chip area
needed for /C21R. In addition, the SP of /C21A would neither need to be esti-
mated nor programmed, improving handling in practice.
To address these issues, we suggest the recent history of the
transient conductance dynamics as reference instead of the trouble-
some programming of predetermined values that depend on the
individual device characteristics. In more detail, we propose to use
choppers as in c-TTv2, so again^x/C17 c/C12x in Fig.1, however, now we set
z
k/C17 ck /C21A
0
vk/C0pref
k
/C16/C17
. where we use additional digital compute and
memory to store a digital reference matrix Pref. Note that the readout
from /C21A
0
could be simply a direct readout of /C21A since Pref is used as
reference values. The additional conductance /C21R are thus not needed.
However, to align the comparison with the other algorithms, we use
/C21A
0
/C17 /C21A/C0/C21R in the numerical simulations.
With that, the schematics becomes
/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
/ λA dðc/C12xÞT
parallel update Supplementary Alg: 1
/C21A
/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
ck λH /C21A0vk/C0pref
kðÞ
every ns an MVM read
hk/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0/C0!
/ hkbc 0 vT
k!
write single pulses
/C21W
ð12Þ
To set the digital reference matrix Pref, another digital matrix P is
computed row-by-row as an leaky average of the recent past readouts
of the k-the row of /C21A
0
, i.e., ω/C17 /C21A
0
vk :
pk ð 1/C0βÞ pk/C0βω ð13Þ
where 0 ≤ β ≤ 1 is the time constant of the leaky average. Then the
reference matrix (row) is setpref
k  pk only when the chopper sign ck
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 12

ﬂips. The chopperﬂips could be either randomly (with probabilityρ)o r
at a ﬁxed period of readouts of row k.
The reasoning of Eq. (13) is that the chopperﬂips are unrelated to
the direction of gradient information. Therefore, if a signiﬁcant aver-
age gradient is currently present, the direction of updates onto /C21A has
to change its direction when the chopper ﬂips. Thus, the recent past
values of /C21A before the sign ﬂip can serve as good reference point for
the following chopper period until the next sign ﬂip.
This algorithm is called (AGAD). See Supplementary Alg. 3 and
Supplementary Fig. 2 for implementation details. Note that here two
additional FP matrices P and P
ref need to be stored in local memory.
However, it is possible to reduce the requirement to one matrix Pref if
the leaky average of the recent past Eq. ( 13) is omitted and only the
previous readout is used instead (that is when formallyβ =1i nE q .(13)).
See “Results” section for a discussion of these choices.
Overall performance. The AGAD algorithm only introduces additional
digital compute in the transfer cycle. Thus, the runtime performance
and analog compute of c-TTv2 still holds. However, to subtract the
vector p
ref it needs OðN=nsÞ additional digital operations per input
sample. Moreover, ifβ ≠ 1, then extra Oð3N=nsÞ digital operations (two
scaling and one addition) are needed for updating p.I nt e r m so f
memory, the digital matrix Pref needs 8 ⋅ N2 bits memory storage.
Additionally,P needs 8 ⋅N2 bits memory storage as well if used in case
of β ≠ 1. The number of memory operations per input sample also
increases by Oð8/C12N=nsÞ or Oð8/C14N=nsÞ, respectively, when β =1 o r
β ≠ 1, for loading and storing the additional rows p and pref.
Determining the learning rates
In the original formulation of the TTv2 algorithm24,t h el e a r n i n gr a t eλH
for writing onto the hidden matrix H was not speci ﬁed explicitly
(compare to Fig. 1). We here suggest to use
λH = λ ns n
γ0δ /C21W λA
ð14Þ
where n is the number of rows of the weight matrix, ns the number of
gradient updates done before a single row-read of /C21A,a n d δ /C21W is the
average update response size at the SP of /C21W (see “Methods” section
“Device material model”).
Here λ is the learning rate of the standard SGD, which might be
scheduled. Note that we thus scale H by the overall SGD learning rate
and not the writing onto/C21A. The hyper-parameterγ0 speciﬁes the length
of accumulation, with larger values averaging the read gradients for
longer. Note, however, that the same effect is done by adjusting λ so
that tuning one of both is enough in practice.
Note that a readout of a given matrix element of/C21A happens every
n
s n input vectors (as the rows are sequentially read, see Fig. 1). Thus,
after t input vectors,m =b t
ns nc additions are made to the hidden matrix.
Therefore, we set the learning rate λH in Eq. ( 14) proportional with
λH ∝ ns n to avoid a weight update magnitude dependence on the
potentially different layer sizes across the DNN.
To recover the original gradient magnitudes of the SGD that are
written onto /C21W approximately, the learning rateλH in Eq. (14)i st ob e
divided with λA, which scales the gradient accumulation onto /C21A
(however, note that we drop this dependence again for our empirical
“Results” section, see paragraph “High-noise and high device asym-
metry limit”). The value of λ
A is dynamically adjusted. Since the con-
ductance range is limited, the amount accumulated must be large
enough to cause a signiﬁcant change in the conductances of/C21A.W et h u s
scale the learning rate λ
A appropriately. Since the gradient magnitude
often differs for individual layers, and might also change over time, we
dynamically divide λA by the recent running average μx and μd of the
absolute maximum of the inputs mx =m a xjjxjj and input gradients
md =m a xijdij,r e s p e c t i v e l y .
λA = η0lmaxδ/C21A
μx μd
ð15Þ
Note that mx and md are needed for the gradient update already
(see Supplementary Alg. 1), so that this does not require any additional
computations, except for the scalar leaky average computations. Since
lmaxδ/C21A is approximately the maximum that the device material can
change during one update ( lmax is the number of pulses used, see
Supplementary Alg. 1), the Eq. ( 15) means in case of η0 =1 t h a t a n
element of the weight gradient is going to be clipped ifxidj > μxμd.T h e
default value of η0 is 1, although in some cases higher values improve
learning.
Expected weight update magnitude in limit cases. It is instructive to
investigate theoretically what weight update the algorithms are writing
onto the weight matrix. Let’s ﬁrst assume an ideal device case without
considering any feedback from the loss function in a typical gradient
descent setting. Assume for simplicity that the gradientdx
T is constant
for each n-dimensional input vector x and n-dimensional back-
propagated error vectord,t h a ti sxjdi ≡ g. Thus, aftert (identical) input
vectors, the accumulated change of each weight element should beλgt
(ignoring the sign of the descent), where λ is the SGD learning rate.
Let’s further assume that the learning rate λA (see Eq. ( 15)) is
roughly constant in the period of t updates. According to the algo-
rithms (see Fig.1), each element/C21a of /C21A is read after a period ofnsn input
vectors and would then be/C21ans n = λAgn s n in the ideal device case. Note
that we write /C21at for the value of /C21a after t input vectors. Since the
algorithms will access each element of /C21Am =b t
ns nc times and add
the readout ontoH,t h ev a l u eo ft h ee l e m e n t sh after t input vectors are
ht =λH = Pm
i =1 /C21ains n = Pm
i =1 i /C21ans n = λAgns n Pm
i =1 i. Note that the term
cm/C17 Pm
i =1 i =ðm +1Þ m
2 results from the fact that (in the ideal case) the
devices of /C21A are not saturating or reset between reads.
Thus, we ﬁnd ht = cmλHλAgnsn.W i t hE q .(14)i ti s ht = cm
λ ns n
γ0 δ /C21W
gns n.
Since W is updated withδ /C21W if h >1a n d h is then reset according to the
algorithms, we havewt = cm
λ ns n
γ0
gns n and witht ≈ ns nm ≈ ns n (m +1 )i t
is wt ≈ λ gt t
2γ0
. Note that if one would setγ0 = t
2 the SGD weight update
amplitude is matched. For instance,t c o u l db et h eb a t c hs i z e( t i m e sr e -
use factor for convolutions).
With choppers. However, when we are using a chopper as in c-TTv2
and AGAD then the change of the chopper sign every 1
ρ readouts
essentially resets the gradient accumulation on/C21A. If we correct (divide)
the writing onto H for the k- t hr e a dw i t h i nac h o p p e rc y c l eb yk then
the pre-factorcm becomes just the number of reads int that is m.T h u s ,
for the chopped algorithm (with multiple-read correction) it
is w
t ≈λ gt ns n
γ0
.
High-noise and high device asymmetry limit.I nc a s eo fh i g hd e v i c e
asymmetry and device noise, the accumulation on /C21A quickly decays
(with typical time constant of 1
δ/C21A
,s e eE q .( 7)). Thus, if the readout
interval and device asymmetry is large, i.e., ns n ≫ 1
δ/C21A
,t h e nt h ea c c u -
mulated value is proportional to aﬁltered version of the instantaneous
gradient ans n/ λAhgi with constant c rather than proportional to nsn
as in the ideal device case above. Thus, it is cm ≈ m, and the updated
gradients are wt ≈ λhgi t c
γ0
. Therefore, the nsn dependence drops,
which is the reason for the choice of Eq. ( 14).
In fact, it turns out empirically that theλA dependence of Eq. (14),
which re-scales the update on the weight with the incoming gradient
magnitude, can be dropped as well. Effectively, the learning rate is then
automatically normalized per layer based on the recent average
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 13

gradient magnitude (μxμd), since it is then wt ≈ λhgi t c
γ0
λA/ 1
μx μd
with
Eq. (15). We ﬁnd that this simpliﬁcation works well in practice for our
simulations where we assume noisy ReRAM-like devices (see“Results”
section). However, we also conﬁrmed that one can get similar accuracy
results when adding the dependence ofλA as in Eq. (14) if the constant
γ0 is appropriately adjusted. The latter might be the preferred choice
for larger or more heterogeneous DNNs to not alter the effective
learning rate per layer and the overall dynamics of the learning in
comparison to training in with standard FP SGD.
Data availability
The training and test datasets used for this study are publicly
available
34,37,52. The raw data that support theﬁndings of this study can
be made available by the corresponding authors upon request after
IBM management approval.
Code availability
The full simulation code used for this study cannot be publicly released
without IBM management approval and is restricted for export by the
US Export Administration Regulations under Export Control Classiﬁ-
cation Number 3A001.a.9. However, the open source Apache License
2.0 (AIHWKIT) at https://github.com/IBM/aihwkit implements all
algorithms discussed here
53.
References
1. Sebastian, A., Le Gallo, M., Khaddam-Aljameh, R. & Eleftheriou, E.
Memory devices and applications for in-memory computing.Nat.
Nanotechnol.15,5 2 9–544 (2020).
2. Burr, G. W. et al. Neuromorphic computing using non-volatile
memory. Adv. Phys. X 2,8 9–124 (2017).
3. Haensch, W., Gokmen, T. & Puri, R. The next generation of deep
learning hardware: analog computing.Proc. IEEE 107,1 0 8–122
(2019).
4. Yang, J. J., Strukov, D. B. & Stewart, D. R. Memristive devices for
computing.Nat. Nanotechnol.8,1 3( 2 0 1 3 ) .
5. Sze, V., Chen, Y.-H., Yang, T.-J. & Emer, J. S. Ef ﬁcient processing of
deep neural networks: a tutorial and survey.Proc. IEEE 105,
2295–2329 (2017).
6. Wan, W. et al. A compute-in-memory chip based on resistive
random-access memory.Nature 608,5 0 4–512 (2022).
7. Xue, C.-X. et al. A CMOS-integrated compute-in-memory macro
based on resistive random-access memory for ai edge devices.Nat.
Electron. 4,8 1–90 (2021).
8. Fick, L., Skrzyniarz, S., Parikh, M., Henry, M. B. & Fick, D. Analog
matrix processor for edge ai real-time video analytics. in2022 IEEE
International Solid-State Circuits Conference (ISSCC),V o l .65,
260–262, (2022).
9. Narayanan, P. et al. Fully on-chi p Mac at 14nm enabled by accurate
row-wise programming of pcm-based weights and parallel vector-
transport in duration-format. in2021 Symposium on VLSI Technol-
ogy,1 –2( I E E E ,2 0 2 1 ) .
10. Yao, P. et al. Fully hardware-implemented memristor convolutional
neural network. Nature 577, 641–646 (2020).
11. Le Gallo, M. et al. A 64-core mixed-signal in-memory compute chip
based on phase-change memory for deep neural network infer-
ence. Nat. Electron. 6,1 –14, 2023.
12. Ambrogio, S. et al. An analog-ai chip for energy-ef ﬁcient speech
recognition and transcription.Nature 620,7 6 8–775 (2023).
13. Gokmen, T. & Vlasov, Y. Accele ration of deep neural network
training with resistive cross-point devices: design considerations.
Front. Neurosci.10, 333 (2016).
14. Jain, S. et al. Neural network accelerator design with resistive
crossbars: opportunities and challenges.I B MJ .R e s .D e v .63,
10–1( 2 0 1 9 ) .
15. Zahoor, F., Azni Zulki ﬂi, T. Z. & Khanday, F. A. Resistive random
access memory (RRAM): an overview of materials, switching
mechanism, performance, multilevel cell (MLC) storage, modeling,
and applications.Nanoscale Res. Lett. 15,1 –26 (2020).
16. Tang, J. et al. ECRAM as scalable synaptic cell for high-speed, low-
power neuromorphic computing (IEDM, 2018).
17. Onen, M. Science 377,5 3 9–543 (2022).
18. Li, Y. et al. Capacitor-based cross-point array for analog neural
network with record symmetry and linearity.Proc. 2018 IEEE Sym-
posium on VLSI Technology,2 5–26 (IEEE, 2018).
19. Nandakumar, S. R. et al. Mixed-precision architecture based on
computational memory for training deep neural networks, Proc.
2018 IEEE International Symposium on Circuits and Systems,1 –5
(ISCAS, 2018).
20. Nandakumar, S. R. et al. Mixed-precision deep learning based on
computational memory.Front. Neurosci. 14, 406 (2020).
21. Agarwal, S. et al. Resistive memory device requirements for a neural
algorithm accelerator.Proc. 2016 International Joint Conference on
Neural Networks (IJCNN),9 2 9–938 (IEEE, 2016).
22. Rasch, M. J., Gokmen, T. & Haensch, W. Training large-scale artiﬁcial
neural networks on simulated resistive crossbar arrays.IEEE Des.
Test. 37,1 9–29 (2019).
23. Gokmen, T. & Haensch, W. Algorithm for training neural networks
on resistive device arrays.Front. Neurosci.14, 103 (2020).
24. Gokmen, T. Enabling training of neural networks on noisy hardware.
Front. Artif. Intell.4,1 –14 (2021).
25. Gong, N. et al. Deep learning ac celeration in 14nm cmos compa-
tible RERAM array: device, material and algorithm co-optimization.
Proc. 2022 International Electron Devices Meeting(IEDM), 33–7
(IEEE, 2022).
26. Kim, H. et al. Zero-shifting technique for deep neural network
training on resistive cross-point arrays, arXiv preprint
arXiv:1907.10228, 2019.
2 7 . E n z ,C .C .&T e m e s ,G .C .C i r c u i ttechniques for reducing the effects
of op-amp imperfections: autozeroing, correlated double sampling,
and chopper stabilization.Proc. IEEE
84,1 5 8 4–1614 (1996).
28. Paszke, A. et al. Pytorch: an imperative style, high-performance
deep learning library.Adv. Neural Inf. Process. Syst. 32,2 0 1 9 .
2 9 . R a s c h ,M .e ta l .Aﬂexible and fast pytorch toolkit for simulating
training and inference on analog crossbar arrays.Proc. IEEE Inter-
national Conference on Artiﬁcial Intelligence Circuits and Systems
(AICAS),1 –4( I E E E ,2 0 2 1 ) .
30. Onen, M. et al. Neural network training with asymmetric crosspoint
elements. Front. Artif. Intell. 5, 891624 (2022).
31. Büchel, J. et al. Gradient descent-based programming of analog in-
memory computing cores.Proc. 2022 International Electron Devi-
ces Meeting (IEDM).3 3–1 (IEEE, 2022).
32. Rasch, M. J. et al. Hardware-aware training for large-scale and
diverse deep learning inference workloads using in-memory com-
puting-based accelerators.Nat. Commun. 14, 5282 (2023).
33. Gokmen, T., Onen, M. & Haensch, W. Training deep convolutional
neural networks with resistive cross-point devices.Front. Neurosci.
11,5 3 8( 2 0 1 7 ) .
34. Deng, L. The mnist database of handwritten digit images for
machine learning research.IEEE Signal Process. Mag. 29,
141–142 (2012).
35. Gokmen, T., Rasch, M. J. & Haensch, W. Training LSTM
networks with resistive cross-point devices. Front. Neurosci.
12, 745 (2018).
36. Lee, S. H. Lee, S. & Song, B. C. Improving vision transformers to
learn small-size dataset from scratch.IEEE Access 10,
123212–123224 (2022).
37. Krizhevsky, A. et al. Learning Multiple Layers of Features from Tiny
Images (University of Toronto, 2009).
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 14

38. Chen, Y. Reram: history, status, and future. IEEE Trans. Electron
Devices 67,1 4 2 0–1433 (2020).
39. Jain, S. et al. A heterogeneous and programmable compute-in-
memory accelerator architecture for analog-ai using dense 2-d
mesh. IEEE Trans. Very Large Scale Integr. (VLSI) Syst.31,
114–127 (2022).
4 0 . L e e ,S .K .e ta l .A7 - n mf o u r - c o r em i x e d - p r e c i s i o na ic h i pw i t h2 6 . 2 -
tﬂops hybrid-fp8 training, 104.9-tops int4 inference, and workload-
aware throttling.IEEE J. Solid-State Circuits57,1 8 2–197 (2022).
41. Bhattacharjee, A., Moitra, A., Kim, Y., Venkatesha, Y. & Panda, P.
Examining the role and limits of batchnorm optimization to mitigate
diverse hardware-noise in in-memory computing.Proc. of the Great
Lakes Symposium on VLSI 2023.( G L S V L S I’ 2 3 ,A C M ,2 0 2 3 ) .
42. Meng, J. et al. Temperature-res ilient rram-based in-memory com-
puting for dnn inference. IEEE Micro 42,8 9–98 (2022).
43. Shankar, S. & A. Reuther, Trends in energy estimates for computing
in ai/machine learning accelerators, supercomputers, and
compute-intensive applications.Proc. 2022 IEEE High Performance
Extreme ComputingConference (HPEC).( I E E E ,2 0 2 2 ) .
44. Zhou, Y. et al. Rethinking co-design of neural architectures and
hardware accelerators. (2021).
45. Moghimi, R. To chop or auto-zero: that is the question, Analog
Devices Technical Note, MS-2062, (2011).
4 6 . S t e i n b u c h ,K .D i eL e r n m a t r i x .Kybernetik1,3 6–45 (1961).
47. Khaddam-Aljameh, R. et al. Hermes core –a 14nm cmos and pcm-
based in-memory compute core using an array of 300ps/lsb line-
arized cco-based adcs and local digital processing.Proc. 2021
Symposium on VLSI Circuits.1 –2( I E E E ,2 0 2 1 ) .
48. Fusi, S. & Abbott, L. Limits on the memory storage capacity of
bounded synapses.Nat. Neurosci. 10,4 8 5–493 (2007).
49. Frascaroli, J., Brivio, S., Covi, E. & Spiga, S. Evidence of soft bound
behaviour in analogue memristive devices for neuromorphic com-
puting. Sci. Rep. 8,1 –12 (2018).
50. Stecconi, T. et al. Analog resistive switching devices for training
deep neural networks with the novel tiki-taka algorithm,Nano
Lett.2 0 2 4 .
51. Kingma, D. P. & Ba, J. Adam: a method for stochastic optimization. In
Proc. International Conference on Learning Representations (ICLR)
(ICLR, 2014).
52. Tolstoy, L., War and Peace.P . O .B o x2 7 8 2 ,C h a m p a i g n ,I L6 1 8 2 5 -
2782, (USA: Project Gutenberg, 1869).
5 3 . R a s c h ,M .J .e ta l . ,I B MA n a l o gH a r d w a r eA c c e l e r a t o rK i t0 . 9 . 1 ,IBM/
aihwkit
, https://doi.org/10.5281/zenodo.11205174,2 0 2 4 .
Acknowledgements
We thank the IBM Research AI HW Center and RPI for access to the
AIMOS supercomputer, and the IBM Cognitive Compute Cluster for
additional compute resources. We would like to thank Takashi Ando,
Hsinyu (Sydney) Tsai, Nanbo Gong, Paul Solomon, and Vijay Narayanan
for fruitful discussions.
Author contributions
M.J.R. and T.G. conceived the study; M.J.R. conceived the AGAD algo-
rithm, T.G. conceived the c-TTv2 algorithm. M.J.R. conducted all
experiments and analyses, except the LSTM training experiments, done
by F.C., the vision transformer experiments, done by O.I.F., and the
MNIST-CNN experiments, done by M.J.R. and O.I.F.; M.J.R. wrote the
manuscript.
Competing interests
The authors declare no competing interests.
Additional information
Supplementary informationThe online version contains
supplementary material available at
https://doi.org/10.1038/s41467-024-51221-z.
Correspondenceand requests for materials should be addressed to
Malte J. Rasch or Tayfun Gokmen.
Peer review informationNature Communicationsthanks Sadasivan
Shankar, and the other, anonymous, reviewer(s) for their contribution to
the peer review of this work. A peer review ﬁle is available.
Reprints and permissions informationis available at
http://www.nature.com/reprints
Publisher’s note Springer Nature remains neutral with regard to jur-
isdictional claims in published maps and institutional afﬁliations.
Open Access This article is licensed under a Creative Commons
Attribution 4.0 International License, which permits use, sharing,
adaptation, distribution and reproduction in any medium or format, as
long as you give appropriate credit to the original author(s) and the
source, provide a link to the Creative Commons licence, and indicate if
changes were made. The images or other third party material in this
article are included in the article’s Creative Commons licence, unless
indicated otherwise in a credit line to the material. If material is not
included in the article’s Creative Commons licence and your intended
use is not permitted by statutory regulation or exceeds the permitted
use, you will need to obtain permission directly from the copyright
holder. To view a copy of this licence, visithttp://creativecommons.org/
licenses/by/4.0/.
© The Author(s) 2024
Article https://doi.org/10.1038/s41467-024-51221-z
Nature Communications|         (2024) 15:7133 15
