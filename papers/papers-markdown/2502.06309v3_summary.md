# Analog In-memory Training on General Non-ideal Resistive Elements: The Impact of Response Functions

**Authors:** Zhaoxian Wu, Quan Xiao, Tayfun Gokmen, Omobayode Fagbohungbe, Tianyi Chen
**Institutions:** Cornell University (formerly RPI), IBM T. J. Watson Research Center
**Year:** 2025
**Paper ID:** 2502.06309v3

## Core Thesis

This paper provides **the first rigorous theoretical foundation for gradient-based training on generic non-ideal response functions** in analog in-memory computing (AIMC) hardware. The authors prove that asymmetric response functions create an **implicit penalty** that biases training, propose a **Residual Learning** algorithm framework that provably achieves exact convergence by aligning algorithmic stationary points with physical symmetric points, and extend the approach to handle limited granularity and noisy I/O.

## The Central Problem

**Observation:** Analog devices have **asymmetric and non-linear response functions** q+(w) and q-(w) that vary with the current weight value and update direction.

**Key challenges:**
1. **Asymmetric update:** q+(w) ≠ q-(w) causes biased gradients
2. **Non-linear response:** Response functions depend on current weight state
3. **Limited granularity:** Δw_min sets minimum update precision
4. **Noisy I/O:** Analog-digital conversion introduces errors
5. **Device variations:** Element-to-element differences

**The gap:** Previous theoretical work (Paper 7) limited to special linear response functions. Generic response functions require new analysis.

## Technical Approach

### Mathematical Model of Analog Update

**Pulse update mechanism:**
- Weight w receives electrical pulses
- Each pulse changes conductance by Δw_min scaled by response function q_s(w)
- Direction s ∈ {+, -} determines which response function applies

**Proposed discrete-time dynamic:**
```
Analog SGD: W_{k+1} = W_k - α∇f(W_k; ξ_k) ⊙ F(W_k) - α|∇f(W_k; ξ_k)| ⊙ G(W_k)
```

Where:
- **F(W) = (Q-(W) + Q+(W))/2**: Symmetric component (average response)
- **G(W) = (Q-(W) - Q+(W))/2**: Asymmetric component (response difference)
- **Extra bias term:** -α|∇f| ⊙ G(W) doesn't appear in digital SGD

**Response Function Class (Definition 1):**
Properties that reflect physical constraints:
1. **Positive-definiteness:** q+(w) > 0 for w < τ_max, q-(w) > 0 for w > τ_min
2. **Differentiable:** Smooth response curves
3. **Saturation:** q+(τ_max) = q-(τ_min) = 0 (bounded dynamic range)

**Key concepts:**
- **Symmetric point w^⋄:** Where q+(w^⋄) = q-(w^⋄), analog update behaves like digital
- **Dynamic range:** [τ_min, τ_max] where weights can exist
- **Saturation degree:** Measured by ratio √(G(W)/F(W))

### Problem 1: Implicit Penalty (Theorem 1)

**Discovery:** Analog SGD doesn't optimize f(W), but implicitly optimizes:
```
min_W f_Σ(W) := f(W) + ⟨Σ, R_c(W)⟩
```

Where:
- **R_c(W):** Accumulated asymmetric function with derivative R(W) = G(W)/F(W)
- **Σ:** Noise level from mini-batching (constant for simplification)
- **Penalty coefficient:** Proportional to noise variance σ²

**For special linear response (q+(w) = 1 - w/τ, q-(w) = 1 + w/τ):**
```
f_Σ(W) = f(W) + Σ/(2τ) ||W||²
```
This is ℓ2 regularization pulling weights toward 0!

**Key insight:** The asymmetric bias term -α|∇f| ⊙ G(W) creates drift toward symmetric point w^⋄. At a critical point where E[∇f(W_k)] = 0:
```
E[W_{k+1}] = W_k - αE[|∇f(W_k; ξ_k)|] ⊙ G(W_k) ≠ W_k
```

The drift is proportional to noise level and asymmetry degree.

### Problem 2: Inexact Convergence of Analog SGD (Theorem 2)

**Convergence bound:**
```
(1/K) Σ E[||∇f(W_k)||²] ≤ O(√(σ²/K)) + σ²S^{ASGD}_K
```

**Two error terms:**
1. **Transient error:** O(√(σ²/K)) - vanishes with iterations (standard SGD rate)
2. **Asymptotic error:** σ²S^{ASGD}_K - DOES NOT vanish!

**Amplification factor:**
```
S^{ASGD}_K = (1/K) Σ_{k=0}^{K-1} ||√(G(W_k))/√(F(W_k))||²_∞
```

**Interpretation:**
- If W* (optimal) is far from w^⋄ (symmetric point): Large S^{ASGD}_K → large asymptotic error
- If W* ≈ w^⋄: Small asymptotic error
- **No amount of training eliminates this error** for generic response functions

**This generalizes Paper 7's result for special linear responses to arbitrary response functions.**

## Solution: Residual Learning

### Core Idea: Align Stationary and Symmetric Points

**Problem:** Algorithmic stationary point W* and physical symmetric point w^⋄ don't match.

**Solution:** Create a system where stationary point is exactly at the symmetric point (zero).

**Bilevel optimization formulation:**
```
Residual Learning: arg min_W ||P*(W)||²
                   s.t. P*(W) ∈ arg min_P f(W + γP)
```

**Interpretation:**
- **Main array W_k:** Stores model weights
- **Residual array P_k:** Tracks residual P*(W) = (W* - W)/γ
- At optimum: W = W* implies P* = 0
- Since 0 is symmetric point: G(0) = 0 → no asymmetric bias!

### Algorithm

**Update rules (alternating):**
```
P_{k+1} = P_k - α∇f(W̄_k; ξ_k) ⊙ F(P_k) - α|∇f(W̄_k; ξ_k)| ⊙ G(P_k)  (residual update)
W_{k+1} = W_k + βP_{k+1} ⊙ F(W_k) - β|P_{k+1}| ⊙ G(W_k)              (transfer step)
```

Where:
- **W̄_k = W_k + γP_k:** Mixed weight for gradient computation
- **γ > 0:** Mixing coefficient (non-zero is critical!)
- **α:** Learning rate for residual
- **β:** Transfer rate (much smaller than α)

**Key innovations:**
1. **Gradient on mixed weight:** ∇f(W_k + γP_k) guides residual update
2. **P_k tracks residual:** P_k → P*(W_k) = (W* - W_k)/γ → 0 as W_k → W*
3. **Zero-shifting:** Requires G(0) = 0 (implemented via reference array subtraction)
4. **Bilevel structure:** Naturally leads to two-array architecture like Tiki-Taka

### Theoretical Guarantee (Theorem 3)

**Convergence bound:**
```
E^{RL}_K ≤ O(√(σ²/K)) + σ²S^{RL}_K
```

Where:
```
S^{RL}_K = (1/K) Σ_{k=0}^K ||√(G(P_k))/√(F(P_k))||²_∞
```

**Key difference from Analog SGD:** S^{RL}_K depends on **P_k** not W_k!

Since P_k → 0 (by design) and G(0) = 0 (by zero-shifting):
```
||G(P_k)/√F(P_k)||_∞ ≤ L_S ||P_k||_∞ → 0
```

**Under Assumption 5 (zero-shifted symmetric point):**

**Corollary 1:** If γ ≥ Ω(R^{-1/5}_min), then:
```
E^{RL}_K ≤ O(√(σ²L/K))    (EXACT CONVERGENCE!)
```

The asymptotic error vanishes! This is **provably better** than Analog SGD.

## Extension: Residual Learning v2

Handles two additional hardware imperfections:

### 1. Noisy I/O

**Problem:** Reading P_{k+1} introduces noise ε_k during transfer:
```
W_{k+1} = W_k + β(P_{k+1} + ε_k) ⊙ F(W_k) - β|P_{k+1} + ε_k| ⊙ G(W_k)
```

This reintroduces implicit penalty ||P*(W)||² + Σ_ε||W||²/2!

**Solution:** Digital buffer H_k with moving average:
```
H_{k+1} = (1 - β)H_k + β(P_{k+1} + ε_{k+1})
```

Then transfer from H_k instead of P_k:
```
W_{k+1} = W_k + βH_{k+1} ⊙ F(W_k) - β|H_{k+1}| ⊙ G(W_k)
```

**Effect:** H_k → P_k with radius O(β), filtering out high-frequency noise.

### 2. Limited Response Granularity

**Problem:** When α∇f < Δw_min, pulse update cannot provide sufficient precision.

**Solution:** Threshold-based transfer rule:
- Only transfer H_k to W_k when elements exceed Δw_min
- Accumulate small updates in H_k until threshold met
- Reduces instability from quantization

## Experimental Results

### MNIST (FCN & CNN)

**FCN (3 fully-connected layers: 784-256-128-10):**
- Power response with γ_res = 0.5, varying τ
- Analog SGD: 93-97% (depending on τ), slow convergence
- Tiki-Taka (= Residual Learning): 97% consistently, <1% drop from digital

**CNN (2 conv + 2 pooling + 2 FC):**
- Analog SGD: ~80% accuracy (significant drop)
- Tiki-Taka: 98.8% accuracy (~0.3% drop from 99.1% digital)

**Key finding:** Residual Learning consistently outperforms Analog SGD across architectures.

### CIFAR-10/100 (ResNet family)

**ResNet18 on CIFAR-10:**
- Digital SGD: 95.43%
- Analog SGD: 84.47% (11% drop!)
- Residual Learning: 94.81%
- Residual Learning v2: **95.31%** (only 0.1% drop!)

**ResNet50 on CIFAR-10:**
- Digital: 96.57%
- Analog SGD: 94.36%
- Residual Learning v2: **96.63%** (exceeds digital!)

**ResNet18 on CIFAR-100:**
- Digital: 81.12%
- Analog SGD: 68.98% (12% drop)
- Residual Learning v2: **79.83%** (1.3% drop)

**Key finding:** Residual Learning v2 with digital buffer and threshold transfer achieves near-digital accuracy even on challenging datasets.

### Ablation Studies

**1. Cycle variation (σ_c from 10% to 120%):**
- Both Analog SGD and Tiki-Taka largely unaffected
- Validates Theorem 4: cycle variation is higher-order error

**2. Various response functions (power/exponential with different τ, γ_res):**
- Analog SGD fails (<15% accuracy) for steep response curves
- Tiki-Taka maintains >96% across all tested configurations
- Demonstrates robustness to diverse hardware characteristics

**3. Mixing coefficient γ:**
- γ = 0: Poor performance (as shown in Figure 2 counterexample)
- γ ≥ 0.1: Saturated performance gain
- **Non-zero γ is critical** for Residual Learning success

## Connections to Other Papers

### Paper 1 (First Memristor Training, 2017):
- Paper 1: Empirical on-chip training demonstration
- **This paper:** Rigorous theory explaining when/why it works
- **Complementary:** Practice meets theory

### Paper 4 (AGAD/Chopped-TTv2, 2024):
- Paper 4: Empirical c-TTv2 with gradient accumulation
- **This paper:** Proves Tiki-Taka (basis of TTv2) achieves exact convergence
- **This provides theoretical justification** for Paper 4's approach

### Paper 5 (Bulk-Switching Memristor, 2023):
- Paper 5: Mixed-precision with 500× fewer writes
- **This paper:** Residual Learning also reduces main array updates (factor βγ)
- **Common theme:** Reduce frequency of noisy analog operations

### Paper 6 (Physical Neural Networks, 2024):
- Paper 6: DSTD for spike-time-based training on CMOS
- **This paper:** Residual Learning for rate-based training on crossbars
- **Contrast:** Time-domain vs. weight-domain; both embrace hardware physics

### Paper 7 (Tiki-Taka Theory, 2024):
- Paper 7: Theory for **special linear** response functions only
- **This paper:** Theory for **generic** response functions (power, exponential, non-monotonic)
- **Major extension:** Shows Tiki-Taka fails for general linear (Figure 2 counterexample)
- **New framework:** Residual Learning via bilevel optimization

### Paper 8 (Photonic Neuromorphic, 2025):
- Paper 8: Fully analog photonic system with MGD
- **This paper:** Fully analog electronic crossbars with Residual Learning
- **Common theme:** Direct physical training without CPU dependency

## Relevance to "Bizarre that we use deterministic silicon to simulate learning"

This paper **mathematically proves** why forcing analog hardware to run digital algorithms fails:

**The Core Issue:**
1. Analog devices have asymmetric response: q+(w) ≠ q-(w)
2. This asymmetry interacts with gradient noise to create bias
3. **The bias cannot be eliminated by standard techniques** (reducing learning rate)
4. Lower bound (if extended to generic responses) would prove fundamental limitations

**The Insight:**
- Don't fight asymmetry—**design algorithms that exploit it**
- **Residual Learning** uses asymmetric bias to auto-regulate auxiliary array near 0
- Zero is symmetric point by construction (zero-shifting)
- This transforms asymmetry from bug to feature!

**Mathematical Proof of Necessity:**
```
Analog SGD:    asymptotic error = σ²S^{ASGD}_K  (unavoidable for generic responses)
Residual Learning: asymptotic error = 0          (provably achievable)
```

**The difference is algorithm-hardware co-design.**

**Key Equation for Linear Response:**
```
Implicit objective: f(W) + Σ/(2τ)||W||²
```
This shows analog hardware **naturally implements regularization**—we should work with it, not against it!

## Technical Innovations

### 1. Generic Response Function Framework

**Covers:**
- Linear: q±(w) = 1 ± w/τ
- Power: q±(w) = (1 ± w/τ)^γ
- Exponential: q±(w) = (exp(γ(1 ± w/τ)) - 1)/(exp(γ) - 1)
- Non-monotonic functions (Figure 3)

**Definition 1 properties capture essential physics:**
- Saturation at boundaries
- Positive responses
- Differentiability for analysis

### 2. Implicit Penalty Discovery

**First to show:** Analog training implicitly solves penalized problem f(W) + ⟨Σ, R_c(W)⟩

**Implications:**
- Explains why Analog SGD drifts toward symmetric points
- Penalty coefficient ∝ noise level (more noise = stronger drift)
- Inverse ∝ dynamic range τ (larger range = weaker penalty)

### 3. Bilevel Optimization Formulation

**Novel framework:**
```
min_W ||P*(W)||²  s.t.  P*(W) ∈ arg min_P f(W + γP)
```

**Advantages:**
- Stationary point P* = 0 by design
- Natural two-array architecture
- Rigorous convergence proof possible
- Extends naturally to handle other imperfections

### 4. Zero-Shifting Technique

**Implementation:** Subtract reference array to ensure G(0) = 0

**Critical for exact convergence:**
- Without zero-shifting: S^{RL}_K may not vanish
- With zero-shifting: ||G(P_k)|| ≤ L_S||P_k|| → 0

### 5. Extension Framework

**Handles:**
- Noisy I/O: Digital buffer H_k
- Limited granularity: Threshold-based transfer
- Device variations: Element-specific response functions
- **Modular design** allows addressing imperfections individually

## Limitations and Future Work

**Current Limitations:**
1. **Strong convexity assumption:** Required for unique P*(W), can be relaxed
2. **Two arrays required:** 2× hardware cost (but Paper 5 shows mixed-precision can reduce this)
3. **Analysis limited to three imperfections:** Other issues (IR drop, stuck devices) remain
4. **Simulation only:** No actual hardware validation yet

**Addressed by:**
1. Bilevel optimization theory advancing (refs 40-43)
2. Hybrid approaches combining with Paper 5's techniques
3. Framework extensible to more imperfections
4. AIHWKIT simulation closely matches hardware (ref 44)

**Future Directions:**
1. **Relax strong convexity** to general non-convex objectives
2. **Extend to other algorithms:** Momentum, Adam, etc.
3. **Analyze more device types:** PCM, ECRAM, MRAM, FTJ, etc. (Appendix A lists many)
4. **Hardware validation:** Test on actual chips
5. **Scaling studies:** Networks with 1000+ layers
6. **Integration with Paper 8's photonic approach:** Hybrid electronic-photonic systems

## Why This Matters

**For Theory:**
- First rigorous convergence theory for generic analog response functions
- Proves exact gradient-based training achievable despite hardware imperfections
- Establishes bilevel optimization as framework for analog algorithm design

**For Practice:**
- Explains when/why Tiki-Taka works (and when it doesn't—Figure 2)
- Provides design guidelines for new analog training algorithms
- Shows how to systematically address hardware imperfections

**For Hardware:**
- Don't need perfect symmetric devices (expensive!)
- Can use manufacturable devices with asymmetric responses
- Zero-shifting technique is practical (reference array)

**For Future Systems:**
- Path to training billion-parameter models on analog hardware
- Energy efficiency gains: 1000×+ over digital (from Papers 1-8)
- Enables edge training without cloud connectivity

## The Big Picture

This paper **completes the theoretical foundation** for analog training:

**Old Understanding:**
- Asymmetric devices → biased gradients → bad performance
- Solution: Make devices more symmetric (expensive!)

**New Understanding:**
- Asymmetric devices → implicit penalty → predictable bias
- Solution: Design algorithms (Residual Learning) that exploit asymmetry
- **Hardware imperfections become algorithmic features**

**The Achievement:**
```
Analog SGD (digital algorithm on analog hardware):
    Error = O(√(σ²/K)) + σ²S^{ASGD}_K    ← Asymptotic error persists

Residual Learning (analog-aware algorithm):
    Error = O(√(σ²/K))                   ← Exact convergence!
```

**Connection to Central Theme:**

This paper **mathematically validates** the analog computing philosophy:

1. **Don't simulate digital on analog** (Analog SGD has unavoidable error)
2. **Design algorithms for analog physics** (Residual Learning achieves parity)
3. **Co-design hardware and algorithms** (zero-shifting + bilevel optimization)
4. **Noise can be beneficial** (helps stabilize auxiliary array)
5. **Imperfections drive innovation** (asymmetry → residual learning idea)

**The Paradigm Shift:**
```
Old: Analog hardware tries to be digital → Fails (provably!)
New: Analog algorithms embrace analog physics → Succeeds (provably!)
```

The "bizarre" approach was trying to run digital SGD on analog hardware. This paper shows we can do **provably better** by designing algorithms specifically for analog's unique characteristics.

**Final Insight:** The future of AI training isn't making analog behave like digital—it's understanding analog deeply enough to design algorithms that naturally fit its physics. This paper provides the mathematical tools to do exactly that.
