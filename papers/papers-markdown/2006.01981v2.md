# 2006.01981v2.pdf

Training End-to-End Analog Neural Networks
with Equilibrium Propagation
Jack Kendall1, Ross Pantone1, Kalpana Manickavasagam1,
Yoshua Bengio2,3, Benjamin Scellier2,∗
1Rain Neuromorphics
2Mila, Université de Montréal
3Canadian Institute for Advanced Research
Abstract
We introduce a principled method to train end-to-end analog neural networks
by stochastic gradient descent. In these analog neural networks, the weights
to be adjusted are implemented by the conductances of programmable resistive
devices such as memristors [Chua, 1971], and the nonlinear transfer functions (or
‘activation functions’) are implemented by nonlinear components such as diodes.
We show mathematically that a class of analog neural networks (called nonlinear
resistive networks) are energy-based models: they possess an energy function as
a consequence of Kirchhoff’s laws governing electrical circuits. This property
enables us to train them using the Equilibrium Propagation framework [Scellier
and Bengio, 2017]. Our update rule for each conductance, which is local and relies
solely on the voltage drop across the corresponding resistor, is shown to compute
the gradient of the loss function. Our numerical simulations, which use the SPICE-
based Spectre simulation framework to simulate the dynamics of electrical circuits,
demonstrate training on the MNIST classiﬁcation task, performing comparably or
better than equivalent-size software-based neural networks. Our work can guide
the development of a new generation of ultra-fast, compact and low-power neural
networks supporting on-chip learning.
1 Introduction
In recent years, deep neural networks have proved extremely effective in machine learning, achieving
state-of-the-art performance in a variety of domains, including image classiﬁcation [He et al., 2016],
speech recognition [Hinton et al., 2012], machine translation [Vaswani et al., 2017], and text-to-speech
[Oord et al., 2016]. One of the core principles to train these deep neural networks is optimization by
stochastic gradient descent (SGD).
However, training these neural networks on graphics processing units (GPUs) is time consuming
and energy intensive. This is due to the separation of memory and processing in von Neumann
hardware, which leads to a severe bottleneck in moving the data back and forth between memory
and compute units – the so-called von Neumann bottleneck. Building fast and energy-efﬁcient neural
networks requires a non-von Neumann computing paradigm which uniﬁes memory and processing,
by performing neural computations at the physical location of the synapses, where the strength of the
connections (the weights of the neural network) are stored and adjusted.
Programmable resistors can implement the synapses of a neural network by encoding the synaptic
weights in their conductances. Such programmable resistors can be built into large crossbar arrays
to represent the weight matrices of the layer-to-layer transformations of a deep neural network.
∗Currently at Google.
Preprint. Under review.
arXiv:2006.01981v2  [cs.NE]  9 Jun 2020

This setup presents signiﬁcant advantages over multi-processors such as GPUs: the number of
operations that can be simultaneously executed in a GPU is limited by its number of processors ; in
contrast, a crossbar array is a massively-parallel computing device in which all resistors do parallel
computations. Furthermore, since the computation in a crossbar array is in the analog domain, the
power consumption is also several orders of magnitude lower than that of a GPU [Burr et al., 2017].
Crossbar arrays have been proposed to accelerate the hardware implementation of the backpropagation
algorithm, which is the key algorithm of deep learning to train conventional neural networks [Li et al.,
2018, Rekhi et al., 2019]. Nevertheless, these implementations require digital-to-analog (DAC) and
analog-to-digital (ADC) conversion between the layers of the network, the latter being known to be
responsible for most of the power consumption [Li et al., 2015]. This suggests an opportunity to
further cut power consumption by avoiding DACs and ADCs.
In this work, we suggest an alternative to the backpropagation algorithm, which eliminates the need
for DACs and ADCs. We introduce end-to-end analog neural networks, in which nonlinear resistive
components such as diodes play the role of nonlinear transfer functions. Crucially, our hardware
implementation of neural networks allows end-to-end analog computing not just for inference, but
also for training. To achieve this, we use the Equilibrium Propagation (or EqProp) framework,
suitable for optimization by SGD with local weight updates [Scellier and Bengio, 2017].
EqProp applies to energy-based models (EBMs), i.e. neural network models that possess an energy
function. A key result we show is that a class of analog neural networks called nonlinear resistive
networks are EBMs: they possess an energy function whose existence is a direct consequence of
Kirchhoff’s laws governing electrical circuits. As a consequence, these analog networks are trainable
by SGD using locally available information for each weight. Speciﬁcally, we show mathematically
that the gradient (of the loss to minimize) with respect to a conductance can be estimated using solely
the voltage drop across the corresponding resistor. This result opens up a path towards dramatically
faster and orders of magnitude higher energy-efﬁcient neural networks trainable by SGD.
The main contributions of the present work are the following.
• Inspired by the work of Johnson [2010], we show that a class of analog neural networks
called nonlinear resistive networks are energy-based models (EBMs): at inference, the
conﬁguration of node voltages chosen by the circuit corresponds to the minimum of a
mathematical function (the energy function) called the total pseudo-power of the circuit, as
a consequence of Kirchhoff’s laws (Lemma 3 in Appendix A). By bridging the conceptual
gap between energy-based models (at a mathematical level 2), and physical energies 3 (at
a hardware level), our work thus introduces an implementation of energy-based neural
networks grounded in device physics.
• We show how these analog neural networks can be trained with the Equilibrium Propagation
framework [Scellier and Bengio, 2017], and we prove a formula for updating the conduc-
tances (the synaptic weights) in proportion to their error gradients, using solely the voltage
drops across the corresponding resistive devices (Theorem 1 in Section 3). This result
provides theoretical ground for implementing end-to-end analog neural networks trainable
by stochastic gradient descent using a fast and low-power weight-update mechanism.
• We introduce a deep analog network architecture inspired by those of conventional deep
learning (Fig.1 and Section 4).
• We demonstrate the potential of our novel neuromorphic hardware methodology with
numerical simulations on the MNIST dataset, using a SPICE-based framework to simulate
the circuit’s dynamics. Due to computational constraints, we train a network with 100
hidden neurons for 10 epochs and obtain 3.43% test error rate, outperforming equivalent-
size software-based neural networks (Section 5).
By explicitly decoupling the training algorithm (EqProp in Section 3) from the speciﬁc neural network
architecture studied in this work (Section 4), we stress that our optimization method can be used
for any network architecture, not just the one of Section 4. Our modular approach thus offers the
possibility to explore the design space of analog network architectures trainable with EqProp, in
2In an EBM, the energy function is a mathematical abstraction of the model, not a physical energy.
3Speciﬁcally the power dissipated in resistive devices.
2

essentially the same way as deep learning researchers explore the design space of differentiable neural
networks trainable with backpropagation.
Figure 1: Deep analog neural network with three input nodes (X1,X2 andX3), two layers of
three hidden neurons each (H1,H2,H3, andH4,H5,H6) and three output nodes ( ˆY1, ˆY2 and ˆY3).
Blue branches and red branches represent neurons and synapses, respectively. Each synapse is a
programmable resistor, whose conductance represents a parameter to be adjusted (section 4.1). Each
neuron is formed of a nonlinear transfer function and a bidirectional ampliﬁer. The nonlinear transfer
function is implemented by a pair of antiparallel diodes (in series with voltage sources), which
forms a sigmoidal function in its voltage response (section 4.2). The bidirectional ampliﬁer consists
of a current-controlled current source (CCCS, shown in brown) and a voltage-controlled voltage
source (VCVS, shown in black), allowing signals to propagate in both directions without a decay in
amplitude (section 4.3). Output nodes are linked to current sources (shown in green) which serve to
inject loss gradient signals during training (section 4.5). Equilibrium Propagation is a two-phase
procedure to compute the gradient of a loss L = ℓ( ˆY,Y ), where Y is the desired target (section
3.2). In the ﬁrst phase (free phase, or inference), input voltages are sourced at input nodes and the
current sources are set to zero; in the second phase ( nudged phase), for each output node ˆYk the
corresponding current source is set toIk = −β ∂ℓ
∂ ˆYk
, whereβ is a scaling factor (a hyperparameter).
The update rule to adjust the conductances of programmable resistors is local (Theorem 1.)
2 Related Work
Accelerators for deep learning. The computations involved in conventional neural networks
consist in large parts of tensor multiplications. A key insight is that the multiply and accumulate
operations of tensor multiplications can be performed in the analog domain by utilizing Ohm’s law
and Kirchhoff’s current law, respectively. In particular, crossbar arrays of analog resistive memory
elements can implement matrix-vector multiplications in their voltage-current transfer function. A
growing ﬁeld of research exploits this idea to develop specialized hardware aimed at speeding up
the forward and backward passes of the backpropagation algorithm [Burr et al., 2017, Jerry et al.,
2017, Ambrogio et al., 2018, Li et al., 2018, Xia and Yang, 2019]. However, in this approach, a
fundamental drawback arises from utilizing a different circuit for the forward and backward passes.
The device mismatches and nonidealities inevitably present in analog hardware cause a layer-speciﬁc
3

error in gradient computation. As the gradients propagate backwards through the network, these
errors accumulate. As a result, the performance of the network is severely degraded. This effect can
be mitigated by using digital-to-analog conversion (DAC) and analog-to-digital conversion (ADC)
between the layers of the network to compute the forward activation function and the backward
pointwise derivatives of the activation function in the digital domain (‘exactly’), but this technique
comes at the cost of greatly increasing power consumption [Li et al., 2015]. In contrast, our method
(EqProp) uses the same circuit for both phases of training (free phase and nudged phase), thereby
greatly simplifying the resulting hardware architecture. Crucially, since our update rule is local
(Theorem 1), the nonidealities of devices do not prevent reliable gradient computation. Finally, our
method does not require DACs and ADCs.
Energy-based models (EBMs). EBMs have played an important role in the foundations of deep
learning – see LeCun et al. [2006] for a comprehensive tutorial on energy-based learning. Historically,
the Hopﬁeld model (ﬁrst introduced in the discrete time setting with binary neurons [Hopﬁeld,
1982] and then adapted to the real-time setting with real-valued neurons [Cohen and Grossberg,
1983, Hopﬁeld, 1984]) and the Boltzmann machine [Ackley et al., 1985] (a stochastic variant of the
former) were the ﬁrst EBMs to be introduced. Unfortunately, these models suffer from long and
slow inference phases, making them mostly impractical and out of stage in modern deep learning
applications. Boltzmann machines require running a Monte Carlo Markov Chain (MCMC), while the
Hopﬁeld model requires an equally long phase of energy minimization. By highlighting the fact that
nonlinear resistive networks possess an energy function (the so-called total pseudo-power), our work
suggests an ultra efﬁcient implementation of energy-based neural networks, in which minimization
of the energy function is performed by the physics of the circuit (Kirchhoff’s laws), rather than by a
lengthy MCMC simulation.
Equilibrium Propagation (EqProp). EqProp is a general algorithm for computing error gradients
in EBMs, inspired by the contrastive learning algorithm for Boltzmann machines [Ackley et al., 1985]
and Hopﬁeld networks [Movellan, 1991]. Previous works have mostly studied EqProp in the setting
of classiﬁcation tasks to train the Hopﬁeld model and variants of it [Scellier and Bengio, 2017, 2019,
Scellier et al., 2018, Khan, 2018, O’Connor et al., 2018, O’Connor et al., 2019, Ernoult et al., 2019,
2020, Zoppo et al., 2020]. However, EqProp is a much more general method. Recently, Ernoult et al.
[2019] used EqProp to train energy-based convolutional networks. In this work, we apply EqProp
to a new class of energy-based neural networks called nonlinear resistive networks and we show in
Appendix B how EqProp can be used in the setting of Generative Adversarial Networks [Goodfellow
et al., 2014].
3 End-to-End Training via Equilibrium Propagation
We consider here for simplicity of presentation the supervised setting in which one has an inputX
and one wants to predict its associated targetY , e.g. the setting of image classiﬁcation whereX is
an image andY a label. Note however that our framework extends beyond this setting: we show in
Appendix B how it can be used in the setting of generative adversarial learning.
3.1 Nonlinear Resistive Networks and Supervised Learning
Nonlinear resistive network. The neural networks studied here are called nonlinear resistive
networks. These are electrical circuits consisting of two-terminal elements with continuous current-
voltage characteristics. This includes programmable resistors (whose conductances play the role of
synaptic weights), diodes (which form non-linear transfer functions), voltage sources (used to set
data samples at input nodes) and current sources (used to inject loss gradient signals during training).
Performing inference. A subset of the nodes of the circuit are input nodes at which input voltages
(denotedX) are sourced. All other nodes – the internal nodes and output nodes – are left ﬂoating:
after the voltages of input nodes have been set, the voltages of internal and output nodes settle to
their steady state. The output nodes, denoted ˆY , represent the readout of the system, i.e. the model
prediction.
4

Loss function to minimize. The architecture and the components of the circuit determine theX ↦→
ˆY function. Speciﬁcally, the conductances of the programmable resistors, denotedθ, parameterize
this function. That is, ˆY can be written as a function ofX andθ in the form ˆY (X,θ ). Training such a
circuit consists in adjusting the values of the conductances (θ) so that the voltages of output nodes (ˆY )
approach the target voltages (Y ). Formally, we cast the goal of training as an optimization problem
in which the loss to be optimized (corresponding to an input-target pair (X,Y )) is of the form:
L(X,Y,θ ) =ℓ
(
ˆY (X,θ ),Y
)
. (1)
In this work we use the squared error loss (Eq. 3). However, our framework applies to any differen-
tiable functionℓ ; see Appendix B.1 for common examples.
3.2 Computing the Gradient with respect to the Conductance of a Resistor
Equilibrium Propagation (EqProp) is a training framework for energy-based models (EBMs) [Scellier
and Bengio, 2017]. A key point we show is that nonlinear resistive networks are EBMs4 (Lemma
3 in Appendix A.2). This allows us to use EqProp in such analog neural networks to compute the
gradient of the loss of Eq. 1. Theorem 1 below provides a formula for computing the loss gradient
with respect to a conductance using solely the voltage drop across the corresponding resistor.
Given an inputX and associated targetY , EqProp proceeds in the following two phases.
Free phase (ﬁrst phase). At inference, input voltages are sourced at input nodes (X), while all
other nodes of the circuit (the internal nodes and output nodes) are left ﬂoating. All internal and
output node voltages are measured. In particular, the voltages of output nodes ( ˆY ) corresponding to
prediction are compared with the target (Y ) to compute the loss L =ℓ( ˆY,Y ).
Nudged phase (second phase). For each output node ˆYk, a currentIk = −β ∂ℓ
∂ ˆYk
is sourced at ˆYk,
whereβ is a positive or negative scaling factor (a hyperparameter). All internal node voltages and
output node voltages are measured anew.
Theorem 1 (Gradient Formula). Consider a nonlinear resistive network, and let gij denote the
conductance of a linear resistor whose terminals arei andj (i.e. a resistor across which the current
Iij and voltage drop ∆Vij satisfy Ohm’s law:Iij =gij∆Vij). Denote ∆V 0
ij the voltage drop across
this resistor in the free phase (when no current is sourced at output nodes), and ∆Vβ
ij the voltage
drop in the nudged phase (when a currentIk = −β ∂ℓ
∂ ˆYk
is sourced at each output node ˆYk). Then,
the gradient of the loss L =ℓ
(
ˆY,Y
)
with respect togij can be estimated, in the limitβ → 0, as
∂L
∂gij
= lim
β→0
1
2β
((
∆Vβ
ij
)2
−
(
∆V 0
ij
)2
)
. (2)
Theorem 1 is proved in Appendix A (the proof comes with a detailed sketch of the proof ﬁrst). It
is a particular case of the more general formula of Theorem 2 (Appendix A.1) which shows how to
compute the gradient in the case of an arbitrary non-linear resistive device.
In Appendix B.1, we give a few examples of common loss functions and derive the corresponding
currents (Ik) to be sourced at output nodes in the nudged phase.
Interestingly, in a chain-like layered neural network such as the one of Figure 1, the second phase of
EqProp (nudged phase) is similar in spirit to the backward pass of the backpropagation algorithm: the
currents introduced at output nodes in the nudged phase can be thought of as error signals propagating
backwards in the layers of the neural network, from output nodes back to input nodes.
4Writing the form of the ‘energy function’ (the total pseudo-power of the circuit) requires introducing a
substantial amount of notation. For this reason, we state and prove this result in Appendix A.
5

4 Deep Analog Neural Network Architecture
The theory of Section 3 applies to any nonlinear resistive network. In this section, we introduce a
neural network architecture inspired by those of conventional deep learning (Figure 1). It is composed
of multiple layers, alternating linear and non-linear processing stages. The linear transformations are
performed by crossbar arrays of programmable resistors, whose conductances play the role of synaptic
weights that parameterize the transformations. The nonlinear transfer function is implemented using
a pair of diodes, followed by a linear ampliﬁer. These crossbar arrays of programmable resistors and
these nonlinear transfer functions are alternated to form a deep network.
4.1 Programmable Resistors as Synapses
In the last decade, important advances in nanotechnology have provided neuromorphic researchers
with a panoply of new devices which allow for ultra low-power synaptic plasticity. Programmable
resistors that have been proposed and tested as synapses include memristors [Jo et al., 2010],
resistive random-access memory [Huang et al., 2019], phase-change memory [Ambrogio et al.,
2016], ferroelectric ﬁeld-effect transistors [Jerry et al., 2017], ﬂash memory [Guo et al., 2017],
magnetic random-access memory [Patil et al., 2019], conductive-bridging random access memory
[Cha et al., 2020] and spin-transfer-torque memory [Vincent et al., 2015], among others. We refer to
Burr et al. [2017] for a review on the use of programmable resistors for neuromorphic computing.
These programmable resistors behave as pure resistors in a low-voltage regime. However, by
applying voltage or current pulses, depending on the device, their conductances can be modiﬁed. The
programming of a conductance requires only a small amount of energy, since such devices can be
extremely small (∼ 2 nm) and their switching speed can be extremely fast (∼ 1 ns).
Programmable resistors are especially suited for a local learning rule such as the one of Theorem
1. In the free phase and nudged phase, the node voltages can be measured (without disturbing the
circuit) and stored by using a sample-and-hold ampliﬁer (SHA). Then, by embedding the resistors
in a suitable synaptic circuit, we can program them to update their conductances proportionally to
their loss gradients, i.e. ∆gij ∝ − 1
β
[(
∆Vβ
ij
)2
−
(
∆V 0
ij
)2
]
, thus performing one step of stochastic
gradient descent (SGD). This can be achieved by means of amplitude/duration modulation of a
voltage or current pulse applied to the resistive device, for example the 1T–1R (one transistor–one
memristor) synapse [Merced-Grafals et al., 2016]. Furthermore, in a crossbar array, all conductances
can be updated simultaneously with two vectors of voltage pulses, a method called the outer product
update [Fuller et al., 2019].
Although the physical realization of memristor synapses presents challenges (such as device-to-device
variability and systematic bias in the weight updates), its investigation is out of the scope of this work.
We refer to Chang et al. [2017] for a thorough analysis.
4.2 Neurons as Nonlinear Transfer Functions
One way to think of a neuron is that it acts as a transfer function between an input current and an
output voltage (Fig. 1). The transfer function (which plays the role of ‘activation function’) is set up
so that the neuron’s output voltage is a smooth and S-shaped (sigmoidal) function of the neuron’s
input current. To achieve this, we place two diodes antiparallel between the neuron’s input node and
ground, in series with voltage sources (used to shift the bounds of the activation function). As a result,
the neuron’s output voltage is linear in the range between zero and one volt, but as the voltage rises
above or below these thresholds, one of the diodes turns on and sinks most of the extra current to
ground. The output voltage remains bounded even as the input current grows very large. The transfer
function equation of a diode is provided in Appendix C.
4.3 Bidirectional Ampliﬁers
In a network consisting only of resistors and diodes, simulations show that the voltages of hidden
neurons span a signiﬁcantly smaller range of voltage values than the voltages of input nodes (the
voltages of hidden neurons are closer to zero). To prevent signal decay, we use voltage-controlled
voltage sources (VCVS) which amplify the voltages of hidden neurons in the forward direction by
6

a gain factorA. To better propagate error signals in the second phase of training (nudged phase),
we also use current-controlled current sources (CCCS) which amplify currents in the backward
direction by a gain factor 1/A. We call such a combination of a forward-directed VCVS and a
backward-directed CCCS a ‘bidirectional ampliﬁer’. More details are provided in Appendix C.4.
4.4 Constraint of Positive Weights – Doubling Input and Output Nodes
One constraint of analog neural networks (compared to conventional neural networks) is that the
conductances of programmable resistors, which represent the weights, are positive. Several ap-
proaches are proposed in the literature to overcome this constraint (Appendix C.5). In this work,
our approach consists in doubling the number of input nodes and inverting one set, and doubling
the number of output nodes. Typically in a classiﬁcation task with K classes, the target vector
Y = (Y1,Y 2,...,Y K) represents the one-hot code of the class label. For each classk, our network
thus has two output nodes ˆY +
k and ˆY−
k , with ˆY +
k − ˆY−
k representing a score assigned to classk. The
loss to optimize is the squared error loss:
ℓ( ˆY,Y ) = 1
2
K∑
k=1
(
ˆY +
k − ˆY−
k −Yk
)2
. (3)
4.5 Injecting Loss Gradient Signals with Current Sources
In the nudged phase (second phase), we require currentsI+
k andI−
k proportional to the gradients of
output node voltages ˆY +
k and ˆY−
k , which must be injected at output nodes. These currents are:
I+
k = −β ∂ℓ
∂ ˆY +
k
=β
(
Yk + ˆY−
k − ˆY +
k
)
, I −
k = −β ∂ℓ
∂ ˆY−
k
=β
(
ˆY +
k − ˆY−
k −Yk
)
. (4)
We achieve this using current sources. In the free phase (ﬁrst phase), the same current sources are set
to zero current, acting like open circuits and not inﬂuencing the voltages of output nodes.
5 Numerical Simulations
We use the SPICE (simulation program with integrated circuit emphasis) framework for realistic
simulations of the circuit’s dynamics [V ogt et al., 2020]. See Appendix D for simulation details.
XOR task. As a proof of concept, we show that our analog neural network can indeed learn a
nonlinear function such asY =X1 XORX2. The architecture together with the ﬁnal weights after
training are reported in Figure 3 (Appendix D).
MNIST digits classiﬁcation task. We perform our simulations on the MNIST dataset using the
high-performance SPICE-class parallel circuit simulator Spectre [Cadence Design Systems, Inc.,
2020]. Despite the use of this high-performance simulator, the computational difﬁculties described
below constrain us to limit our simulations to training a small (by deep learning standards) network
with a single hidden layer of 100 neurons. Since training takes 18 hours per epoch, we stop training
after 10 epochs (for a total training duration of one week).
After 10 epochs of training, our SPICE-based network achieves a test error rate of 3.43% (Table
1), while the training curve (Figure 4 in Appendix D) suggests that training isn’t complete. For
comparison, we train a logistic regression model which achieves 7.27% test error (Appendix D),
thus demonstrating that our SPICE-based network beneﬁts from the non-linearities offered by the
diodes. We then benchmark our SPICE-based network against a PyTorch implementation of the
original EqProp model [Scellier and Bengio, 2017]. We train two kinds of PyTorch-based networks:
one whose weights are free to be either positive or negative, and one that also possesses the same
strictly positive weight constraints as our SPICE-based network (see Section 4.4). In both cases, our
SPICE-based network outperforms the PyTorch-based networks with 100 hidden neurons (Table 1).
We also show that increasing the number of hidden neurons in the PyTorch-based networks results in
lower, more competitive test error rates (2.01%). These results suggest that with more computational
power, training a larger SPICE-based network to convergence would result in a lower ﬁnal test error
rate. Altogether our results demonstrate the potential of our novel neuromorphic approach.
7

Table 1: Results on MNIST. SPICE EqProp (our model) is benchmarked against a PyTorch imple-
mentation of the original EqProp model [Scellier and Bengio, 2017]. For the PyTorch models, the
mean values and 95% conﬁdence intervals of test errors are reported over ﬁve runs, after 10 epochs
of training and after training completion. 100 and 500 denote the number of neurons in the hidden
layer. ‘pos. weights’ means that weights are constrained to be positive (as explained in section 4.4).
Error rates after 10 epochs (%) Final error rates ( %)
Test (Train) Test (Train)
SPICE EqProp 100 (our model) 3.43 (2.68)
PyTorch EqProp 100 (pos. weights) 3.85 ± 0.05 (2 .87) 3 .44 ± 0.04 (1 .38)
PyTorch EqProp 100 3.99 ± 0.19 (2 .93) 3 .59 ± 0.06 (1 .35)
PyTorch EqProp 500 (pos. weights) 2.49 ± 0.01 (1 .47) 2.01 ± 0.02 (0.26)
PyTorch EqProp 500 2.92 ± 0.09 (1 .82) 2 .38 ± 0.11 (0 .52)
Circuit simulators and machine learning. We note that SPICE (and similar circuit simulators as
well) is not ideal for the types of simulations required in machine learning, due to the sequential
nature of training. Traditionally, one designs a circuit and uses a simulator to verify the correctness of
it via a tractable number of test cases. Importantly, these simulators are not designed for performing
many simulations iteratively. Perhaps interestingly, we found that simulators often possessed a large
overhead that surpassed the actual simulation time, which further points out that these simulators are
not designed for running millions of iterations.
6 Discussion
Even though the idea of neuromorphic hardware has existed for long [Mead, 1989], in the last decades
neural networks have more rapidly evolved to ﬁt the constraints of conventional von Neumann
computers. As a consequence, today’s neural networks (together with the backpropagation algorithm
which was invented to train them) are fundamentally discrete-time dynamical systems. With the
recent success of deep learning, a growing ﬁeld of research is emerging which uses mixed signal
analog/digital hardware to accelerate these discrete-time neural networks. However, when moving
from the digital world to the analog world, much more improvement in speed and power efﬁciency is
possible by rethinking neural networks as well as their training algorithm. Alternative neural network
paradigms which make use of the full potential of analog hardware for ultra-low energy computing
have been proposed and studied (e.g. spiking neural networks) but their development has thus far
been hindered due to the lack of a theoretical framework to train them.
Our work uses the formalism of energy-based models and equilibrium propagation to provide such a
theoretical framework for end-to-end analog neural networks trainable by stochastic gradient descent
with a local weight update mechanism. The modularity of our framework, in which the training
algorithm (EqProp) can be decoupled from the neural network architecture, offers the possibility
for neuromorphic researchers to explore the design space of analog network architectures, with the
perspective of ﬁnding circuits and conﬁgurations of components that best ﬁt with EqProp training.
Among the remaining challenges, the main one is perhaps to ﬁgure out how to leverage the working
mechanisms of memristors to implement reliable conductance changes. While experimental imple-
mentation of memristive crossbar arrays is still in its infancy, numerous simulations have indicated
their potential. In particular, Ambrogio et al. [2018] have demonstrated in the context of feedforward
networks trained with backpropagation that memristive crossbar arrays can achieve comparable
accuracy to software-based networks running on conventional computer hardware, while increasing
speed and reducing power consumption by two orders of magnitude. The potential speedup and
power reduction offered by analog computing are especially critical for scaling up neural networks to
billions of neurons (sizes that are far out of reach with current GPU-based deep learning models).
To achieve such sizes, it will also become critical to take into account geometric constraints such
as sparsity in neural connectivity. For this purpose, sparse analog hardware architectures such as
Memristive Nanowire Neural Networks (MN3) are a promising alternative to dense crossbar arrays
[Kendall et al., 2020].
8

Finally, our framework, which is a theory of how circuit dynamics can optimize objective functions,
may also inspire neuroscientists who seek to explain the mechanisms of credit assignment in the
brain [Whittington and Bogacz, 2019, Richards et al., 2019, Lillicrap et al., 2020].
Acknowledgments
The authors would like to thank Thomas Fischbacher, Maxence Ernoult, Michal Januszewski and
Effrosyni Kokiopoulou for valuable feedback and discussions. The authors would also like to thank
John O’Donovan for his invaluable assistance with the Spectre simulator.
References
D. H. Ackley, G. E. Hinton, and T. J. Sejnowski. A learning algorithm for boltzmann machines.
Cognitive science, 9(1):147–169, 1985.
S. Ambrogio, N. Ciocchini, M. Laudato, V . Milo, A. Pirovano, P. Fantini, and D. Ielmini. Unsu-
pervised learning by spike timing dependent plasticity in phase change memory (pcm) synapses.
Frontiers in neuroscience, 10:56, 2016.
S. Ambrogio, P. Narayanan, H. Tsai, R. M. Shelby, I. Boybat, C. di Nolfo, S. Sidler, M. Giordano,
M. Bodini, N. C. Farinha, et al. Equivalent-accuracy accelerated neural-network training using
analogue memory. Nature, 558(7708):60–67, 2018.
J. C. Baez and B. Fong. A compositional framework for passive linear networks. arXiv preprint
arXiv:1504.05625, 2015.
G. W. Burr, R. M. Shelby, A. Sebastian, S. Kim, S. Kim, S. Sidler, K. Virwani, M. Ishii, P. Narayanan,
A. Fumarola, et al. Neuromorphic computing using non-volatile memory. Advances in Physics: X,
2(1):89–124, 2017.
Cadence Design Systems, Inc. Spectre circuit simulator reference, version 19.1, Jan 2020.
J.-H. Cha, S. Y . Yang, J. Oh, S. Choi, S. Park, B. C. Jang, W. B. Ahn, and S.-Y . Choi. Conductive-
bridging random-access memories for emerging neuromorphic computing. Nanoscale, 2020.
C.-C. Chang, P.-C. Chen, T. Chou, I.-T. Wang, B. Hudec, C.-C. Chang, C.-M. Tsai, T.-S. Chang, and
T.-H. Hou. Mitigating asymmetric nonlinear weight update effects in hardware neural network
based on analog resistive synapse. IEEE Journal on Emerging and Selected Topics in Circuits and
Systems, 8(1):116–124, 2017.
K. Christianson and L. Erickson. The dirichlet problem on directed networks. 2007. URL https:
//sites.math.washington.edu/~reu/papers/2007/KariLindsay/dirichlet.pdf. Ac-
cessed: 2020-05-31.
L. Chua. Memristor-the missing circuit element. IEEE Transactions on circuit theory, 18(5):507–519,
1971.
M. A. Cohen and S. Grossberg. Absolute stability of global pattern formation and parallel memory
storage by competitive neural networks. IEEE transactions on systems, man, and cybernetics, (5):
815–826, 1983.
M. Ernoult, J. Grollier, D. Querlioz, Y . Bengio, and B. Scellier. Updates of equilibrium prop match
gradients of backprop through time in an rnn with static input. In Advances in Neural Information
Processing Systems, pages 7079–7089, 2019.
M. Ernoult, J. Grollier, D. Querlioz, Y . Bengio, and B. Scellier. Equilibrium propagation with
continual weight updates. arXiv preprint arXiv:2005.04168, 2020.
E. J. Fuller, S. T. Keene, A. Melianas, Z. Wang, S. Agarwal, Y . Li, Y . Tuchman, C. D. James, M. J.
Marinella, J. J. Yang, et al. Parallel programming of an ionic ﬂoating-gate memory array for
scalable neuromorphic computing. Science, 364(6440):570–574, 2019.
9

X. Glorot and Y . Bengio. Understanding the difﬁculty of training deep feedforward neural networks.
In Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics,
pages 249–256, 2010.
I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair, A. Courville, and
Y . Bengio. Generative adversarial nets. In Advances in neural information processing systems,
pages 2672–2680, 2014.
X. Guo, F. M. Bayat, M. Bavandpour, M. Klachko, M. Mahmoodi, M. Prezioso, K. Likharev, and
D. Strukov. Fast, energy-efﬁcient, robust, and reproducible mixed-signal neuromorphic classiﬁer
based on embedded nor ﬂash memory technology. In 2017 IEEE International Electron Devices
Meeting (IEDM), pages 6–5. IEEE, 2017.
K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proceedings
of the IEEE conference on computer vision and pattern recognition, pages 770–778, 2016.
G. Hinton, L. Deng, D. Yu, G. E. Dahl, A.-r. Mohamed, N. Jaitly, A. Senior, V . Vanhoucke, P. Nguyen,
T. N. Sainath, et al. Deep neural networks for acoustic modeling in speech recognition: The shared
views of four research groups. IEEE Signal processing magazine, 29(6):82–97, 2012.
J. J. Hopﬁeld. Neural networks and physical systems with emergent collective computational abilities.
Proceedings of the national academy of sciences, 79(8):2554–2558, 1982.
J. J. Hopﬁeld. Neurons with graded response have collective computational properties like those of
two-state neurons. Proceedings of the national academy of sciences, 81(10):3088–3092, 1984.
B. D. Hoskins, M. W. Daniels, S. Huang, A. Madhavan, G. C. Adam, N. Zhitenev, J. J. McClelland,
and M. D. Stiles. Streaming batch eigenupdates for hardware neural networks. Frontiers in
Neuroscience, 13, 2019.
M. Hu, J. P. Strachan, Z. Li, E. M. Grafals, N. Davila, C. Graves, S. Lam, N. Ge, J. J. Yang, and
R. S. Williams. Dot-product engine for neuromorphic computing: Programming 1t1m crossbar
to accelerate matrix-vector multiplication. In 2016 53nd ACM/EDAC/IEEE Design Automation
Conference (DAC), pages 1–6. IEEE, 2016.
P. Huang, Z. Zhou, Y . Zhang, Y . Xiang, R. Han, L. Liu, X. Liu, and J. Kang. Hardware implementation
of rram based binarized neural networks. APL Materials, 7(8):081105, 2019.
M. Jerry, P.-Y . Chen, J. Zhang, P. Sharma, K. Ni, S. Yu, and S. Datta. Ferroelectric fet analog synapse
for acceleration of deep neural network training. In 2017 IEEE International Electron Devices
Meeting (IEDM), pages 6–2. IEEE, 2017.
S. H. Jo, T. Chang, I. Ebong, B. B. Bhadviya, P. Mazumder, and W. Lu. Nanoscale memristor device
as synapse in neuromorphic systems. Nano letters, 10(4):1297–1301, 2010.
W. Johnson. Nonlinear electrical networks, 2010. URL https://sites.math.washington.edu/
~reu/papers/2017/willjohnson/directed-networks.pdf. Accessed: 2020-05-31.
J. D. Kendall, R. D. Pantone, and J. C. Nino. Deep learning in memristive nanowire networks. arXiv
preprint arXiv:2003.02642, 2020.
A. F. Khan. Bidirectional learning in recurrent neural networks using equilibrium propagation.
Master’s thesis, University of Waterloo, 2018.
Y . LeCun, L. Bottou, Y . Bengio, and P. Haffner. Gradient-based learning applied to document
recognition. Proceedings of the IEEE, 86(11):2278–2324, 1998.
Y . LeCun, S. Chopra, R. Hadsell, M. Ranzato, and F. Huang. A tutorial on energy-based learning.
Predicting structured data, 1(0), 2006.
B. Li, L. Xia, P. Gu, Y . Wang, and H. Yang. Merging the interface: Power, area and accuracy
co-optimization for rram crossbar-based mixed-signal computing system. In Proceedings of the
52nd Annual Design Automation Conference, pages 1–6, 2015.
10

C. Li, D. Belkin, Y . Li, P. Yan, M. Hu, N. Ge, H. Jiang, E. Montgomery, P. Lin, Z. Wang, et al.
Efﬁcient and self-adaptive in-situ learning in multilayer memristor neural networks. Nature
communications, 9(1):1–8, 2018.
T. P. Lillicrap, A. Santoro, L. Marris, C. J. Akerman, and G. Hinton. Backpropagation and the brain.
Nature Reviews Neuroscience, pages 1–12, 2020.
C. Mead. Analog vlsi and neutral systems. NASA STI/Recon Technical Report A, 90, 1989.
E. J. Merced-Grafals, N. Dávila, N. Ge, R. S. Williams, and J. P. Strachan. Repeatable, accurate, and
high speed multi-level programming of memristor 1t1r arrays for power efﬁcient analog computing
applications. Nanotechnology, 27(36):365202, 2016.
J. R. Movellan. Contrastive hebbian learning in the continuous hopﬁeld model. In Connectionist
models, pages 10–17. Elsevier, 1991.
P. O’Connor, E. Gavves, and M. Welling. Initialized equilibrium propagation for backprop-free
training. 2018.
A. v. d. Oord, S. Dieleman, H. Zen, K. Simonyan, O. Vinyals, A. Graves, N. Kalchbrenner,
A. Senior, and K. Kavukcuoglu. Wavenet: A generative model for raw audio. arXiv preprint
arXiv:1609.03499, 2016.
P. O’Connor, E. Gavves, and M. Welling. Training a spiking neural network with equilibrium
propagation. In The 22nd International Conference on Artiﬁcial Intelligence and Statistics, pages
1516–1523, 2019.
A. D. Patil, H. Hua, S. Gonugondla, M. Kang, and N. R. Shanbhag. An mram-based deep in-memory
architecture for deep neural networks. In 2019 IEEE International Symposium on Circuits and
Systems (ISCAS), pages 1–5, 2019.
A. S. Rekhi, B. Zimmer, N. Nedovic, N. Liu, R. Venkatesan, M. Wang, B. Khailany, W. J. Dally,
and C. T. Gray. Analog/mixed-signal hardware error modeling for deep learning inference. In
Proceedings of the 56th Annual Design Automation Conference 2019, DAC ’19, New York, NY ,
USA, 2019. Association for Computing Machinery. ISBN 9781450367257. doi: 10.1145/3316781.
3317770. URL https://doi.org/10.1145/3316781.3317770.
B. A. Richards, T. P. Lillicrap, P. Beaudoin, Y . Bengio, R. Bogacz, A. Christensen, C. Clopath, R. P.
Costa, A. de Berker, S. Ganguli, et al. A deep learning framework for neuroscience. Nature
neuroscience, 22(11):1761–1770, 2019.
B. Scellier and Y . Bengio. Equilibrium propagation: Bridging the gap between energy-based models
and backpropagation. Frontiers in computational neuroscience, 11, 2017.
B. Scellier and Y . Bengio. Equivalence of equilibrium propagation and recurrent backpropagation.
Neural computation, 31(2):312–329, 2019.
B. Scellier, A. Goyal, J. Binas, T. Mesnard, and Y . Bengio. Generalization of equilibrium propagation
to vector ﬁeld dynamics. arXiv preprint arXiv:1808.04873, 2018.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, Ł. Kaiser, and I. Polosukhin.
Attention is all you need. In Advances in neural information processing systems, pages 5998–6008,
2017.
A. F. Vincent, J. Larroque, N. Locatelli, N. Ben Romdhane, O. Bichler, C. Gamrat, W. S. Zhao,
J. Klein, S. Galdin-Retailleau, and D. Querlioz. Spin-transfer torque magnetic memory as a
stochastic memristive synapse for neuromorphic systems. IEEE Transactions on Biomedical
Circuits and Systems, 9(2):166–174, 2015.
H. V ogt, M. Hendrix, and P. Nenzi. Ngspice (version 31), 2020. URL http://ngspice.
sourceforge.net/docs/ngspice-manual.pdf.
Z. Wang, C. Li, W. Song, M. Rao, D. Belkin, Y . Li, P. Yan, H. Jiang, P. Lin, M. Hu, et al. Reinforce-
ment learning with analogue memristor arrays. Nature Electronics, 2(3):115–124, 2019.
11

J. C. Whittington and R. Bogacz. Theories of error back-propagation in the brain. Trends in cognitive
sciences, 2019.
Q. Xia and J. J. Yang. Memristive crossbar arrays for brain-inspired computing. Nature materials, 18
(4):309–323, 2019.
G. Zoppo, F. Marrone, and F. Corinto. Equilibrium propagation for memristor-based recurrent neural
networks. Frontiers in Neuroscience, 14:240, 2020.
12

Appendix
A Generalisation of Theorem 1 and Proof
In this appendix we prove Theorem 1, which we recall here. Theorem 1 holds for any nonlinear
resistive network, that is any circuit consisting of interconnected two-terminal components for which
the current-voltage characteristics (deﬁned in section A.1) are well deﬁned and continuous.
Theorem 1 (Gradient Formula). Consider a nonlinear resistive network, and let gij denote the
conductance of a linear resistor whose terminals arei andj (i.e. a resistor across which the current
Iij and voltage drop ∆Vij satisfy Ohm’s law:Iij =gij∆Vij). Denote ∆V 0
ij the voltage drop across
this resistor in the free phase (when no current is sourced at output nodes), and ∆Vβ
ij the voltage
drop in the nudged phase (when a currentIk = −β ∂ℓ
∂ ˆYk
is sourced at each output node ˆYk). Then,
the gradient of the loss L =ℓ
(
ˆY,Y
)
with respect togij can be estimated, in the limitβ → 0, as
∂L
∂gij
= lim
β→0
1
2β
((
∆Vβ
ij
)2
−
(
∆V 0
ij
)2
)
. (2)
Sketch of the proof. We prove Theorem 1 in three steps.
1. In Section A.1, we state a more general formula for computing the gradient with respect
to a parameter of an arbitrary two-terminal component (Theorem 2), and we show that
the formula of Theorem 1 is a particular case of Theorem 2. More speciﬁcally, for every
two-terminal component we deﬁne a quantity called the pseudo-power of the component.
Theorem 2 states that the gradient of the loss with respect to a parameter of this component
can be expressed in terms of its pseudo-power. In the case of a linear resistor we recover the
formula of Theorem 1. (At this stage it then remains to prove Theorem 2.)
2. In Section A.2, following the work of Johnson [2010], we show that in a nonlinear resistive
network, the steady state of the circuit imposed by Kirchhoff’s laws is a critical point of
the total pseudo-power of the circuit (Lemma 3), which by deﬁnition is the sum of the
pseudo-powers of its individual components. In this sense we say that nonlinear resistive
networks are energy-based models (EBMs) whose energy function is the total pseudo-power.
This result bridges a conceptual gap between energy functions in EBMs (at a mathematical
level) and physical energies (at a hardware level). (At this stage it then remains to prove
Theorem 2, given Lemma 3.)
3. In Section A.3, we follow the general method of Scellier and Bengio [2017] to prove
Theorem 2 using Lemma 3. Speciﬁcally, we proceed in three sub-steps. First, by denoting
P the total pseudo-power of the circuit in the free phase (ﬁrst phase of training), and ℓ
the cost function, we show that the total pseudo-power of the circuit in the nudged phase
(second phase of training, when we inject currentsIk = −β ∂ℓ
∂ ˆYk
at output nodes) is equal
to P = P +βℓ. Second, we use the criticality condition of Lemma 3 to state and prove a
result about P (Lemma 4). Third, we show that Theorem 2 is a consequence of Lemma 4.
A.1 Gradient With Respect to a Parameter of an Arbitrary Two-Terminal Component
In this subsection, we state Theorem 2, and we show that Theorem 1 is a special case of it. Theorem
2 states that the gradient of the loss with respect to the parameter of an arbitrary two-terminal
component can be expressed in terms of a quantity called the pseudo-power of the component. The
pseudo-power is itself deﬁned in terms of the current-voltage characteristic of the component.
Current-voltage characteristic of a two-terminal component. Consider a two-terminal compo-
nent (such as a resistor or a diode) with end nodesi andj. Its behaviour is determined by its current-
voltage characteristic, which is a functionγij that takes as input the voltage drop ∆Vij =Vi −Vj
across the component and returns the current Iij = γij (∆Vij) moving from node i to node j in
response to the voltage drop ∆Vij. Since the current ﬂowing from i toj is the negative of the current
13

ﬂowing from j toi, we have by deﬁnition:
∀i,j, γ ij (∆Vij) = −γji (∆Vji) (5)
where ∆Vji = −∆Vij.
For example, the current-voltage characteristic of a linear resistor of conductancegij linking nodei
to nodej is given by Ohm’s law:Iij =gij∆Vij, which by deﬁnition ofγij implies that
γij (∆Vij) =gij∆Vij. (6)
Pseudo-power of a two-terminal component. For each two-terminal component of current-
voltage characteristicIij =γij(∆Vij), we deﬁnepij(∆Vij) as the primitive function ofγij(∆Vij)
that vanishes at 0, i.e.
pij(∆Vij) =
∫ ∆Vij
0
γij(v)dv. (7)
The quantitypij (∆Vij) has the physical dimensions of power, being a product of a voltage and a
current. We callpij (∆Vij) the pseudo-power along the branch fromi toj, following the terminology
of Johnson [2010]. Note that as a consequence of Eq. 5 we have
∀i,j, p ij(∆Vij) =pji(∆Vji), (8)
i.e. the pseudo-power fromi toj and the pseudo-power fromj toi are the same thing. We call this
property the pseudo-power symmetry.
For example, in the case of a linear resistor of conductancegij linking nodei to nodej, the current-
voltage characteristicγij(∆Vij) is given by Eq. 6, and its corresponding pseudo-power is:
pij(∆Vij) = 1
2gij∆V 2
ij. (9)
In this case, the pseudo-power is half the physical power dissipated in the resistor.
Theorem 2. Consider a nonlinear resistive network, and letwij denote an adjustable parameter of
a two-terminal component whose terminals arei andj. Denote ∆V 0
ij the voltage drop across this
two-terminal component in the free phase (when no current is sourced at output nodes), and∆Vβ
ij
the voltage drop in the nudged phase (when a currentIk = −β ∂ℓ
∂ ˆYk
is sourced at each output node
ˆYk). Then, the gradient of the loss L =ℓ
(
ˆY,Y
)
with respect towij can be estimated, in the limit
β → 0, as
∂L
∂wij
= lim
β→0
1
β


∂pij
(
∆Vβ
ij
)
∂wij
− ∂pij
(
∆V 0
ij
)
∂wij

. (10)
In the case of a resistor of conductance gij, the adjustable parameter iswij =gij and the pseudo-
powerpij (∆Vij) is given by Eq. 9, so that
∂pij (∆Vij)
∂gij
= 1
2 (∆Vij)2. (11)
Thus, Theorem 1 is a special case of Theorem 2. It remains to prove Theorem 2, which we do next.
A.2 Nonlinear Resistive Networks are Energy-Based Models
In this subsection we show that, in a nonlinear resistive network, the steady state of the circuit
characterized by Kirchhoff’s laws is a critical point of a function called the total pseudo-power
(Lemma 3). The total pseudo-power is the sum of the pseudo-powers of the individual components of
the circuit.
We number the nodes of the circuiti = 1, 2,...,N and denote the node voltagesV1,V 2,...,V N .
Conﬁguration of the circuit. We call a vector of voltage valuesV = (V1,V 2,...,V N) a conﬁgu-
ration. Importantly, according to our deﬁnition, any vector of voltage values is a conﬁguration, even
those that are not compatible with the laws governing electrical circuits (Kirchhoff’s current law).
14

Total pseudo-power of a conﬁguration. Recall the deﬁnition of the pseudo-power of a two-
terminal component (Eq. 7). We deﬁne the total pseudo-power of a conﬁguration V =
(V1,V 2,...,V N) as the sum of pseudo-powers along all branches:
P(V1, · · ·,VN) =
∑
i<j
pij(Vi −Vj). (12)
Notice that the pseudo-power symmetry (Eq. 8) guarantees that this deﬁnition does not depend on
node ordering.
Consider as an example the case of a linear resistance network, i.e. a circuit composed of nodes
interconnected by linear resistors. The pseudo-power of each individual resistor is given by Eq. 9 and
is half the power dissipated by the resistor. Therefore the total pseudo-power of the circuit is half the
total power dissipated by the circuit:
P(V1,V 2,...,V N) = 1
2
∑
i<j
gij (Vj −Vi)2. (13)
We stress that P is a mathematical function deﬁned on any conﬁgurationV1,V 2,...,V N , even those
that are not compatible with Kirchhoff’s current law.
Steady state of the circuit. The conﬁguration of the circuit that is physically realized is imposed
by Kirchhoff’s current law (KCL). We denoteV⋆
1 ,V⋆
2 ,... ,V⋆
N the voltage values imposed by KCL,
and we callV⋆ = (V⋆
1,V ⋆
2,...,V ⋆
N) the steady state of the circuit. Speciﬁcally, for every (internal
or output) ﬂoating node i, KCL implies ∑N
j=1Iij = 0, which rewrites
N∑
j=1
γij
(
V⋆
i −V⋆
j
)
= 0. (14)
Lemma 3 (Johnson [2010]). The steady state of the circuit, denoted (V⋆
1,V ⋆
2,...,V ⋆
N), is a critical
point5 of the total pseudo-power: for every ﬂoating nodei, we have
∂P
∂Vi
(V⋆
1,V ⋆
2,...,V ⋆
N) = 0. (15)
We say in this sense that the circuit is an energy-based model, whose energy function is the total
pseudo-power.
Proof of Lemma 3. We use the deﬁnition of the total pseudo-power (Eq. 12), the pseudo-power
symmetry (Eq. 8), the deﬁnition of the pseudo-power (Eq. 7) and the fact that the steady state of the
circuit satisﬁes Kirchhoff’s current law (Eq. 14). For every ﬂoating nodei we have:
∂P
∂Vi
(V⋆
1,V ⋆
2,...,V ⋆
N) =
∑
j
∂pij
∂Vi
(V⋆
i −V⋆
j ) (16)
=
∑
j
γij(V⋆
i −V⋆
j ) = 0. (17)
Lemma 3 generalizes a well-known property of linear resistance networks (i.e. circuits composed
of linear resistors) called the principle of minimum dissipated power [Baez and Fong, 2015]. This
principle states that in a linear resistance network, if the voltages are imposed at a set of input nodes,
the circuit will choose the voltages at other nodes so as to minimize the total power dissipated in the
resistors (i.e. minimize Eq. 13).
5With further assumptions on the current-voltage functions γij, Christianson and Erickson [2007] and
Johnson [2010] show that the function P is convex, so that the steady state is the global minimum of P. In our
case however, in order to prove Theorem 2, all one needs in the framework of Equilibrium Propagation is the
ﬁrst order condition, i.e. the fact that the steady state is a critical point ofP, not necessarily a global or local
minimum.
15

A.3 Proof of Theorem 2
Equilibrium Propagation [Scellier and Bengio, 2017] applies to models that possess an energy
function, in the sense of Lemma 3. Equipped with Lemma 3, we can therefore prove Theorem 2 by
following the general method of Scellier and Bengio [2017]. We do this in three steps:
• First, we rewrite the formula of Theorem 2 in terms of the total pseudo-power of the ﬁrst
phase (free phase), denoted P, and the cost functionℓ.
• Second, we introduce the total pseudo-power of the second phase (nudged phase), denoted
P, which we show to be equal to P = P +βℓ, and we rewrite Theorem 2 in terms of P.
• Third, we prove Theorem 2 using Lemma 3.
First step. Rewriting Theorem 2 in terms of the total pseudo-power of the ﬁrst phase P and
the cost function ℓ. Let V be the conﬁguration of ﬂoating node voltages, which includes the
internal nodes and output nodes.θ denotes the vector of adjustable parameters (conductances), and
P(X,V,θ ) denotes the total pseudo-power of the circuit in the ﬁrst phase of training (free phase).
Consider an adjustable parameterwij of a component whose terminals arei andj. This parameter
contributes to P(X,V,θ ) only through the pseudo-power of the component it belongs to, i.e. only
through the termpij(Vi −Vj). Thus, we have (see Eq. 12)
∂P(X,V,θ )
∂wij
= ∂pij(Vi −Vj)
∂wij
. (18)
Therefore, the formula to be proved (Theorem 2) rewrites:
∂L(X,Y,θ )
∂wij
= lim
β→0
1
β
(
∂P
(
X,V β,θ
)
∂wij
− ∂P
(
X,V 0,θ
)
∂wij
)
. (19)
HereV 0 andVβ are the steady states of the circuit in the ﬁrst phase (free phase) and second phase
(nudged phase), respectively. Since the formula of Eq. 19 is to be proved for all parameterswij of the
circuit, we can also rewrite Theorem 2 in the more compact form:
∂L
∂θ (X,Y,θ ) = lim
β→0
1
β
(∂P
∂θ
(
X,V β,θ
)
− ∂P
∂θ
(
X,V 0,θ
))
. (20)
Recall that the loss to be optimized (Eq. 1) is
L(X,Y,θ ) =ℓ(V 0,Y ). (21)
Note the difference between the lossL and the cost functionℓ: the cost is deﬁned for any conﬁguration
V , whereas the loss is the cost value at the steady stateV 0. Recall here that L depends onX andθ
throughV 0. Now, the formula to be proved (Eq. 20) rewrites:
d
dθℓ(V 0,Y ) = lim
β→0
1
β
(∂P
∂θ
(
X,V β,θ
)
− ∂P
∂θ
(
X,V 0,θ
))
. (22)
Differentiation with respect toθ on the left-hand side is to be understood throughV 0.
Second step. Rewriting Theorem 2 in terms of the total pseudo-power of the second phase P.
Recall that P(X,V,θ ) is the total pseudo-power in the ﬁrst phase of training (free phase). In the
second phase of training though (nudged phase), a currentIk = −γβ
k (V ) is sourced at every ﬂoating
node6Vk, where
γβ
k (V ) =β ∂ℓ
∂Vk
(V,Y ) (23)
6In section 3.1, we have considered a cost functionℓ( ˆY,Y ) which depends only on output node voltages ˆY ,
not on internal node voltages. In this context, in the second phase of EqProp we only source currents at output
nodes. However, this setting can be directly generalized to the case of a cost functionℓ(V,Y ) that depends on
internal node voltages too.
16

represents the current ﬂowing from node k outwards (towards the current source), in agreement with
the sign convention in the deﬁnition of the current-voltage characteristic adopted in section A.1. We
can then deﬁne
pβ(V ) =β ℓ(V,Y ), (24)
which plays the role of pseudo-power associated to all the currentsIk as it satisﬁes:
∀k, γ β
k (V ) = ∂pβ
∂Vk
(V ). (25)
Thus, in the second phase of training (nudged phase), the total pseudo-power of the circuit is
P(θ,β,V ) = P(X,V,θ ) +β ℓ(V,Y ). (26)
Subsequently, we drop the input X and targetY in the notations, since they are static and do not
play any role in the proof of Theorem 2. With these notations the total pseudo-power of the ﬁrst
phase (free phase) is P(θ, 0,V ), and the total pseudo-power of the second phase (nudged phase) is
P(θ,β,V ).
From now on we also rewriteV 0
θ andVβ
θ the steady states of the circuit in the ﬁrst phase (free phase)
and second phase (nudged phase), respectively, to stress that they depend on the parameterθ. From
Lemma 3,V 0
θ is a critical point of P(θ, 0, ·), andVβ
θ is a critical point of P(θ,β, ·). With these
notations, the formula to be proved (Eq. 22) rewrites:
d
dθ
∂P
∂β (θ, 0,V 0
θ ) = lim
β→0
1
β
(∂P
∂θ
(
θ,β,V β
θ
)
− ∂P
∂θ
(
θ, 0,V 0
θ
))
, (27)
or more compactly7:
d
dθ
∂P
∂β (θ, 0,V 0
θ ) = d
dβ
⏐⏐⏐⏐
β=0
∂P
∂θ (θ,β,V β
θ ), (28)
where d
dβ
⏐⏐⏐
β=0
represents the total derivative of the expression, evaluated at the pointβ = 0.
Third step. Statement and Proof of Lemma 4. The formula to be proved (Eq. 28) is a direct
consequence of the following result proved in Scellier and Bengio [2017], which we also prove here
for completeness.
Lemma 4 (Scellier and Bengio [2017]). For any value ofβ, we have the relationship:
d
dθ
∂P
∂β (θ,β,V β
θ ) = d
dβ
∂P
∂θ (θ,β,V β
θ ). (29)
Proof of Lemma 4. Let us deﬁne a functionH of the two argumentsθ andβ:
H (θ,β ) = P
(
θ,β,V β
θ
)
. (30)
Note thatH is a function of (θ,β ) not only through P(θ,β, ·) but also throughVβ
θ . Using the chain
rule of differentiation and Lemma 3 we have
∂H
∂β (θ,β ) = ∂P
∂β
(
θ,β,V β
θ
)
+ ∂P
∂V
(
θ,β,V β
θ
)
  
= 0
·∂V β
θ
∂β (31)
= ∂P
∂β
(
θ,β,V β
θ
)
. (32)
Differentiating this expression with respect toθ, we get
∂2H
∂θ∂β (θ,β ) = d
dθ
∂P
∂β
(
θ,β,V β
θ
)
. (33)
7Note that the notations d
dθ and ∂
∂θ (as well as d
dβ and ∂
∂β ) have two different meanings. The notation
∂P
∂θ (θ,β,V ) represents the partial derivative of P with respect to its ﬁrst argument ( θ) ; this excludes
differentiation throughVβ
θ . The notation d
dθf (θ,β,V β
θ ) represents the total derivative of the expression with
respect toθ: this includes the differentiation paths through both the ﬁrst argument off and throughVβ
θ .
17

Similarly, we have that
∂H
∂θ (θ,β ) = ∂P
∂θ
(
θ,β,V β
θ
)
+ ∂P
∂V
(
θ,β,V β
θ
)
  
= 0
·∂V β
θ
∂θ (34)
= ∂P
∂θ
(
θ,β,V β
θ
)
, (35)
so that
∂2H
∂β∂θ (θ,β ) = d
dβ
∂P
∂θ
(
θ,β,V β
θ
)
. (36)
Finally, we conclude using the symmetry of second derivatives ofH:
∂2H
∂θ∂β (θ,β ) = ∂2H
∂β∂θ (θ,β ). (37)
18

B General Applicability of EqProp (Complement of Section 3)
In section 3 we have presented the setting of classiﬁcation and regression tasks where the goal is to
predict a targetY given an inputX.
In this appendix, we show the general applicability of EqProp. In section B.1, we give examples
of loss functions and the corresponding gradient currents to be sourced in the second phase of
training (nudged phase). We then show in section B.2 that EqProp extends well beyond the setting
of classiﬁcation and regression tasks, and can also be used in the setting of generative adversarial
networks [Goodfellow et al., 2014].
B.1 Loss Functions
In the supervised setting, the goal of training is to minimize the expected loss
J(θ) = E(X,Y )∼p(X,Y ) [L(X,Y,θ )], (38)
over input-target pairs (X,Y ) drawn from a data distributionp(X,Y ). Recall that the loss L of Eq. 1
for a given input-target pair (X,Y ) is of the form:
L(X,Y,θ ) =ℓ
(
ˆY,Y
)
, (39)
where ˆY is the steady state of output nodes at inference, andℓ( ˆY,Y ) is an arbitrary differentiable
cost function.
We study here two examples of common cost functionsℓ and give the corresponding currents to be
sourced in the second phase of training (nudged phase).
B.1.1 Squared Error Loss
The most straightforward example is the squared error cost function, given by
ℓ( ˆY,Y ) = 1
2
K∑
k=1
(
ˆYk −Yk
)2
, (40)
which sums the quadratic distances between each output voltage ˆYk and its corresponding target
voltageYk. In this case the error derivatives of output nodes are:
∂ℓ
∂ ˆYk
( ˆY,Y ) = ˆYk −Yk, (41)
and the corresponding currentIk to source at output node ˆYk in the second phase of training is
Ik =β
(
Yk − ˆYk
)
. (42)
B.1.2 Softmax and Cross-Entropy Loss
In a classiﬁcation task, the target vector Y = (Y1,Y 2,...,Y K) represents the one-hot code of
the class label. In order to apply the cross-entropy loss, we view the vector of output voltages
ˆY = ( ˆY1, ˆY2,..., ˆYK) as the vector of logits, i.e. the vector of scores assigned to each of the K
classes. The vector of class probabilities associated to these logits isp = (p1,p 2,...,p K), where
∀k = 1, 2,...,K, p k = exp( ˆYk)∑K
i=1 exp( ˆYi)
. (43)
We write for short p = softmax( ˆY ) the vector of class probabilities, and pk = softmaxk( ˆY ) the
probability of classk. The cost function associated to the cross-entropy loss is
ℓ( ˆY,Y ) = −
K∑
k=1
Yk log(softmaxk( ˆY )). (44)
19

In this case the error derivatives of output nodes (logits) are given by:
∀k, ∂ℓ
∂ ˆYk
( ˆY,Y ) = softmaxk( ˆY ) −Yk, (45)
so the corresponding currentIk to be sourced at output node ˆYk in the second phase of EqProp is
Ik =β (Yk −pk). (46)
In practical neuromorphic hardware, the vector of class probabilities p = (p1,p 2,...,p K) can be
computed digitally, and then the currents Ik = β (Yk −pk) can be injected at output nodes with
current sources.
B.1.3 Binary Cross-Entropy
A special important case of the softmax/cross-entropy loss deﬁned above is the binary cross-entropy,
which is useful in the setting of GANs (section B.2).
In this case there is only one output node, i.e. ˆY is a scalar, which we view as the logit. The
probability associated to this logit is the sigmoid, deﬁned as:
σ( ˆY ) = 1
1 + exp(− ˆY )
. (47)
The binary cross-entropy cost function is then deﬁned as:
ℓBCE ( ˆY,Y ) = −Y log(σ( ˆY )) − (1 −Y ) log(1 −σ( ˆY )). (48)
The gradient with respect to the unique output node is
∂ℓBCE
∂ ˆY
( ˆY,Y ) =σ( ˆY ) −Y, (49)
and the corresponding current injected in the second phase of EqProp is
I =β
(
Y −σ( ˆY )
)
. (50)
B.1.4 Weight Decay
One can easily adapt the update rule of Theorem 1 to include a weight regularization term. Suppose
that, instead of the loss of Eq. 39, we want to optimize a loss of the form
L(X,Y,θ ) =ℓ
(
ˆY,Y
)
+ Ω(θ), (51)
where Ω(θ) is a weight regularization term. Typically, Ω(θ) is the sum of squared weights:
Ω(θ) = λ
2
∑
i,j
g2
ij, (52)
where thegij’s are the conductances of the programmable resistors. In order to compute the gradient
of L, the formula of Theorem 1 needs to be slightly changed by adding the term ∂Ω
∂gij
, which here is
equal toλgij. We get:
∂L
∂gij
= lim
β→0
1
β
[(
∆Vβ
ij
)2
−
(
∆V 0
ij
)2
]
+λgij. (53)
Note that the currents injected in the second phase of EqProp are unchanged, i.e. Ik = − ∂ℓ
∂ ˆYk
for
each output node ˆYk.
B.2 Generative Adversarial Networks
In this section, we show how the setting of Generative Adversarial Networks (GANs) [Goodfellow
et al., 2014] can be applied to analog neural networks trained with EqProp.
20

B.2.1 Description of the GAN setting
In the setting of GANs, a generative network (or simply ‘generator’, denotedG) and a discriminative
network (or simply ‘discriminator’, denoted D) are trained concurrently. The goal is to have the
generatorG produce samples (denoted ˆX) with similar statistical properties as the data samples
(denotedX) of a data distributionp(X). To achieve this, the discriminatorD is trained to distinguish
samples ˆX produced byG from true samplesX coming fromp(X). Simultaneously, the generator
G is trained to maximise the probability of D making a mistake, i.e. G is trained to ‘fool’ the
discriminator by producing samples that the discriminator thinks are part of the true data distribution
p(X).
More formally, the generatorG is parameterized by a set of weights θ and implements a function
ˆX =Gθ(Z), whereZ is a latent variable coming from a known distributionp(Z). The discriminator
D is parameterized by a set of weightsφ and implements a function ˆY =Dφ( ˜X), where ˜X is either
generated byG or coming from the true p(X), and ˆY is the probability of ˜X coming fromp(X).
The discriminator and the generator are trained to minimize the following two objectivesJD andJG
with respect toφ andθ, respectively:
JD(φ) = − EX∼p(X) [log(Dφ(X))] − EZ∼p(Z) [log(1 −Dφ(Gθ(Z)))], (54)
JG(θ) = − EZ∼p(Z) [log(Dφ(Gθ(Z)))]. (55)
B.2.2 Training the Generator
In the generatorG, a set of nodes are the ‘latent nodes’Z whose voltage values are sampled from
p(Z) and sourced. Another set of nodes are the ‘data nodes’ ˆX which correspond to the generated
sample.
The objective function for the generator (Eq. 55) rewritesJG(θ) = EZ∼p(Z) [LG(Z,θ )], with
LG(Z,θ ) =ℓG(Gθ(Z)), (56)
ℓG( ˆX) = − log(Dφ( ˆX)). (57)
Having deﬁned the lossLG and the cost functionℓG corresponding to the generatorG, we can then
use EqProp to trainG on a given latent sampleZ ∼p(Z), provided that for each data node ˆXi of
G we can compute the current Ii to be injected at ˆXi in the second phase (nudged phase). These
currents to be sourced in the nudged phase are equal to:
Ii = −β ∂ℓG
∂ ˆXi
( ˆX). (58)
Thus we need to compute ∂ℓG
∂ ˆXi
( ˆX). To do this, let us rewrite the functionℓG( ˆX) of Eq. 57 as
ℓG( ˆX) =ℓBCE (Dφ( ˆX), 1), (59)
whereℓBCE is the binary cross-entropy8 cost function presented in section B.1.3, with targetY = 1
(the generatorG is trained to trickD to classify the sample ˆX as a true data sample). We see now
from Eq. 59 that ∂ℓG
∂ ˆXi
( ˆX) represents the gradient with respect to the input nodes of the discriminator
D. This can be achieved using EqProp (again) in the discriminatorD this time, using the following
Lemma.
Lemma 5. Consider an input node of the discriminator D. Let ˆXi be the voltage sourced at this
input node andIi the current ﬂowing through it. Denote I0
i andIβ
i the currents in the ﬁrst phase
(free phase) and second phase (nudged phase) of EqProp, respectively. Then, the gradient ofℓG( ˆX)
with respect to ˆXi is equal, in the limitβ → 0, to
∂ℓG
∂ ˆXi
( ˆX) = lim
β→0
1
β
(
Iβ
i −I0
i
)
. (60)
8For more readability we have chosen here to denote ˆY =Dφ( ˆX) as the probability ofD classifying ˆX
as true data, to be consistent with the notations used in the GAN literature. However in an analog network,
ˆY =Dφ( ˆX) would actually be the logit, and the probability would beσ( ˆY ) whereσ is the sigmoid function.
21

Proof. Let P (X,V,φ ) denote the total pseudo-power of the discriminator D. Recall the form of
the total pseudo-power (Eq. 12) and note that the current ﬂowing through input node Xi of the
discriminatorD is
Ii = ∂P
∂Xi
(X,V,φ ). (61)
Now we can rewrite the formula to be proved in the form:
∂LD
∂Xi
(X) = lim
β→0
1
β
(∂P
∂Xi
(
X,V β,φ
)
− ∂P
∂Xi
(
X,V 0,φ
))
, (62)
where we recall that ℓG = LD. Since this formula must be shown for all input nodes Xi, we can
rewrite the formula to be proved more compactly as:
∂LD
∂X (X) = lim
β→0
1
β
(∂P
∂X
(
X,V β,φ
)
− ∂P
∂X
(
X,V 0,φ
))
. (63)
The proof of this formula is identical to the proof of the formula of Eq. 20.
Finally, combining Eq. 58 with Lemma 5 we see that for each data node ˆXi ofG, the required current
Ii to be injected at ˆXi in the second phase (nudged phase) is equal to:
Ii =Iβ
i −I0
i. (64)
Altogether, this gives us the following procedure to optimize the generatorG by SGD.
Equilibrium Propagation to Train the Generator of a GAN. In the ﬁrst phase (free phase),
sampleZ ∼p(Z), source the corresponding voltages at theZ nodes of the generatorG and measure
the voltages of all ﬂoating nodes in G. Then use the voltages of the ˆX nodes inG as voltage sources
for the input nodes of the discriminatorD, and measure the currentsI0
i at input nodes ofD. This
constitutes the ﬁrst phase of EqProp.
In the second phase (nudged phase), source a currentI = −β∂ℓBCE
∂ ˆY at the output node ˆY ofD and
measure the currentsIβ
i at input nodes ofD anew. For eachi, compute the differenceIi =Iβ
i −I0
i
and inject this currentIi at the date node ˆXi ofG. Measure the voltages of ﬂoating nodes in G anew.
This constitutes the second phase of EqProp.
Finally, the gradient ofLG(Z,θ ) with respect to the conductances of the generatorG can be estimated
using the formula of Theorem 1. Speciﬁcally, let∆Vij denote the voltage drop across a programmable
resistor of conductancegij in the generatorG. Denoting ∆V 0
ij and ∆Vβ
ij the voltage drops in the
ﬁrst phase (free phase) and second phase (nudged phase) of EqProp, respectively, the gradient with
respect togij is
∂LG
∂gij
= lim
β→0
1
β
[(
∆Vβ
ij
)2
−
(
∆V 0
ij
)2
]
. (65)
B.2.3 Training the Discriminator
In the discriminator D, a set of nodes are the ‘input nodes’ X whose voltages are sourced (and
correspond to either real data coming fromp(X) or fake data generated byG), and a set of nodes are
the ‘output nodes’ ˆY which correspond to the probability ofX coming fromp(X).
The objective to be optimized (Eq. 54) can be written in the form
JD(φ) = EX∼p(X)
[
L1
D(X,φ )
]
+ EZ∼p(Z)
[
L2
D(Z,φ )
]
, (66)
where L1
D and L2
D are per-sample losses deﬁned by
L1
D(X,φ ) =ℓBCE (Dφ(X), 1), (67)
L2
D(Z,φ ) =ℓBCE (Dφ(Gθ(Z)), 0). (68)
The function ℓBCE is the binary cross-entropy cost function of Section B.1.3. Computing the
gradients of L1
D(X,φ ) and L2
D(Z,φ ) with EqProp can be done as in the supervised setting.
22

Remark. Note that another possibility is to have an analog generatorG (trained with EqProp) com-
bined with a software-based discriminatorD (e.g. a multilayer perceptron) trained on conventional
computer hardware with backpropagation. In this case, the currents of Eq. 58 can also be computed
in software with backpropagation.
23

C Architecture Details (Complement of Section 4)
In this appendix, we provide details about the analog neural network architecture introduced in
section 4.
First we describe the linear resistance network model (Section C.1). Although this model is not useful
in practice, studying it is helpful to gain understanding of the working mechanisms of analog neural
networks. It helps understand the limits of linear devices and the need to introduce nonlinear devices
such as diodes (Section C.2). The linear resistance network model also helps understand the vanishing
signal effect observed in our simulations (Section C.3) and thus the need to introduce ampliﬁers
(Section C.4). Another difference between conventional neural networks and analog neural networks
is the fact that the weights (conductances) are all positive (Section C.5). This structural constraint of
analog networks requires practices that are uncommon in deep learning, such as doubling the number
of input nodes and output nodes in the network.
C.1 Linear Resistance Network: Insights and Limitations
A linear resistance network is an electrical circuit whose nodes are linked pairwise by linear resistors.
Linear resistance networks are extensively studied in the literature – see e.g. Baez and Fong [2015].
Recall that the current-voltage relationship of a linear resistor with end node voltagesVi andVj and
with conductancegij is given by Ohm’s law:Iij =gij(Vi −Vj), whereIij is the current through the
resistor. By Kirchhoff’s current law, for each ﬂoating node i in a linear resistance network, we have
the equation: ∑
jgij(Vi −Vj) = 0. This rewrites:
Vi =
∑
jgijVj
∑
jgij
. (69)
This operation resembles the usual multiply-accumulate operation of artiﬁcial neurons in conventional
deep learning, with two notable differences: ﬁrst, an additional factorGi = ∑
jgij referred to as the
G term normalizes the weighted sum of voltages; second, there is no nonlinear activation function.
Thus, in a linear resistance network, each ﬂoating node voltage Vi is a weighted mean of its adjacent
node voltages. Nonlinear components (such as diodes) are necessary to perform nonlinear operations.
C.2 Current-Voltage Transfer Function of a Diode
The transfer function of a diode takes the following form. The currentI across a diode as a function
of its voltage drop ∆V is given by
I =IS
[
exp
(∆V
ηVT
)
− 1
]
, (70)
whereIS is the reverse saturation current (IS ∼ 1µA),VT is the thermal voltage (VT ∼ 25.85 mV at
300 K) andη is the emission coefﬁcient of the diode (η = 2 for a silicon diode).
The voltage sources in series with the diodes, used to shift the bounds of the activation function, are
set to 0.3 V and −0.7 V .
C.3 Vanishing Signals Effect
Simulations show that in a circuit composed only of resistors and diodes (e.g. the network architecture
of Figure 1 without the ampliﬁers), the signal amplitudes (i.e. node voltages) decay when going from
the layer of input nodes to the ﬁrst layer of hidden nodes. In the case of a linear resistance network,
this vanishing signals effect can be explained by the formula of Eq. 69. This formula shows that each
ﬂoating node voltage is a weighted mean of its adjacent node voltages, a consequence of which is that
the extremal voltage values must be reached at input nodes (i.e. nodes whose voltages are sourced).
Another way to see it is that the current through resistive devices always ﬂows from high electric
potential to low electric potential.
24

Figure 2: Bidirectional ampliﬁer, composed of a voltage-controlled voltage source (VCVS) and a
current-controlled current source (CCCS). The output voltage (V2) is related to the input voltage (V1)
by the relationshipV2 =AV1, whereA is a gain factor. The input current (I1) is related to the output
current (I2) by the relationshipI1 = 1
AI2. In the control branches (represented by dashed lines), the
current is zero.
C.4 Bidirectional Ampliﬁer
To overcome the vanishing signal effect, we use ampliﬁers. Speciﬁcally, we design a bidirectional
ampliﬁer which ampliﬁes voltages in the forward direction by a gain factorA, and ampliﬁes currents
in the backward direction by a gain factor 1/A, whereA is a hyperparameter to be chosen. This setup
is represented in Figure 2 and can be described by the following equations:
V2 =AV1, (71)
I2 =AI1. (72)
Note that if the gain is set to A = 1, then the bidirectional ampliﬁer behaves as if it were a short
circuit, not inﬂuencing the steady state of the network.
V oltage ampliﬁcation is performed in the forward direction using a voltage-controlled voltage source
(VCVS). Speciﬁcally, the VCVS ampliﬁes the voltage at the input of the bidirectional ampliﬁer (V1 in
Fig. 2) by a factor of e.g.A = 4, resulting in an output voltage (V2 in Fig. 2) of 4x the input voltage.
Current ampliﬁcation is performed in the backward direction by a current-controlled current source
(CCCS). Speciﬁcally, the CCCS senses the current through the VCVS and reﬂects it backward at
the input of the bidirectional ampliﬁer, with a gain of1/A = 1/4. This setup gives the bidirectional
ampliﬁer we require to allow signals to propagate bidirectionally.
C.5 Constraint of Positive Weights
The weights in analog neural networks are represented by conductances, which are positive. This
contrasts with conventional neural networks which are not subject to such constraints and whose
weights are free to take either positive or negative values.
There are several ways of dealing with the constraint of positive conductances. One approach to
allow weights to be either positive or negative is to decompose each weight as the difference of two
(positive) conductances [Wang et al., 2019]. Another approach is to shift the mean of the matrix by
a constant factor as described in Section 4.1. of Hu et al. [2016]. In this work, we choose a third
approach, which consists in doubling the number of input and output nodes.
C.6 Analog Neural Networks vs Conventional Neural Networks
The computations in analog neural networks are carried out in ways that are fundamentally different
than in conventional deep learning.
To illustrate this, we consider the setting of Section 4, in which the circuit is composed of input nodes
(denotedX), internal nodes, and output nodes (denoted ˆY ). The architecture and the components
of the circuit (programmable resistors, diodes, ampliﬁers, etc.) determine theX ↦→ ˆY function. In
particular, the conductances of the programmable resistors (denotedθ) parameterize this function,
which we can write in the form ˆY =f(X,θ ).
25

In conventional deep learning, the function ˆY = f(X,θ ) implemented by a neural network is
explicitly implemented as a composition of elementary operations (such as tensor multiplications and
nonlinear activation functions). In contrast, one way to think of an analog neural network, is that its
state is described by its node voltages, and that the function ˆY =f(X,θ ) is implicitly determined by
Kirchhoff’s current law.
C.6.1 Network Architecture vs Hardware Architecture
Computer architectures for conventional deep learning involve several layers of abstractions which
separate the neural network implementation (at a software level) from the underlying device physics
(at a hardware level). This enables deep learning practitioners to design their neural networks without
necessarily understanding the hardware design, and vice-versa. This has led software engineers and
hardware engineers to adopt somewhat different languages. In particular, the term ‘architecture’ is
used both in hardware design and neural network design, but with very different meanings.
In contrast, by implementing the neural network directly at the hardware level, our framework for
end-to-end analog neural networks eliminates all intermediate abstractions between the hardware
and the neural network and thus blurs the distinction between ‘hardware architecture’ and ‘network
architecture’. This ‘architecture’ implements and determines the ˆY = f(X,θ ) input-to-output
function of the circuit.
26

D SPICE Simulation Details (Complement of Section 5)
In this appendix, we provide details about our numerical simulations with SPICE. First we describe
the general training procedure (section D.1), and then we describe the speciﬁc details related to
solving the XOR task (section D.2) and the MNIST task (section D.3).
D.1 Simulation Setup
In our simulations, we use SPICE to perform the ﬁrst phase (free phase) as well as the second phase
(nudged phase) of EqProp. The other operations are performed in Python: this includes weight
initialization and netlist generation (before training starts), data normalization (before the ﬁrst phase
of EqProp), calculating loss and gradient currents (between the ﬁrst and second phases of EqProp),
weight gradient calculation (at the end of the second phase of EqProp) and performing the weight
updates (we update resistances in software).
Weight Initialization and Netlist Generation. In SPICE, a circuit is deﬁned by anetlist, which is
a text-based representation of the circuit. For an analog network with nin input nodes, one hidden
layer ofnhidden neurons, andnout output nodes, the corresponding netlist is created by sequentially
deﬁning and linking the following components:
1. nin input voltage sources initialized at ground,
2. nin nodes representing input units,
3. a resistance matrix R1 ∈ Rnin×nhidden,
4. nhidden nodes representing the input nodes of hidden neurons,
5. nhidden diodes, where each anode is connected to a hidden node and each cathode is initialized
at ground,
6. nhidden diodes, where each anode is initialized at ground and each cathode is connected to a
hidden node,
7. nhidden CCCS current ampliﬁers,
8. nhidden VCVS current ampliﬁers,
9. nhidden nodes representing the output nodes of hidden neurons (after nonlinear transfer
function and ampliﬁcation),
10. a resistance matrix R2 ∈ Rnhidden×nout,
11. nout nodes representing output units,
12. nout output current sources initialized at ground for current injection during the weakly
clamped phase.
The conductances (weights) are initialized by drawing i.i.d. samples uniformly at random in a range
of values [L,U ]. The values ofL andU used for XOR and MNIST are provided in sections D.2 and
D.3 respectively. We note that SPICE does not support conductances, only resistances; therefore the
weights of the network are stored as resistances (R1 and R2 here).
Steps 5 and 6 form the nonlinear transfer function and steps 7 and 8 form the signal ampliﬁcation
procedure. Steps 3 through 9 may be repeated to create deeper networks (assuming appropriately
sized resistance matrices).
Training Iteration. Given a data set D of training samples (X,Y ), we optimize the network by
stochastic gradient descent (SGD). In order to perform one training iteration on a sample (X,Y ) (i.e.
to compute the corresponding gradient and to take one step of SGD), we perform the following four
steps below.
Free Phase (Inference). We set the DC voltage of each input nodei ∈ {1, 2,...,n in} toXi and
set the current through each output node to zero. We then run the SPICE simulation under standard
temperature conditions (25◦C). This calculates the (ﬁrst) steady state of the network. We then extract
the voltages at each hidden neuron (both before and after the transfer function) and output node.
We denote these voltagesH0
in,H0
out and ˆY 0, respectively. Recall that, to address the constraint of
nonnegative weights, the number of output nodes are doubled, so that ˆY 0 = { ˆY +,0
k , ˆY−,0
k }1≤k≤K,
with ˆY +,0
k − ˆY−,0
k serving as prediction for classk (1 ≤k ≤K).
27

Loss Calculation and Gradient Currents. We then calculate in software the squared error loss:
L =
∑
k
(
Yk − ˆY +,0
k + ˆY−,0
k
)2
, (73)
as well as the gradient currentsI+
k andI−
k to source at output nodes ˆY +
k and ˆY−
k :
I+
k =β (Yk − ˆY +,0
k + ˆY−,0
k ), I −
k =β ( ˆY +,0
k − ˆY−,0
k −Yk), (74)
whereβ is a hyperparameter that has the physical dimensions of a conductance.
Nudged Phase. We set the current at each output nodes ˆY +
k and ˆY−
k toI+
k andI−
k , respectively.
We then run the simulation under the same conditions as in the free phase. This simulation produces
the second steady state of the network. We extract anew the voltages at hidden and output nodes,
denotedHβ
in,Hβ
out and ˆYβ, respectively.
Weight Updates. Using the listed voltages, we calculate the voltage drops in software. For the ﬁrst
layer, we calculate ∆V 0
1 =X ⊖H0
in and ∆Vβ
1 =X ⊖Hβ
in, where ⊖ denotes the outer subtraction
operator. Similarly for the second layer, we calculate ∆V 0
2 =H0
out ⊖ ˆY 0 and ∆Vβ
2 =Hβ
out ⊖ ˆYβ.
We then read the resistances from the netlist, convert them to conductances which we gather into
matrices of conductances G1 and G2. We then update these matrices according to Theorem 1:
G1 ← G1 − α1
β
[(
∆Vβ
1
)2
−
(
∆V 0
1
)2
]
, (75)
G2 ← G2 − α2
β
[(
∆Vβ
2
)2
−
(
∆V 0
2
)2
]
, (76)
whereα1 andα2 are learning rates for the ﬁrst and second layers, respectively. All conductances
below a threshold levelL are then clipped toL, to account for the fact that real-world conductances
are strictly positive. We then convert these conductance matrices ( G1 and G2) back to resistance
matrices (R1 and R2), and we update the netlist accordingly.
Mini-batch Updates. Updating the resistance matrices in software with SPICE is time consuming,
as this framework was not designed for such purpose. To limit the time wasted in these updates, we
perform mini-batch gradient descent with mini-batches of size m = 100, i.e. each weight update
corresponds to the sum of the gradients ofm samples. However, contrary to conventional mini-batch
processing in deep learning (where allm gradients would be computed in parallel), in our setting
the gradients are computed one at a time. Indeed, data samples must be processed one at a time in
our analog neural network. For each sample within a mini-batch, we sequentially perform the ﬁrst
phase (free phase) and second phase (nudged phase) to compute the corresponding gradients, sans
weight update. The gradients are stored 9. After processing all samples of the mini-batch, we perform
the weight updates corresponding to the sum of them gradients. The weight updates of Eq. 75-76
adapted for mini-batch updates are:
G1 ← G1 − α1
mβ
m∑
i=1
[(
∆Vβ
i,1
)2
−
(
∆V 0
i,1
)2
]
, (77)
G2 ← G2 − α2
mβ
m∑
i=1
[(
∆Vβ
i,2
)2
−
(
∆V 0
i,2
)2
]
, (78)
where the indexi refers to the data samplei.
D.2 XOR Task
The XOR task consists in training a function ˆY =f(X1,X 2,θ ) to produce ˆY =X1 XORX2. The
corresponding inputs-target pairs are given by the following table.
9We note that it is possible to perform approximate mini-batch updates on analog crossbar arrays without stor-
ing the gradients [Hoskins et al., 2019]. In this case the mini-batch update corresponds to a rank-1 approximation
of the gradient for a given mini-batch.
28

X Y
⟨0, 0⟩ ⟨ 0⟩
⟨0, 1⟩ ⟨ 1⟩
⟨1, 0⟩ ⟨ 1⟩
⟨1, 1⟩ ⟨ 0⟩
We ﬁnd it necessary to shift and scale the input values. Instead of using the values0 and 1, we use
−2 and 2. Hence, our training dataset is as follows:
X Y
⟨−2, −2⟩ ⟨ 0⟩
⟨−2, 2⟩ ⟨ 1⟩
⟨2, −2⟩ ⟨ 1⟩
⟨2, 2⟩ ⟨ 0⟩
The network architecture used is represented in Figure. 3.
The conductances (weights) are initialized by drawing i.i.d. samples uniformly at random in the
range [L,U ] withL = 0.0001 S andU = 0.1 S. In the second phase (nudged phase), we scale the
gradient currents by a factorβ = 0.001. We useα = 0.001 as the learning rate (for all conductances).
During the weight update phase, all conductances that fall below the threshold levelL = 10−7 S are
clipped to 10−7 S to account for the fact that real-world conductances are strictly positive. We train
the network for 1000 iterations.
This training procedure produces a network that is able to perform the XOR operation. See Fig. 3
for the ﬁnal network after completion of training. Since this analog network is very small and only
requires 1000 training iterations, the simulation can run in under a minute on a standard computer.
Figure 3: SPICE network used to solve the XOR task, together with the ﬁnal weights after training.
The network has two input nodes (X1 andX2), plus a node whose voltage is always set to the same
value ofXbias = 1V which serves as a bias for the two hidden neurons. The symbolσ denotes the
antiparallel diodes and the bidirectional ampliﬁer which implement the nonlinear transfer function
(as in Figure 1). The gains of the ampliﬁers are set to A = 4 . To overcome the constraint of
non-negative weights (non-negative conductances), our network has two output nodes ˆY+ and ˆY−,
with the prediction being ˆY = ˆY+ − ˆY−. Also, note that SPICE does not support conductances;
therefore the weights are represented as resistances.
D.3 MNIST Classiﬁcation Task
We now give details about our simulations on the MNIST dataset (the ‘modiﬁed’ version of the
National Institute of Standards and Technology dataset) [LeCun et al., 1998]. The MNIST dataset
consists of 32x32 gray-scaled images representing digits, together with their class label (a digit
between 0 and 9). It is composed of 50,000 training samples and 10,000 test samples.
29

To overcome the constraint of non-negative weights (non-negative conductances), we double the
number of output nodes. We also double the number of input nodes and invert one set. Thus, our
network has 1568 input nodes (two nodes per pixel of a 28 × 28 image), one hidden layer of 100
neurons, and 20 output nodes (two nodes per each of the 10 digit classes). We also use a ‘node’Xbias
whose voltage is always set to the same value of 1V , which serves as a bias for the hidden neurons.
This gives usX1, −X1,X2, −X2, ...,X784, −X784 andXbias as input nodes,H1,H2, ...,H100 as
hidden neurons, and ˆY +
0 , ˆY−
0 , ˆY +
1 , ˆY−
1 , ..., ˆY +
9 , ˆY−
9 as output nodes. The prediction of the model is
by deﬁnition
Ypred = arg max
0≤i≤9
(
ˆY +
i − ˆY−
i
)
. (79)
For each weight matrix, the conductances are initialized by drawing i.i.d. samples uniformly at
random in the range [L,U ] withL = 10−7 andU = 0.08√ni+ni+1
, whereni is the fan-in andni+1 is
the fan-out of the weight matrix 10. The gains of the ampliﬁers are set to A = 4. At each training
iteration, we normalize each image sample to have mean 0 and standard deviation 5. In the second
phase (nudged phase), we scale the gradient currents by a factor β with |β| = 0.01. We choose
the sign ofβ at random for each training iteration11. We useα1 = 0.1, andα2 = 0.05 as learning
rates for the ﬁrst and second weight matrices. During the weight update phase, all conductances
that fall below the threshold level L = 10−7 S are clipped to 10−7 S to account for the fact that
real-world conductances are strictly positive. Since writing and loading new resistances for each
training iteration is costly (in software simulations), we save simulation time by performing the
weight updates every 100 iterations, by storing intermediate cumulative gradients. The resulting
weight updates are thus equivalent to those of mini-batch gradient descent with minibatches of size
m = 100 (although data samples are processed one at a time).
D.3.1 Logistic Regression Classiﬁer (Benchmark)
To demonstrate that our analog neural network (SPICE-based network) beneﬁts from the nonlinearity
of its devices, we show that it outperforms a logistic regression model. The logistic regression model
is a linear transformation of the inputs, followed by a softmax and the cross-entropy loss. For a fair
comparison with our SPICE-based network, we double the number of input nodes (we use two nodes
per pixel and invert one set), and each image sample is normalized to have mean 0 and standard
deviation 1. Thus the logistic regression model has 2 × 784 = 1568 input units and 10 output units.
The test error rate obtained is 7.27%, which is signiﬁcantly higher than the test error rate of our
SPICE implementation (3.43%). LeCun et al. [1998] also report results with different kinds of linear
classiﬁers (corresponding to different pre-processing methods), all performing signiﬁcantly worse
than our SPICE model.
D.3.2 PyTorch Implementation of EqProp (Benchmark)
We benchmark our SPICE-implementation of EqProp against a PyTorch implementation of the
original EqProp model [Scellier and Bengio, 2017]. We use two models for benchmarking: a
standard model in which the weights are free to be either positive or negative, and a positive model
with positive weights. For the positive model, we double the input units (and invert one set) and
output units as is the case in the SPICE implementation.
Standard model. In the standard model, we use the Glorot initialization scheme [Glorot and
Bengio, 2010], i.e. each weight matrix is initialized by drawing i.i.d. samples uniformly at random
in the range [L,U ], where L = −
√
6√ni+ni+1
andU =
√
6√ni+ni+1
, with ni the fan-in and ni+1 the
fan-out of the weight matrix. In the second phase (nudged phase), we use a scaling factor β with
|β| = 0.01. We choose the sign of β at random for each training iteration. We use α1 = 0.1 and
α2 = 0.05 as learning rates for the ﬁrst and second weight matrices. We use500 and 100 iterations
10We found that the Glorot initialization scheme [Glorot and Bengio, 2010] was not optimal and that using the
scaling factor 0.08 in the upper boundU yields better results. Future work should investigate better initialization
schemes for analog neural networks.
11This technique, which is also used by Scellier and Bengio [2017] and Ernoult et al. [2020], helps improve
the test accuracy.
30

to approximate the steady states during the free phase and nudged phase, respectively, with a step
size ofϵ = 0.02. We use minibatches of size 100.
Positive model. In the positive model, each weight matrix is initialized by drawing i.i.d. samples
uniformly at random in the range [L,U ], whereL = 10−7 andU = 0.08√ni+ni+1
. In the second phase
(nudged phase), we use a scaling factorβ with |β| = 0.01. We choose the sign ofβ at random for
each training iteration. We useα1 = 0.2 andα2 = 0.1. During the weight update phase, all weights
that fall below the threshold levelL = 10−7 are clipped to 10−7. We use 500 and 100 iterations to
approximate the steady states during the free phase and nudged phase, respectively, with a step size
ofϵ = 0.02. We use minibatches of size 100.
Figure 4: Training results on MNIST. SPICE EqProp (our model) is benchmarked against PyTorch
EqProp, a PyTorch implementation of the original EqProp model [Scellier and Bengio, 2017]. 100
and 500 are the numbers of hidden neurons. ‘pos. weights’ means that the weights are constrained to
be positive.
31
