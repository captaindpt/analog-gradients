# Bulk-Switching Memristor CIM Module for DNN Training

**Authors:** Yuting Wu, Qiwen Wang, Wei D. Lu (University of Michigan), et al.
**Year:** 2023
**arXiv:** 2305.14547v1

## Core Thesis
This paper demonstrates **actual on-chip training** using a **mixed-precision approach** where analog memristor CIM (Compute-In-Memory) modules perform fast VMM operations, while gradients are accumulated in high-precision digital units. Only when accumulated gradients exceed a threshold are the memristor weights physically updated.

## The Key Innovation: Practical On-Chip Training

This is the **real-world implementation** that makes memristor training practical:

```
Previous approaches (Papers 1 & 4):
    ❌ Every weight update requires physical device changes
    ❌ Needs high-precision (FP32) analog devices → Impossible
    ❌ Requires massive endurance (millions of write cycles)
    ❌ Non-linear updates cause training to fail

This Paper's Approach (Mixed-Precision):
    ✅ VMM operations: Fast analog memristors (forward pass)
    ✅ Gradient computation: High-precision digital (backward pass)
    ✅ Gradient accumulation: Digital memory
    ✅ Weight updates: Only when accumulated change > threshold
    ✅ Result: 500-1000× fewer device writes!
```

## The Device Breakthrough: Bulk-Switching RRAM (b-RRAM)

Unlike filamentary RRAM (abrupt, stochastic switching), this uses **bulk switching**:

**How it works:**
```
Traditional f-RRAM:
    Switching controlled by conductive filaments
    ❌ A few ions → Abrupt changes
    ❌ Stochastic behavior
    ❌ Hard to control precisely

Bulk RRAM (b-RRAM):
    Switching controlled by oxygen vacancy profile in bulk volume
    ✅ Many ions → Gradual changes
    ✅ Predictable behavior
    ✅ 128 programmable levels (7-bit precision!)
```

**Key characteristics:**
- **Forming-free:** Ready to use immediately (no high-voltage initialization)
- **128 conductance levels:** Much higher precision than typical memristors
- **High uniformity:** Demonstrated across 300mm wafer
- **Low noise:** Bulk switching averages out random ion motion
- **Low voltage:** <1V operation
- **Stable retention:** 24+ hours at room temperature

## Mixed-Precision Training: The Algorithm

**The workflow:**

```python
# Simplified pseudocode
W_FP = initial_weights  # High-precision digital copy
ΔW_FP = zeros()         # Accumulated weight changes
W_RRAM = map_to_memristors(W_FP)  # Physical analog weights

for epoch in training:
    for batch in data:
        # Forward pass: ANALOG (fast!)
        outputs = W_RRAM @ inputs  # Physical VMM on chip

        # Backward pass: DIGITAL (precise!)
        loss = compute_loss(outputs, labels)
        gradients = backprop(loss, W_FP)  # Use digital copy

        # Accumulate in digital
        ΔW_FP += learning_rate * gradients

        # Sparse update: Only change memristors when needed
        for each weight:
            if abs(ΔW_FP[i,j]) > threshold:
                # Program memristor to new value
                W_RRAM[i,j] = W_FP[i,j] + ΔW_FP[i,j]
                # Update digital copy
                W_FP[i,j] = W_FP[i,j] + ΔW_FP[i,j]
                # Reset accumulator
                ΔW_FP[i,j] = 0
```

**Why this is brilliant:**

```
Inference (every weight used):
    Weights stored as physical conductances
    VMM happens via Ohm's law
    O(1) time complexity
    Ultra-low power

Training (gradients accumulated digitally):
    Small updates don't require device changes
    Only significant updates trigger programming
    Devices written ~100× total vs millions in naive approach
    Endurance requirements drastically reduced
```

## The Hardware: Fully Integrated SoC

**System architecture:**
- **4 tiles** of 64×64 1T1R b-RRAM crossbar arrays
- **RISC-V processor** with 512KB memory
- **Mixed-signal peripherals:** DACs, ADCs, TIA
- **65nm CMOS** technology
- **Bit-serial operation** for improved throughput

**Each tile includes:**
- 64 8-bit DACs for input quantization
- 8-bit ADCs (shared among 8 columns)
- Transimpedance amplifiers (TIA)
- Local timing controller

**Energy efficiency:**
- **3.08 TOPS/W** (98.6 TOPS/W normalized to 1-bit)
- **2.66 nJ per VMM operation**
- Potential for 20.7 TOPS/W with optimized circuits

## Experimental Results

### On-Chip Training (LeNet on MNIST)

**Setup:**
- Network: 3-layer CNN (LeNet)
- Dataset: MNIST handwritten digits
- Weight precision: Only 2-bit during training!
- Update threshold: 4× on/off ratio

**Results:**
- **Accuracy: 97.73%** (vs 98.7% software baseline)
- **Training time: 13 epochs** to convergence
- **Weight updates: 500× reduction** vs naive approach
- **Average updates per weight: 10.8 times total**

**Key insight:** Without gradient accumulation (naive software approach), training **completely fails** (only 77.8% accuracy).

### Simulated Training (Larger Networks)

Used hardware-aware simulator accounting for all realistic constraints:

**VGG-8 on CIFAR-10:**
- Software baseline: 87.41%
- Mixed-precision: **86.51%** (0.9% drop)
- Weight precision: 4-bit
- Updates reduced: ~1000×

**ResNet-18 on CIFAR-10:**
- Software baseline: 93.18%
- Mixed-precision: **93.18%** (0% drop!)
- Weight precision: 4-bit
- Updates reduced: ~1000×
- Average programming per weight: <100 times

## The Transfer Learning Advantage

**A critical discovery:** Models trained with mixed-precision are **inherently robust** to hardware variations!

```
Traditional approach:
    1. Train on GPU (perfect)
    2. Transfer to memristor chip
    3. ❌ Accuracy drops 10-30% due to programming errors
    4. Must retrain on each individual chip (impractical!)

Mixed-precision approach:
    1. Train with hardware constraints already included
    2. Transfer to memristor chip
    3. ✅ Negligible accuracy drop (<2%)
    4. No retraining needed!
```

**Comparison after weight transfer:**
- **FP32 trained models:** Severe accuracy degradation
- **Quantization-aware training:** Still significant drops
- **Mixed-precision training:** Software-comparable accuracy maintained

This eliminates the need for expensive chip-in-the-loop retraining!

## Hardware-Aware Simulator

Organized into **3 levels of constraints:**

**Level 1: Device non-idealities**
- Quantization effects (limited precision)
- Read noise
- Limited on/off ratio
- Conductance drift

**Level 2: Array constraints**
- Dual-column mapping (for signed weights)
- Differential read

**Level 3: System constraints**
- Finite array size
- ADC quantization and noise
- Tiled architecture
- Programming errors

The simulator **accurately predicts** on-chip training results, validating its use for larger networks.

## The Paradigm This Enables

**Best of both worlds:**

```
Digital computation:
    ✅ High precision
    ✅ Flexible
    ❌ Slow VMM
    ❌ Power hungry

Pure analog computation:
    ✅ Fast VMM
    ✅ Low power
    ❌ Low precision
    ❌ Hard to update

Mixed-Precision (this paper):
    ✅ Fast VMM (analog)
    ✅ Low power (analog)
    ✅ High precision gradients (digital)
    ✅ Sparse updates (practical endurance)
    ✅ Robust to variations
    ✅ No retraining needed
```

## Comparison to Previous Papers

### vs Paper 1 (2017 TaOx/HfOx on-chip training)
| Aspect | Paper 1 | This Paper |
|--------|---------|------------|
| Training approach | Direct analog backprop | Mixed-precision |
| Device type | Filamentary RRAM | Bulk RRAM |
| Precision | ~4-5 bits | 7 bits (128 levels) |
| Forming needed? | Yes | No |
| Weight updates | Every batch | Sparse (when > threshold) |
| MNIST accuracy | 90.67% | 97.73% |
| Endurance required | Very high | 100× lower |

### vs Paper 4 (2024 AGAD algorithm)
| Aspect | AGAD Paper | This Paper |
|--------|------------|------------|
| Focus | Algorithm innovation | Hardware implementation |
| Training | Simulated | **Actual on-chip** |
| Device | Simulated ideal | Real b-RRAM devices |
| Key contribution | Chopper technique | Mixed-precision + bulk devices |
| Demonstrated | Software simulation | **Physical chip** |

### vs Paper 3 (2025 HfOxSy/HfS2)
| Aspect | Paper 3 | This Paper |
|--------|---------|------------|
| Devices | 2D materials (HfOxSy/HfS2) | Bulk oxide RRAM |
| Training | External (then deploy) | **On-chip training** |
| Precision | ~5 bits (31 levels) | 7 bits (128 levels) |
| Focus | Better materials | Better algorithm + system |
| Demonstrated | Inference with pretrained weights | **Full training loop** |

## Technical Innovations

### 1. Bit-Serial Operation
Instead of applying full analog voltages:
```
Traditional: Apply V proportional to input value
Problem: Non-linear device response

Bit-serial:
    - Break input into 8 bits
    - Apply pulses for each bit sequentially
    - ADC accumulates weighted sum
    - Result: Linear effective operation
```

### 2. Write-and-Verify Programming
```
Target: Program device to conductance G_target

Procedure:
1. Apply pulse
2. Read conductance
3. If within target range → Done
4. If too low → Apply SET pulse
5. If too high → Apply RESET pulse
6. Repeat (max 2 cycles for training)

Result: Precise programming despite device variation
```

### 3. Gradient Accumulation Threshold
```
Key hyperparameter: Update threshold

Too small:
    - Frequent device updates
    - High endurance requirement
    - Lots of programming energy

Too large:
    - Infrequent updates
    - Low effective precision
    - Slow convergence

Sweet spot (this paper):
    - Threshold = 1/15 of conductance range
    - Corresponds to 4-bit effective precision
    - ~1000× reduction in updates
```

## Practical Implications

### Energy and Speed
**LeNet on MNIST:**
- Energy: 1.9 µJ/image
- Latency: 0.46 ms/image
- With pipelining: 0.05 ms/image (7.9× speedup)

**ResNet-18 on CIFAR-10:**
- Energy: 0.24 mJ/image
- Latency: 4.9 ms/image
- With pipelining + copies: 0.2 ms/image (24× speedup)

### Endurance Requirements
**Comparison for 100 epochs training:**

```
Naive approach (update every batch):
    Total updates per weight: ~100,000
    Required endurance: >100k cycles

Mixed-precision approach:
    Total updates per weight: ~100
    Required endurance: <1k cycles
    → 1000× more practical!
```

### Scalability
Demonstrated path from small (LeNet) to large (ResNet-18) networks using:
- Tiled architecture
- 256×64 crossbar arrays
- Hierarchical organization

## Connection to Your Insight

This paper directly addresses "why use deterministic digital gates to simulate learning":

**The answer:** **Don't choose one—use both strategically!**

```
Previous thinking:
    "Either digital OR analog"

This paper's insight:
    "Digital where you need precision (gradients)
     Analog where you need speed (VMM)
     Physical devices where you want efficiency (weights)"

The substrate adapts to the computational need:
    - Electrons in silicon (digital): Precise gradient math
    - Conductance in memristors (analog): Fast matrix operations
    - No simulation—actual physics doing the computation
```

## Limitations and Future Work

**Current limitations:**
1. Network size limited (largest: ResNet-18, 22M parameters)
2. 65nm technology (older node)
3. ADC/DAC dominate energy (crossbar is <2%)
4. Only demonstrated up to 4-bit weight precision during training

**Future directions:**
1. Scale to truly large networks (billions of parameters)
2. Advanced technology nodes (e.g., 7nm, 5nm)
3. Optimized peripheral circuits (already shown: 20.7 TOPS/W possible)
4. Integration with other optimizers (demonstrated Adam, but could do more)
5. Training-from-scratch for larger models

## Bottom Line: Why This Matters

This paper provides **the missing piece** for practical analog training:

```
The Problem:
    Previous work showed analog training is possible,
    but required unrealistic device precision or massive endurance

The Solution:
    Mixed-precision training bridges the gap:
    - Uses realistic devices (4-7 bit precision)
    - Requires practical endurance (<1000 cycles)
    - Achieves software-comparable accuracy
    - Actually demonstrated on real hardware

The Impact:
    This moves analog training from
    "interesting research demo" to "potentially deployable technology"
```

**Key achievements:**
1. ✅ First on-chip training with bulk-switching RRAM
2. ✅ 97.73% MNIST accuracy (97.73% vs 98.7% baseline)
3. ✅ 500-1000× reduction in weight updates
4. ✅ Software-comparable accuracy on larger networks (simulated)
5. ✅ No retraining needed after weight transfer
6. ✅ Fully integrated SoC with RISC-V processor
7. ✅ Practical endurance requirements

**The paradigm shift:**

```
Old thinking:
    "We need perfect analog devices to do training"

New thinking:
    "We can use imperfect analog devices if we're smart about
     when and how we update them"

Result:
    Analog in-memory training becomes commercially viable
```

This is the **hardware implementation** that proves the **algorithmic concepts** from Paper 4 (AGAD) can work on **real devices**, bridging the gap between simulation and reality.

## The Grand Picture Emerging

Across Papers 1-5, we see a progression:

```
Paper 1 (2017): "We can train on memristor hardware!"
                → Proof of concept, but limited accuracy

Paper 2 (2020): "What if we use photons instead?"
                → Ultimate speed, but no on-chip training yet

Paper 3 (2025): "Better materials solve device problems"
                → HfOxSy/HfS2 for high accuracy inference

Paper 4 (2024): "Better algorithms solve reference problems"
                → AGAD makes training robust to device errors

THIS PAPER (2023): "Hybrid approach makes training practical"
                   → Mixed-precision + bulk devices = deployable system
```

The answer to "how do we train neural networks in hardware" is becoming clear:
1. Use **bulk-switching devices** (not filamentary)
2. Use **mixed-precision** (not pure analog)
3. Use **gradient accumulation** (not direct updates)
4. Use **hardware-aware training** (not transfer learning)

**Result:** Training happens **in the hardware**, weights **are physical properties**, but we're smart about **when and how** we change them.
