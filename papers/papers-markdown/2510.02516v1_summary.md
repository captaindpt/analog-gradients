# In-memory Training on Analog Devices with Limited Conductance States via Multi-tile Residual Learning

**Authors:** Jindan Li, Zhaoxian Wu, Gaowen Liu, Tayfun Gokmen, Tianyi Chen  
**Institutions:** Cornell University, Rensselaer Polytechnic Institute, Cisco Research, IBM T. J. Watson Research Center  
**Year:** 2025  
**Paper ID:** 2510.02516v1  
**arXiv:** https://arxiv.org/abs/2510.02516

## Core Thesis
Gradient-based training on analog in-memory computing (AIMC) crossbars collapses when memristive devices offer only a handful of stable conductance states (~4 bits). This paper introduces **multi-timescale residual learning (MT-RL)**, a framework that stitches multiple low-precision analog tiles into a composite high-precision weight, delivering linear-convergence guarantees and state-of-the-art accuracy under severe device constraints—without resorting to expensive closed-loop tuning or digital gradient accumulation.

---

## Why AIMC Training Breaks at Low Precision
- **Limited conductance states** (e.g., ReRAM’s practical 4-bit resolution) inflate the minimum weight increment `Δw_min`, introducing quantization noise `ζ` that scales with `Θ(α·Δw_min)` for each pulse-based update.
- **Asymmetric device responses** (positive vs. negative pulse mismatch) add a bias term `G(w)` that stops single-tile analog SGD from converging even with diminishing learning rates.
- **Theorem 1**: Analog SGD’s average distance to the optimum plateaus at `O(σ² + Δw_min)`, establishing a non-vanishing error floor.
- **Theorem 2** (matching lower bound): No algorithm relying on a single tile can beat that error floor under the same hardware assumptions.

**Key takeaway:** Pushing for higher device resolution is expensive; smarter algorithms are needed to recover digital-like accuracy on realistic low-state hardware.

---

## Multi-timescale Residual Learning (MT-RL)

### 1. Composite Weights Across Tiles
- Represent the final weight as `Ŵ = Σ_{n=0}^{N} γⁿ W^{(n)}`, where each tile `W^{(n)}` resides on its own crossbar with the same low-state device.
- Scaling factor `γ∈(0,1)` exponentially increases the composite resolution (`N` tiles ≈ `(states)^{N+1}` effective levels) while keeping each tile’s dynamic range aligned with its precision.

### 2. Residual Chaining
- Tile 0 learns the coarse solution; Tile 1 tracks the residual left by Tile 0 (scaled by `γ⁻¹`), Tile 2 tracks the residual left by Tiles 0–1, and so on.
- Each tile only needs the **descent direction** of its objective (`Pₙ* − W^{(n)}`), enabling **open-loop weight transfer** (no read-write-verify cycles).

### 3. Multi-timescale Scheduling
- Because updating a lower-index tile instantly shifts the higher tiles’ optima, MT-RL freezes tiles `{0,…,n−1}` while running an inner loop on tile `n`.
- Inner loop lengths scale as `Tₙ = Θ(γ⁻¹)` for intermediate tiles and `Θ(γ⁻ᴺ)` for the finest tile, ensuring higher tiles track slowly drifting targets.
- Algorithm 1 orchestrates nested loops so that tile `N` updates with stochastic gradients, tiles `n<N` inherit its cached residual direction, and all updates obey analog pulse physics (`F(w), G(w)` response factors).

---

## Theoretical Guarantees
- **Lemma 2 & 3**: Each tile contracts towards its drifting optimum at rate `(1−Θ(γ^p))`, with bounded steady-state error driven by `σ` (stochastic gradient noise) and `Δw_min`.
- **Theorem 3**: The Lyapunov function over all tiles decays linearly up to an asymptotic bound `Θ(γ^{-4N} (σ Δw_min)^{2/3})`.
- **Corollary 1**: The composite weight’s optimality gap satisfies  
  `lim sup E[||W* − Ŵ||²] ≤ Θ(γ^{2N} (σ Δw_min)^{2/3})`,  
  meaning each additional tile reduces the error floor exponentially in `N`.
- **Practical implication:** With 4–6 tiles and realistic `γ≈1/states`, MT-RL reaches the accuracy of digital mixed-precision training while staying fully analog.

---

## Hardware Implementation Insights
- **Forward/Backward (Figure 6):** Each tile performs its own matrix–vector multiply; currents are scaled by resistors implementing `γⁿ` before analog summation. Backprop reuses the same composite weight.
- **No closed-loop verify:** Only pulse coincidences (`p_i ∝ x_i`, `q_j ∝ δ_j`) and cached residual vectors are transferred—keeping all updates open-loop.
- **Complexity vs. Baselines (Table 5):**
  - Digital storage: `O(2D)` (only inputs & errors) vs. `O(D²+2DB)` for digital Mixed Precision (MP).
  - Per-sample latency estimate (D=512, B=100):  
    - TT-v2: 56.3 ns  
    - Analog SGD: 30.9 ns  
    - MP: 3024.5 ns  
    - **MT-RL (any tile count): 95.9 ns** — ~30× faster than MP while retaining analog storage advantages.
- **Scalability:** Latency stays bounded even as tiles→∞ because higher tiles’ updates reuse cached pulses rather than introducing new digital loops.

---

## Experimental Evidence (AIHWKIT Simulations)

### Low-capacity models (LeNet-5)
- **Fashion-MNIST (4 states):**  
  - TT-v2: 47.5% ±0.9  
  - MP: 75.6% ±0.7  
  - **MT-RL (6 tiles): 75.1% ±0.1**
- **MNIST (10 states):**  
  - TT-v2: 95.4% ±0.2  
  - MP: 99.1% ±0.0  
  - **MT-RL (6 tiles): 98.5% ±0.1**

### Deep models (ResNet-34 with analog Layer3/Layer4/FC)
- **CIFAR-10 (4 states):**  
  - TT-v2: 87.4% ±0.1  
  - MP: 93.3% ±0.0  
  - **MT-RL (6 tiles): 92.0% ±0.1**  
  - **MT-RL (8 tiles): 90.7% ±0.1** (over-tiling offers diminishing returns for 4-state case)
- **CIFAR-100 (4 states):**  
  - TT-v2: 11.3% ±0.6 (fails to learn)  
  - MP: 70.3% ±0.6  
  - **MT-RL (6 tiles): 68.5% ±0.2**  
  - **MT-RL (8 tiles): 69.6% ±0.4 (best)**
- **Higher precision (16 states):**  
  - TT-v2: 65.2% (C100), 84.3% (C10)  
  - MP: 73.2% (C100), 95.0% (C10)  
  - **MT-RL (8 tiles): 72.2% (C100), 94.4% (C10)** — nearly matching MP with dramatically lower digital overhead.

**Bottom line:** MT-RL recovers mixed-precision accuracy with fewer states and modest tile counts, while pure analog baselines fail catastrophically in the 4-state regime.

---

## Ablation Studies (Figure 7 & Appendix K)
- **Device asymmetry (`τ_max` sweep):** Accuracy remains stable across realistic asymmetry levels, confirming robustness to response-function mismatch.
- **Scaling factor `γ`:** Optimal values cluster near `γ ≈ 1/n_states`, balancing dynamic range between successive tiles. Too-large `γ` reintroduces residual spillover.
- **Toy problem:** For a simple least-squares objective with 2-bit tiles, the training loss falls along both epoch and tile-count axes—empirically validating the residual refinement story.

---

## Limitations & Future Directions
- No tape-out yet; all results rely on IBM’s AIHWKIT simulator with SoftBounds device models.
- Inner-loop scheduling assumes stable drift rates; real hardware with slow drift or device aging may require adaptive `Tₙ` tuning.
- Energy/area analysis is pencil-and-paper; chip-level validation is listed as future work.

---

## Why This Paper Matters for Analog Gradients
- Establishes a **device-aware training algorithm** that embraces, rather than fights, low-precision analog updates.
- Demonstrates that **algorithm–hardware co-design** (tile scaling, multi-timescale control, open-loop transfer) can reclaim accuracy without digital fallbacks.
- Provides both **theoretical guarantees** and **system-level metrics** (latency/storage) that make the case for near-term in-memory training on realistic crossbar arrays.

**Takeaway:** When each memristor can only store a few reliable levels, you can still train deep networks in situ—if you chain enough limited-state tiles and choreograph them across timescales. MT-RL is a blueprint for that choreography.
