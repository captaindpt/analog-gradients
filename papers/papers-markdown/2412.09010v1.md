# 2412.09010v1.pdf

Training Physical Neural Networks for Analog In-Memory
Computing
Yusuke Sakemi1,2, Yuji Okamoto 2,3, Takashi Morie 4, Sou Nobukawa 1,5,6, Takeo Hosomi 7,
and Kazuyuki Aihara 1,2
1Research Center for Mathematical Engineering, Chiba Institute of Technology, Narashino,
Japan
2International Research Center for Neurointelligence (WPI-IRCN), The University of
Tokyo, Tokyo, Japan
3Graduate School of Medicine, Kyoto University, Kyoto, Japan
4Graduate School of Life Science and Systems Engineering, Kyushu Institute of Technology,
Kitakyushu, Japan
5Department of Computer Science, Chiba Institute of Technology, Narashino, Japan
6Department of Preventive Intervention for Psychiatric Disorders, National Institute of
Mental Health, National Center of Neurology and Psychiatry, Tokyo, Japan
7NEC Corporation, Kawasaki, Japan
December 13, 2024
Abstract
In-memory computing (IMC) architectures mitigate the von Neumann bottleneck encountered in tra-
ditional deep learning accelerators. Its energy efficiency can realize deep learning-based edge applications.
However, because IMC is implemented using analog circuits, inherent non-idealities in the hardware pose
significant challenges. This paper presents physical neural networks (PNNs) for constructing physical
models of IMC. PNNs can address the synaptic current’s dependence on membrane potential—a chal-
lenge in charge-domain IMC systems. The proposed model is mathematically equivalent to spiking neural
networks with reversal potentials. With a novel technique called differentiable spike-time discretization,
the PNNs are efficiently trained. We show that hardware non-idealities traditionally viewed as detri-
mental can enhance the model’s learning performance. This bottom-up methodology was validated by
designing an IMC circuit with non-ideal characteristics using the sky130 process. When employing this
bottom-up approach, the modeling error reduced by an order of magnitude compared to conventional
top-down methods in post-layout simulations.
Introduction
Deep learning is a state-of-the-art methodology in numerous domains, including image recognition, natural
language processing, and data generation [1]. The discovery of scaling laws in deep learning models [2, 3] has
motivated the development of increasingly larger models, commonly referred to as foundation models [4, 5, 6].
Recent studies have shown that reasoning tasks can be improved through iterative computations during the
inference phase [7]. While computational power continues to be a major driver of artificial intelligence (AI)
advancements, the associated costs remain a significant barrier to broader adoption across diverse industries
[8, 9]. This issue is especially critical in edge AI systems, where energy consumption is constrained by the
limited capacity of batteries, making the need for more efficient computation paramount [10].
One promising strategy to enhance energy efficiency is fabricating dedicated hardware. Since matrix-
vector multiplication is the computational core in deep learning, parallelization greatly enhances computa-
tional efficiency [11]. Moreover, in data-driven applications such as deep learning, a substantial portion of
power consumption is due to data movement between the processor and memory, commonly referred to as
the von Neumann bottleneck [12]. Consequently, a variety of hardware accelerators have been developed,
1
arXiv:2412.09010v1  [cs.LG]  12 Dec 2024

including graphical processing units (GPUs) optimized for data centers [13], tensor processing units (TPUs)
[14], and application-specific integrated circuits (ASICs) [15, 16].
Among the various AI hardware architectures proposed to date, in-memory computing (IMC) has garnered
significant attention for its potential to achieve the highest levels of energy efficiency. IMC eliminates the von
Neumann bottleneck by performing computations directly within memory, thereby demonstrating superior
power efficiency [17, 18, 19, 20, 21, 22]. With some exceptions [23], IMC is typically implemented as an analog
computing system that sums currents flowing through resistive memory in accordance with Kirchhoff’s laws.
Various resistive memory technologies have been utilized to implement IMC, including static random access
memory (SRAM) [24, 25, 26, 27, 28], floating gate memory [29, 30], resistive random access memory (ReRAM)
[31, 32], and phase-change memory (PCM) [33].
Despite its potential, designing IMC circuits to achieve both high energy efficiency and reliability re-
mains challenging. Owing to the non-ideal characteristics of analog circuits, discrepancies may arise between
software-trained artificial neural networks (ANNs) and their hardware implementations, leading to inaccurate
inferences [18]. The primary sources of these inaccuracies are process variation and nonlinearity in circuit
characteristics. Process variation refers to the fact that even circuits fabricated from the same design data
exhibit performance variations , sometimes substantially. Nonlinearity occurs even in the absence of process
variation and represents a pure modeling error inherent to the behavior of analog components. Nonlinearity
becomes more pronounced in advanced semiconductor fabrication processes [34].
These traits are referred to as non-ideal because AI hardware research typically follows a top-down
approach, where a model is first defined, and the hardware is then developed to efficiently execute it. Although
algorithm-hardware co-design is often adopted, primarily focusing on model downsizing techniques such as
quantization, pruning, and distillation [35], complex behaviors inherent to analog hardware systems are
not easily captured. If these complex behaviors could be incorporated directly into the models, the issue
of non-ideal characteristics may be addressed. The human brain, which outperforms current AI hardware
in terms of energy efficiency, inherently embodies such a bottom-up approach. For instance, neurons and
synapses exhibit complex, nonlinear physiological characteristics, and properties of individual neurons vary
[36, 37]. Furthermore, as the brain develops, it dynamically adjusts its physiological characteristics, such as
the excitation-inhibition (E-I) balance [38], to create an efficient learning system.
Neuromorphic engineering is one such bottom-up approach [39]. Neuromorphic engineering focuses on
mimicking biological neural systems, such as implementing silicon retinas by leveraging the dynamics of
metal-oxide-semiconductor field-effect transistors (MOSFETs) in their subthreshold regions [40]. However,
they typically employed biologically plausible learning rules, such as Hebbian learning. As a consequence, they
were unable to rival the high performance of modern deep learning models empowered by backpropagation
algorithms.
Another emerging bottom-up approach is physics-aware training (PAT) [41]. PAT enables the training
of physical neural networks (PNNs) [42] by modeling any physical system, such as analog circuits or op-
tical systems, as a deep neural network. However, applying PAT to IMC circuits is challenging owing to
the significant data requirements needed to model input–output relations when adjusting weights. Further
optimization of IMC circuits would require even more extensive data, rendering this method impractical for
IMC. Therefore, to design IMC circuits using a bottom-up approach, a more efficient modeling and training
framework is needed.
In this study, we address the nonlinearity problem by modeling the IMC circuits as PNNs that precisely
capture the complex analog dynamics of an IMC circuit. Unlike PAT, our PNNs are modeled in the form of
ordinary differential equations (ODEs), and each neuron in the PNN is formulated as a spiking neuron with a
reversal potential. However, because these PNNs are based on ODEs, the training process is inefficient, scaling
with the square of the input dimension, which makes it difficult to apply to large-scale network architectures.
To overcome this challenge, we introduce differentiable spike-time discretization (DSTD), which significantly
improves computational speed and memory efficiency by 10–100 times and demonstrates the ability to train
convolutional neural networks on the CIFAR-10 dataset [43]. We also show that this method can be applied
to spiking neural networks (SNNs) based on time-to-first-spike (TTFS) coding.
Finally, to demonstrate the utility of this bottom-up design approach, we designed an IMC circuit with
non-ideal characteristics using the Sky130 process [44]. Post-layout simulations show that the modeling error
from this bottom-up approach is less than one-tenth of that from the conventional top-down approach.
2

Excitatory
synaptic current
Inhibitory
synaptic current
time
spike events
0
IMC-aware neuron model
w w w w w
w w w w w
w w w w w
w w w w w
w w w w w
synaptic
currentsspike inputs
a b
c
d
Figure 1: Comparison of the dynamics between IMC circuits and biological neurons a. Schematic
of charge-domain IMC Circuits. Input signals are delivered through horizontal lines in the form of spikes.
Upon receiving these spikes, synaptic currents are induced along the vertical lines due to interactions between
the spike signals and the memory elements, denoted as ‘W’ in the figure. The currents are integrated and
converted into voltages at capacitors. Circuit examples within the regions outlined by dashed blue lines
are illustrated in panels b and c. b. Dynamics of IMC circuits with resistors and transistor switches. At
time t1, a transistor switch is activated, allowing current to flow through a resistor with conductance σ1.
Subsequently, at time t2, another transistor switch is turned ON, similarly permits current to pass through
a resistor of conductance σ2. The resulting current induces a change in the voltage across the capacitor,
v(t). According to Ohm’s law, the current magnitude depends on the capacitor voltage v(t). c. Dynamics of
IMC circuits with current sources and transistor switches. While an ideal current source provides a constant
current that is independent of the capacitor voltage v(t), real-world current sources exhibit a behavior in
which the current varies linearly with the capacitor voltage. This non-ideal behavior can be characterized by
the parameter λ±. d. Dynamics of membrane potentials in IMC-aware neuron models. The net activity of
receptors for excitatory current p+(t) and inhibitory current p−(t) exhibits stepwise changes in response to
incoming spikes. Synaptic currents are governed by Ohm’s law, incorporating reversal potentials E+
rev and
E−
rev. This neuron model, referred to as the IMC-aware neuron model, captures the dynamics observed in
the IMC circuits described in a and b.
3

Results
The IMC circuit is structured as a crossbar array, as depicted in Fig. 1 a. In this configuration, the input
signals propagate horizontally along the axon or word lines, while the current flows vertically through the
dendrite or bit lines [18]. The magnitude of the current is determined by the state of the resistive memory
elements, denoted as ‘W’. The matrix-vector multiplication (MVM) operations within the crossbar array can
be executed through various approaches, which are broadly classified into the current domain and the charge
domain [24]. In current-domain IMC, computations are performed by summing currents, while in charge-
domain IMC, the current is stored in a capacitor, where the result is held in the form of an electric charge.
By reducing the amount of current needed for calculations, charge-domain IMC typically achieves higher
energy efficiency [45, 46, 29, 24, 47]. Additionally, its dynamics resembles those of the biological neurons,
which accumulate synaptic currents in the membrane capacitance. Consequently, this study focuses on the
charge-domain IMC owing to its potential advantages.
Figure 1 b illustrates the analog dynamics of an IMC circuit when using a synapse composed of a resistor
and transistor switch. This configuration is known as a 1-transistor-1-resistor (1T1R) memory topology
[48, 49]. When a spike signal is input, the switch turns ON, allowing current to flow. The amount of
current is controlled by the conductance of the resistor, denoted as σi, which corresponds to the weight of
the network. The current alters the voltage across the capacitor, v(t). According to Ohm’s law, the current
flowing through the resistor depends on the capacitor’s voltage. A similar dynamic behavior can also be
observed in a 1-diode-1-resistor (1D1R) topology [50, 51]. Fig. 1 c represents an IMC circuit configured
with current sources as synapses. Although an ideal current source does not depend on the capacitor voltage
Vmem, actual current sources exhibit some dependency on this voltage [52, 29]. This dependency can be
approximated linearly with the introduction of factors λ±. In a later section, we present the circuit design
and simulations based on this topology.
Biological neurons receive spike signals from other neurons via synapses, as shown in Fig. 1 d. A
leaky integrate-and-fire (LIF) model is well-known for reproducing the dynamics of biological neurons in a
computationally efficient manner [53]. When a spike signal arrives at a synapse, it prompts the release of
neurotransmitters from the presynaptic neuron, which are then detected by receptors on the postsynaptic
neuron. This interaction opens ion channels on the postsynaptic neuron, resulting in the generation of an
ionic current. The magnitude of this current is proportional to the difference between the membrane potential
and reversal potential for specific ions, which is determined by the ion concentrations inside and outside a
cell [36]. In other words, the current generated by receiving spike signals can be described in terms of two
factors: one dependent on the opening and closing of ion channels and the other on the membrane potential.
We introduce the IMC-aware neuron model, which simplifies the nonlinear dynamics of ion channels using
stepwise functions. Crucially, the dynamics of the IMC-aware neuron model is equivalent to that of the circuit
depicted in Fig. 1 b and c. For further details on this equivalence, refer to Appendix A.
The reversal potential E±
rev can be considered as a non-ideal characteristic in conventional IMC circuit
design because when |E±
rev| → ∞ , the voltage after a period of current accumulation becomes a weighted
sum of the inputs, which is core computation of ANNs. Conversely, when |E±
rev| is finite, the results deviate.
To mitigate this effect, various methods are employed, such as using operational amplifiers to set v(t) as
virtual ground [45, 30] or reducing the amplitude of the capacitor voltage [52, 29]. However, these solutions
often involve trade-offs, such as increased circuit area and reduced power efficiency, and fail to provide a
fundamental resolution.
To overcome this issue, we model the complex analog dynamics using an ODE-based PNN by introducing
an IMC-aware neuron model, which incorporates the effects of E±
rev. Because the PNNs are ODE systems, its
training is computationally inefficient. To address this, we introduce DSTD, which drastically enhances the
training speed. Finally, we designed an IMC circuit using the sky130 process [44] and show that the PNN
approach significantly reduces modeling errors compared to traditional mapping approaches.
4

Fast training by differentiable spike-time discretization
In this study, we investigate the characteristics of deep neural networks where each neuron is modeled as an
IMC-aware neuron model. The time evolution of the IMC-aware neuron model is described as follows:
d
dt v(t) = −αv(t) +
X
q∈{+,−}
pq(t) (Eq
rev − v(t)) (1)
= −

α +
X
q∈{+,−}
pq(t)

 v(t) +
X
q∈{+,−}
pq(t)Eq
rev (2)
where α is the leak constant of the neuron. E+
rev and E−
rev are the reversal potentials associated with the
excitatory (positive) and inhibitory (negative) synapses, respectively. Synaptic currents flow depending
on the activity of the channel gates, represented as p+(t) for excitatory currents and p−(t) for inhibitory
currents. This activity exhibits a stepwise behavior as a function of the input spike timing, corresponding to
synaptic currents that lack decay. Consider a scenario where N input spikes are given to this neuron within
a normalized time interval [0 , 1]. The times at which these spikes occur are T0 < T1 < . . . , TM −1 < T M = 1,
and some spikes may coincide. In this case, the membrane potential at t = 1 can be calculated analytically
(see Method):
v(1) = v(0) +
M −1X
i=0
g(ti)
f(ti) (Zi − Zi−1) , (3)
where we defined the following variables
Zi = e−PM
j=k f(tj)(tj+1−tj), f i = α +
X
q∈{+,−}
pq(ti), g i =
X
q∈{+,−}
pq(ti)Eq
rev. (4)
When E+
rev → ∞ and E−
rev → −∞, v(1) converges to a weighted sum of input spike timesti, which is the ideal
case for charge-domain computing [29] (see Method). However, in practical cases where these conditions are
not met, Eq. (3) is complex, which costs O(M N) of computations because p±(t) is a summation of input
spikes. Here, O represents the big O notation. Technically, different spike times in continuous time do not
overlap, which leads to a computational complexity of O(N2). This is much larger than the O(N) complexity
of a neuron model in ANNs; consequently, scaling the model to larger sizes is difficult [54]. To address this
issue, we propose DSTD as follows.
DSTD combines the benefits of continuous-time systems, which can differentiate spike timings [55], with
the computational efficiency of discrete-time systems [56]. Let the spike time from neuron j in layer l − 1 be
denoted as t(l−1)
j . Note that we limit the number of spikes produced by a neuron to one. During processing
in layer l, the spike time representation is approximated and converted into a discrete-time representation at
the discrete time points {T (l)
m }m=1,2,...,M, as follows:
{s(l−1)
j0 , s(l−1)
j1 , . . . , s(l−1)
jM } = DSTD

t(l−1)
j

(5)
Here, s(l−1)
jm is a spike variable that indicates whether the spike from neuron j in layer l − 1 occurred at time
T (l)
m . Owing to this approximation, the input spike can only occur at one of the M discrete time points,
which reduces the required memory for membrane potential calculations fromO(N2) to O(N M), as discussed
earlier. Importantly, the discrete-time spike variable s(l−1)
jm is not a typical all-or-none value but can take a
real value within [0 , 1], making gradient calculation possible.
Figure 2 a illustrates the processing when two spike inputs,t(l−1)
j , t (l−1)
k , are present in the lth layer. t(l)
offset
denotes the difference between t = 0 and T (l)
0 , which plays an important role in the subsequent sections. In
this scenario, because the spike t(l−1)
j is received within the time interval [T (l)
m−1, T(l)
m ], only the spike variables
s(l−1)
j m −1 and s(l−1)
jm , corresponding to time T (l)
m−1 and T (l)
m , respectively, take non-zero values. Similarly, because
t(l−1)
k exists within the time interval [ T (l)
m , T(l)
m+1], only the spike variables s(l−1)
km and s(l−1)
k m +1, corresponding
to time T (l)
m and T (l)
m+1, respectively, take non-zero values. From these discrete-time spike variables, the
membrane potential v(l)
im at each discrete time step can be computed in parallel. Subsequently, the firing
5

a
b
Spikes (discrete-time rep.)
Membrane potentials
(discrete-time rep.)
Spikes (continuous-time rep.)
l th layer
Spikes (discrete-time rep.)
Membrane potentials
(discrete-time rep.)
Spikes (continuous-time rep.)
l+1 th layer
DSTD DSTDDSTD
Time
Continuouts-time
spikes
Continuouts-time
spikes
(Parallel computation)
0.7 0.3 0.4 0.6
Discrete-time
spikes
Membrane
potentials
d
=1
M=3 M=6 M=10
c
=10 =2
Figure 2: Computation with differentiable spike-time discretization (DSTD) . a. Illustration of
calculating the firing time of a neuron in the lth layer when spikes are input from the jth and kth neurons in
the preceding l −1th layer at times t(l−1)
j and t(l−1)
k , respectively, using DSTD. First, the discrete time points
T (l)
m are determined based on an offset time t(l)
offset and a time interval ∆ τ. The discrete spike variables s(l)
im
at these time points are calculated from the continuous-time spikes using DSTD. Using these discrete-time
spikes, the membrane potential at each discrete time point is computed via an analytical solution. Based on
this membrane potential, the firing time of the neuron is computed. b. The computational method using
DSTD for a multilayer model. The output of each layer is a continuous-time spike signal, which is then
converted into a discrete-time spike signal by DSTD. This discrete spike signal serves as input to the next
layer. (continues to the next page)
6

Figure 2: (continued) c. Examples of the time evolution of the membrane potentials in a single-layer
network without learning. The input consists of 1000 random spikes generated from a uniform distribution
over the interval [0 , 1]. The weights are in a random initial state. The results for three different reversal
potentials ( |E±
rev| = 10 , 2, and 1) are shown in separate graphs. In each figure, the solid line represents
the exact trajectory of the membrane potential, while the dashed line and plotted symbols represent the
approximate membrane potential calculated using DSTD with steps M = 4 when toffset is set to 0. d. Same
as c, but the membrane potentials are plotted for different values of M in different graphs. The |E±
rev| is set
to 1.
time t(l)
i of the output spike in continuous time is calculated based on these membrane potentials. Figure 2
b illustrates the computational flow for a multilayer network. Continuous-time spikes, either from the input
layer or from each hidden layer, are transformed into discrete-time representations via DSTD. Upon receiving
the discrete-time spikes, each layer computes the membrane potential at discrete time steps and, based on
that, produces output spikes in continuous-time representation. As all computations are differentiable, the
model is trainable using the backpropagation algorithm [57].
Figure 2 c presents the time evolution of the membrane potentials for varying values of the reversal
potentials E±
rev. This evolution results from the input of 1000 uniformly distributed random spikes over the
time interval [0,1], with each spike assigned a random weight. The solid line in the figure represents the exact
time evolution, while the dashed line illustrates the approximate time evolution obtained with DSTD steps
of four ( M = 4). Because E+
rev and E−
rev serve as the upper and lower bounds of the membrane potential,
respectively, the range of the membrane potentials at time 1 narrows as |E±
rev| decreases. Note that as
|E±
rev| decreases, the approximation error in the DSTD method increases. Nevertheless, the DSTD method
achieves reasonable accuracy even when the number of DSTD steps is four. Figure 2 d compares the exact
temporal evolution of the membrane potentials (solid lines) for E+
rev = 1 and E−
rev = −1 with the approximate
evolution obtained for different values of DSTD steps M (dashed lines). For M = 3, although there are minor
discrepancies in the final membrane potential, the overall agreement is reasonable. At M = 10, the error is
reduced to a negligible level.
Figures 3 a and b illustrate the convergence of the error δerror = ⟨(|vd
i (1) − ˆvd
i (1)|⟩i,d as the number of
DSTD steps M increased and as |E±
rev| became larger. Here, vd
i (1) represents the exact membrane potential of
the ith neuron for the dth data point at time 1, and ˆvd
i (1)) is the corresponding approximation obtained with
DSTD at time 1. The notation ⟨·⟩i,d denotes averaging over neurons and data points. The input data consist
of spike trains of dimension 1000, uniformly distributed in the range [0 , 1], with 1000 data samples. Under
various conditions, an increase in the number of DSTD steps M and a large E+
rev(= −E−
rev) resulted in a
uniform error reduction. This behavior can be mathematically expressed as |v(1) − ˆv(1)| = O(M −2|E±
rev|−1).
We provide a proof of this property in Appendix B. For sufficiently large values ofM and |E±
rev|, the numerical
results aligned closely with the theoretical predictions.
The use of DSTD significantly reduces memory usage and computation time in the presence of reversal
potentials. In Fig. 3 c, we compare the computational performance, in terms of time and peak GPU memory
usage, of a single-layer network with and without the DSTD method. This network can be viewed as a
single-layer version of the RC-Spike model, which incorporates reversal potentials into ANNs (see Method
and Fig. 4 a for detail) [54]. Experiments were conducted on an NVIDIA V100 SXM2 GPU (16 GB HBM2)
using the PyTorch framework [58]. The figure shows the training time per epoch and peak GPU memory
consumption, with a network comprising 1000 neurons and a dataset with 1000 samples, utilizing a batch
size of 100. The results reflect variations in the number of input spikes, where spike timings were uniformly
distributed within the time interval [0 , 1]. The performance improvements are shown in Fig. 3 d. When the
number of DSTD steps is 10, the peak memory usage was reduced by a factor of approximately 100, while
the computation time was reduced by a factor of 30. Figures 3 e and f depict the results for the TTFS-SNN
model. TTFS-SNN is a type of SNNs in which neurons encode information using a TTFS coding scheme,
firing at most once per neuron [59, 55]. In TTFS-SNN models, computation is more complex than RC-Spike
models owing to the firing events triggered upon reaching a threshold value. As shown in Fig. 3 f , for a step
count of 20, the peak memory usage was reduced by a factor of 10, and the computation time by a factor of
1000.
7

a b
c
d
e
f
Figure 3: Basic properties of DSTD. a, b. Errors between the membrane potential values at time 1 when
using the exact solution and the approximated solution with DSTD. The membrane potential is obtained
from a single-layer, untrained network consisting of 10 neurons, with spike inputs. The data consist of 1000
samples, with each sample containing 1000 input spikes. The input spike times are uniformly distributed
over the interval [0 , 1], and the synaptic weights are randomly assigned. In a., a double logarithmic plot
illustrates how the error decreases as the number of DSTD steps M increases, for various values of |E±
rev|
(with E+
rev = −E−
rev). In b., the error is plotted as a function of |E±
rev| (with E+
rev = −E−
rev), for different
numbers of DSTD steps M. In both a and b, dashed lines denote experimental results, while solid lines
denotes theoretical predictions. c, d. These figures depict the computational efficiency for the case of RC-
Spike models when the number of input spikes is varied for a single-layer network consisting of 1000 neurons.
The dataset consists of 1000 samples, with a batch size of 100. The input spike times for each sample are
uniformly distributed over the interval [0 , 1]. The top and second panels in c shows the computation time
and GPU memory usage, respectively, and the ratio between these quantities is shown in d. e, f. Same as c
and d but for the cases of TTFS-SNN models.
8

Time
T
0
Accumuation phase Firing phase
Input spikes0 2TSpike timing
a
c
e
b
d
f
Data
# 1
# 2
Normalized features Label
Input spikes for data #1
(0th feature)
(1st feature)
Time
Time
0
1
0
0
1
1
Figure 4: Learning results for fully connected RC-Spike models on Fashion-MNIST dataset.
a. The RC-Spike model operates in two phases: the accumulation phase and firing phase. During the
accumulation phase, the neuron membrane potential evolves over time as it receives spikes from the preceding
layer. In this example, three input spikes arrive at times t1, t2, and t3. Between receiving consecutive spikes,
the membrane potential follows an exponential decay curve (RC-decay), influenced by the reversal potential.
In the firing phase, the membrane potential increases linearly, and a spike is generated when it exceeds the
firing threshold. b. Each data sample in the Fashion-MNIST dataset is represented by a 784-dimensional
feature vector, where each feature xi is converted into a single input spike, t(0)
i = 1 − xi. The RC-Spike model
processes the data using these input spikes. c. Learning curves with error bars representing the standard
deviation across five trials with different initial weights. Each panel shows the results for different values of
DSTD steps M. Additionally, we show the cases where the offset time toffset was randomized (random offset)
and fixed (fixed offset) for each mini-batch. (continues to the next page)
9

Figure 4: (continued) d. The histogram of spike timing for the first, second, and third layer, from top to
bottom. For each layer, we show the cases for various values of |E±
rev|. e. Accuracies obtained from training
the model with different DSTD steps M. For each M, results for |E±
rev| = 4, 2, and 1 are presented from
top to bottom. Each panel compares the model performance with and without the introduction of a random
offset. Each data point represents the mean and standard deviation of the training results from five models,
each initialized with different random weights. f. Recognition accuracy of models trained with 15 DSTD
steps (upper) and 3 DSTD steps (lower) as a function of varying |E±
rev|. The 95% confidence intervals are
estimated using Gaussian process regression. The green line represents the performance of a model trained
with |E±
rev| = 100 (equivalent to an ANN). The green dashed line represents the performance of the ANN
case, but the positive and negative weights are optimized for specific |E±
rev| values in the test phase.
Reversal potentials are not nonidealities
DSTD facilitates the training of large-scale neural networks in which neurons possess reversal potentials.
These reversal potentials can be regarded as non-ideal characteristics inherent to conventional charge-domain
IMC circuits. Next, we investigate how the reversal potentials, E±
rev, influence learning performance. Ad-
ditionally, we examine the impact of the approximations introduced by DSTD on learning dynamics. We
set E+
rev = −E−
rev. We utilized the RC-Spike model to directly assess the effect of E±
rev. As depicted in Fig.
4 a, the RC-Spike model operates in two temporal phases: an accumulation phase, where the membrane
potential evolves in response to input spikes, and a firing phase, during which spikes are generated [54]. The
membrane potential undergoes exponential decay between successive input spikes, as illustrated in Fig. 4 a,
a behavior driven by the reversal potentials. Notably, when |E±
rev| → ∞ , the model’s behavior converges to
the ideal operation of charge-domain computing [29, 46], where the output of each layer corresponds precisely
to a weighted sum with a hard sigmoid activation function. Consequently, the RC-Spike model continuously
interpolate between ideal and non-ideal behaviors of charge-domain computing through the parameter E±
rev.
For detailed mathematical formulations, refer to the Methods section. A similar set of experiments was
conducted with TTFS-SNN, with the results summarized in Appendix C.
Figure 4 presents the results of training on the Fashion-MNIST dataset [60] using DSTD. As illustrated in
Fig. 4 b, the Fashion-MNIST dataset consists of 60,000 training images and 10,000 test images, where each
image is a 28 ×28 grayscale representation (a 2-dimensional tensor) of fashion items, such as T-shirts and
sneakers. Each element of the tensor corresponds to the pixel intensity of the image. To train the RC-Spike
model on these image data, the 28×28 two-dimensional image was first flattened into a one-dimensional tensor,
resulting in a 784-dimensional vector. Each element xi ∈ [0, 1] (i = 0, 1, . . . ,783) represents the normalized
pixel intensity. The input spike times were then computed as t(0)
i = 1 − xi, which served as the input to
the RC-Spike model. The RC-Spike model employed was a fully connected network with two hidden layers
(784-400-400-10). In these experiments, additive Gaussian noise with a mean of 0 and a standard deviation
of σspike was applied to the output spikes, including during the inference phase. This is because, when the
values of |E±
rev| are small, the network tends to minimize the membrane potential to avoid the influence of
|E±
rev|. However, from a circuit implementation perspective, this is undesirable as both membrane potentials
and output spikes have finite precision. To consider this limited precision, spike noise was introduced. We
set σspike = 0.01 throughout this study. Refer to Appendix D for details on the results with various levels
of spike noise. Furthermore, DSTD was also applied during the test phase. To ensure reliable evaluations, a
large number of DSTD steps were utilized during the test phase, as detailed in Appendix E.
Figure 4 c presents the learning curves of the training process, which was performed using the Adam
optimizer with a mini-batch size of 32 over 50 epochs (see Method). The errors represent the standard
deviation across five independently initialized networks. The upper panel shows the results for |E±
rev| = 4,
while the lower panel corresponds to |E±
rev| = 1. Each panel compares the results for DSTD steps M = 3
and M = 15. Additionally, two conditions were compared: one where the offset time for discretization, t(l)
offset
(see Fig. 2 a), was randomized across mini-batches (random offset), and the other where it was fixed at
toffset = 0 (fixed offset). In all the conditions, the learning process was stable. However, for M = 3 under
the fixed offset condition, recognition performance decreased significantly, particularly for |E±
rev| = 1. By
contrast, even with M = 3, the random offset condition achieved an accuracy comparable to that observed
for M = 15.
Figure 4 d illustrates the distribution of spike times across the layers after training. The results indicate
that larger values of |E±
rev| are associated with an increase in early firing events. This phenomenon can be
10

attributed to E+
rev, which represents the upper bound of the membrane potential; a larger E+
rev facilitates the
generation of higher membrane potentials. Note that higher membrane potentials correspond to earlier firing
times.
Figure 4 e presents the recognition performance results when varying the number of DSTD steps, M.
Each panel, from top to bottom, shows the results for |E±
rev| = 4, |E±
rev| = 2, and |E±
rev| = 1. In each panel,
the results are shown for the case where the time offset t(l)
offset was randomized (random offset) and where
t(l)
offset = 0 (fixed offset). The error bars for each plot represent the standard deviation across five models
initialized with different random weights. In all cases, the recognition accuracy saturated when the number
of DSTD steps, M, reached approximately 15. Furthermore, in the random offset condition, performance
saturated with fewer DSTD steps, which could be attributed to the reduction in the gradient bias caused by
time-discretization of spikes. Notably, this effect was more pronounced when |E±
rev| was small, corresponding
to the case of stronger non-ideal characteristics.
Figure 4 f illustrates the change in recognition performance as |E±
rev| was varied. The upper panel shows
results for DSTD steps of M = 15, while the lower panel corresponds to M = 3. In both the random offset
and fixed offset cases, a decrease in |E±
rev|, which reflects an increase in non-ideal characteristics, resulted
in performance degradation. However, even in the most extreme case where |E±
rev| = 1, the performance
drop was at most 1 point. Moreover, in the random offset case, the performance degradation was mitigated
compared to the fixed offset condition, particularly for DSTD stepsM = 3. We also compared the approach of
modeling non-ideal characteristics using ODE-based PNNs (proposed approach) with the conventional ANN-
to-hardware mapping approach. The network trained with |E±
rev| = 100 represents a model that minimally
incorporates non-ideal characteristics, akin to an ANN. The inference results of this network under various
E±
rev values are represented as green solid lines (referred to as ANN). Additionally, we present the results
in which the positive and negative weights of the network were optimized during inference to incorporate
the effect of the reversal potential (referred to as scaled ANN). Specifically, the values of a scaling vector
(α+, α−), where positive and negative trained weights are respectively scaled by α+ and α−, are optimized
for each value of |E±
rev| during the inference phase. The detail of the optimization procedure is described in
Appendix F. Although the scaled ANN achieved higher recognition performance than the standard ANN,
it performance degraded significantly when |E±
rev| < 4. This result indicates that non-ideal characteristics
modeled by the reversal potential are difficult to capture accurately using a conventional ANN.
We conducted similar experiments with a larger-scale convolutional network. The network architecture
was similar to Ref. [61]: Conv(64)-Conv(64)-PL-Conv(128)-Conv(128)-PL-Conv(256)-Conv(256)-PL-512-10,
where Conv(N) represents a convolution layer with a kernel size of 3 × 3, number of output channels N, and
stride 1, while PL represents a pooling layer with kernel size of 2 × 2, stride 2. As shown in Fig. 5 a, we
evaluated the model performance on the CIFAR-10 dataset [43]. The CIFAR-10 dataset consists of natural
color images with dimensions of 32 × 32 pixels and three RGB channels. This dataset can be represented
as a three-dimensional tensor xijk. Each element of this tensor was converted into input spikes and fed into
the RC-Spike model for training (see Method for details). The learning curve is presented in Fig. 5 b. As
shown, stable learning was achieved under various DSTD steps M and different values of |E±
rev|.
Figure 5 c illustrates the variation in recognition performance as M was altered under different |E±
rev|
conditions. Similar to the results in Fig. 4 e, recognition performance was maximized when M ≳ 10. In the
case of fixed offsets, recognition performance decreased when the number of DSTD steps was small. However,
with the introduction of random offsets, maximum performance was achieved even at M = 2. Figure 5 d
shows the impact on recognition performance as |E±
rev| was varied, with DSTD steps set to M = 10 or 15.
As with the MLP results, recognition performance degraded when |E±
rev| was extremely small (approximately
∼ 1), particularly when M was small. However, this degradation was mitigated with the introduction
of random offsets. Unlike the MLP network, which exhibited a monotonic increase in performance with
increasing |E±
rev|, the convolutional network demonstrated peak performance at specific |E±
rev| values. This
result indicates that reversal potentials E±
rev, which have traditionally been considered non-idealities in circuit
design, can be exploited to improve model performance if these characteristic are properly incorporated into
models.
Examination of PNN approach by Circuit Simulations
As depicted in Fig. 1, the effects of the reversal potential in neuron models corresponds to the non-ideal
characteristics observed in IMC circuits, particularly the membrane potential-dependent behavior of current.
In the previous section, we introduced a method for accelerating models that incorporate this non-ideal
11

i: horizontal position
j: vertical position
k: color (RGB)
Time
0 1
Convert into spike timing
a
c d
b
Figure 5: Learning results for convolutional RC-Spike models on CIFAR-10. a. Natural image
data from the CIFAR-10 dataset normalized and represented as a three-dimensional tensor xijk. Each data
point is converted into an input spike, which is then processed and learned by the RC-Spike model with
a convolutional architecture. b. Learning curves for |E±
rev| = 4 (upper) and |E±
rev| = 1 (lower). Each
panel shows the results for different DSTD steps M, along with the outcomes for both fixed offset time and
randomized offset (random offset). The average recognition performance across five networks with different
initial weights is plotted, with error bars indicating the standard deviation. c. Recognition performance
with changes in the DSTD steps M for different values of |E±
rev|. The values of |E±
rev| in each panel are 4, 2,
and 1, respectively, from top to bottom. Results with and without the introduction of a random offset are
compared in each panel. d. Changes in recognition accuracy as a function of |E±
rev| for DSTD steps M = 15
(upper) and M = 4 (lower). In each panel, the effects of introducing a random offset are compared. The 95%
confidence intervals are estimated using Gaussian process regression. Additionally, the recognition accuracy
of an ANN model, which corresponds to the RC-Spike model with |E±
rev| = 100, is shown by a green solid
line. The recognition accuracy of the ANN model, when its positive and negative weights were optimally
scaled, is represented by a green dashed line.
12

TimeInput layer
1st layer
2nd layer
1st data 2nd data
Green: reset phase, Blue: accumulation phase
Yellow: firing phase,         : spikes
0
1st layer 2nd layer
Phase diagram Digital signals
2 3 4 5-
a
c d
b
Synapses
Input spikes
Synapses
Axons
Axons
Neurons
Neurons
NS11 NM11 PS11 PM11 NS21 NM21 PS21 PM21
NS12 NM12 PS12 PM12 NS22 NM22 PS22 PM22
To neuron crcuit To neuron crcuit
Axon Dendrite Current
Discharger
Sensing inverter
Synaptic 
currents
NAND gate Inverter Transmission gate
Input component Output component
Figure 6: Circuit overview. a. Circuit layout of the RC-Spike circuit designed using the sky130 PDK. This
circuit forms a two-layer network, with each layer composed of five neurons. The inset shows a schematic
overview of the layout. One synapse circuit in the second layer and one neuron circuit in the first layer are
enclosed by orange dotted and dashed lines, respectively. b. Timing diagram. Each layer of the RC-Spike
circuit operates through three phases: the reset phase, accumulation phase, and firing phase. These phases
are controlled by two digital signals: the phase signal and reset signal. During the accumulation phase, spike
signals are received by each layer, while in the firing phase, the spike signals are transmitted. c. Circuit
diagram of the synapse circuit. The diagram illustrates the case of two inputs and two outputs for simplicity.
The circuit employs a paired configuration of MOSFETs for weights and selectors, commonly referred to as
a 1T1R topology. Two types of pairs exist: one composed of N-type MOSFETs (NMOS) for positive weights
(denoted as NM ij and NS ij) and the other composed of P-type MOSFETs (PMOS) for negative weights
(denoted as PM ij and PS ij). Note that the membrane potential in the circuit is inverted in polarity. The
MOSFETs responsible for the weights (NM ij and PMij) regulate the current, with the amount controlled by
the bias voltage V N(P)
ij . In this simulation, the bias voltage is externally supplied. The selector MOSFETs
turn ON when a spike signal V spike
i arrives, allowing current to flow into the dendrites. All MOSFETs used
in the synapse circuit have a gate length of 250 nm and a gate width of 1 µm. d. Circuit diagram of the
neuron circuit. The neuron circuit consists of two components: the input component and output component.
The input component stores current from the synapse circuit during the accumulation phase. The output
component generates a spike during the firing phase. During the firing phase, the discharger circuit decreases
the membrane potential at a constant rate, and the sensing inverter triggers a spike when the membrane
potential falls below a threshold value, by inverting its output.
13

characteristic (reversal potential E±
rev) when computed on digital platforms such as GPUs. Through numer-
ical experiments, we demonstrated that the proposed method prevents degradation in learning performance
caused by these non-idealities and, in some cases, enhances learning performance. In this section, we evaluate
the efficacy of the proposed method in a post-layout circuit simulation. We designed an IMC circuit based
on the architecture illustrated in Fig. 1 c. Figure 6 a presents the layout of the designed circuit (RC-Spike
circuit), which was designed using the open-source processing design kit (PDK) for the sky130 process [44].
All design tools employed were also open-source software [62, 63, 64, 65].
The circuit consists of two layers: a hidden layer and an output layer, both sharing an identical structure.
Each layer includes synapse circuits and neuron circuits, which emulate five synapses and five neurons,
respectively. Within each layer, the synapse circuit receives input spikes and delivers synaptic current to the
neuron circuit. The neuron circuit accumulates the synaptic current in a capacitor, converting it into voltage,
and subsequently generates spikes, which are transmitted to the synapse circuit of the subsequent layer (refer
to the inset in Fig. 6 a). Figure 6 b illustrates the overall operation of the circuit. The upper panel shows
the waveform of the digital control signals applied to the circuit. These digital signals, provided externally,
control the circuit’s operation. The lower panel outlines the phase transitions within the circuit. Each layer
operates cyclically, alternating among the reset, accumulation, and firing phases. Each phase persists for a
duration of T circ. Since the timing of these phases is shifted for each layer, the circuit operates in a pipelined
manner, enabling efficient processing [46, 29].
Figure 6 c presents the schematic diagram of the synapse circuit, illustrating a case with two inputs
and two outputs for simplicity (the actual circuit has five inputs and five outputs). When a spike pulse
V spike
j arrives (transitioning from 0 V to the supply voltage Vdd, with the opposite transition for V spike
j ), the
corresponding selector metal-oxide-semiconductor field-effect transistors (MOSFETs) (NS ij and PS ij) are
activated, allowing current to flow. The magnitude of this current is determined by the weight MOSFETs
(NMij and PM ij), which operate as non-ideal current sources. In this circuit, the current of the weight
MOSFETs is controlled by applying a bias voltage for simplicity. However, alternative memory technologies,
such as SRAM [46] or non-volatile memories such as floating-gate devices [29], can also be used to set the
current. Figure 6 d shows the schematic of the neuron circuit, which consists of input and output stages. The
input stage charges a capacitor using the synaptic current, while the output stage generates the spike signal.
The neuron circuit operates in three distinct phases: the reset, accumulation, and firing phases. During
the reset phase, Vreset is activated, fixing the voltage across the membrane capacitor Cm (the membrane
potential) to the resting potential V0. Simultaneously, node Vout is grounded, bringing the output spike
V spike
i to the ground potential. In the accumulation phase, both Vreset and Vphase are deactivated, allowing
synaptic current to flow into (or out of) the capacitor, which is constructed with a metal-insulator-metal
(MIM) structure. During the firing phase, Vreset remains deactivated while Vphase is activated. In this phase,
the discharger circuit operates to lower the membrane potential. Simultaneously, the sensing inverter monitors
the membrane potential, triggering an output spike when the potential falls below a certain threshold voltage.
For further details on the circuit characteristics, refer to Appendix G.
To process data using the RC-Spike circuit, it is essential to precisely replicate the software-trained model
on the hardware through a process known as model-to-hardware mapping. The mapping and simulation
methodology is illustrated in Fig. 7 a. The procedure for mapping an ANN model to hardware involved
first training the model on the Iris dataset [66, 67], followed by converting the trained model’s weights into
synaptic current values (the ANN-to-IMC mapping). When mapping the PNN model (in this case, we used
a RC-Spike model) to hardware (the PNN-to-IMC mapping), the reversal potentials, E±
rev, from the circuit’s
non-ideal characteristics were first estimated via circuit simulations. These reversal potentials were then
used to construct the RC-Spike model. The remaining steps were the same as those used in ANN model
mapping. Note that the designed circuit exhibited significant non-ideal characteristics, corresponding to
reversal potentials of E+
rev = 2 .80 and E−
rev = −1.53. During the model-to-hardware mapping, a uniform
scaling of the currents, corresponding to network weights, was required to mitigate the effects of parasitic
capacitances present in the hardware. Specifically, the values of a scaling vector (α+, α−), where positive and
negative trained currents are respectively scaled by α+ and α−, respectively, were optimized for achieving
the best results in SPICE simulation. In the case of the ANN-to-IMC mapping, both scaling elements α+
and α− are separately scaled (2-dimensional scaling) to also mitigate the effects of hardware nonidealities,
corresponding to reversal potentials. Conversely, in the PNN-to-IMC mapping, only a global scaling was
applied (1-dimensional scaling, where α+ = α−), to only account the effects of parasitic capacitance. Post-
layout circuit simulations were carried out using the open-source tools magic and ngspice [63, 64]. For a more
detailed description of the mapping methods and the hardware and model parameters, refer to the Method
14

ANN-to-IMC mapping
PNN-to-IMC mapping
SPICE simulationModel
Data
# 1
Features Label
# 2
0
1
Iris dataset
Train Weight mapping
(2-dim. optimization)
Model
 SPICE simulation
Train
2nd step
Obtain
1st step
3rd step
Weight mapping
(1-dim. optimization)
2nd step1st step
a
cb
d
(model) (SPICE) (model) (SPICE)(SPICE) (SPICE)
Figure 7: Circuit simulation results. a. Outline of methods for reproducing the behavior of ANNs
and PNNs, the RC-Spike model in this case, on IMC hardware, termed ANN-to-IMC and PNN-to-IMC
mapping, respectively. In the ANN-to-IMC mapping, we first train an ANN model (approximated here
using |E±
rev| = 10 2 for the RC-Spike model) on the Iris dataset to extract its weights. These weights are
subsequently mapped to current values in the synaptic circuits of the RC-Spike hardware based on specific
hardware parameters. In the PNN-to-IMC mapping, the process begins with determining E±
rev to construct
the PNN model (the RC-Spike model in this case). The procedure then follows the same steps as in the
ANN-to-IMC mapping. To account for the influence of parasitic capacitance in the hardware, current values
are appropriately scaled. In the case of ANN-to-IMC mapping, both positive and negative currents are scaled
and optimized to account for the effects ofE±
rev present in the hardware (2-dimensional scaling). For the PNN-
to-IMC mapping, the absolute values of weights are scaled uniformly without distinguishing between positive
and negative values (1-dimensional scaling). b. Comparison between the results of the mathematical model
and the SPICE simulation for the ANN-to-IMC mapping. The upper panel illustrates the hidden layer’s
results, while the lower panel shows the output layer’s results. In each panel, the reset, accumulation, and
firing phases are highlighted in green, blue, and yellow, respectively. The time evolution of the membrane
potential as predicted by the mathematical model is depicted by the orange dashed line, where firing events
are modeled by resetting the membrane potential to its resting value. The SPICE simulation’s membrane
potential time course is shown as a black solid line, and the output spike signals are represented by blue
dashdotted lines. c. Comparison between the mathematical model and SPICE simulation results for the
PNN-to-IMC mapping, with the same notation and line styles as described in b. d. Histogram of the timing
differences in output layer spikes between the mathematical model and SPICE simulation. The top and
bottom panels present the results for the ANN-to-IMC mapping and PNN-to-IMC mapping, respectively.
The root mean square error (RMSE) is indicated in the upper-left corner of each panel.
15

section and Appendix F.
Figure 7 b presents the results of the model and the circuit simulation for the ANN-to-IMC mapping case.
In the hidden layer (upper panel) and output layer (lower panel), the reset, accumulation, and firing phases
are masked in green, blue, and yellow, respectively. We set T circ = 1 µs. The temporal evolution of the
membrane potential in the mathematical model is depicted with an orange dashed line, while the firing time
is represented in the firing phase by a reset of the membrane potential to the resting potential ( V0 = 1.3 V).
The temporal evolution of the membrane potential in the circuit simulation is shown with a solid black line,
and the spike signals are indicated by a blue dashdotted line. The input data in Fig. 7 b correspond to the
instance among 50 test data points where the root mean square error (RMSE) between the firing times of the
output layer in the model and those in the circuit simulation was the largest. As seen in the figure, the firing
times in the output layer were not consistent between the model and SPICE simulation results. Moreover, for
the neuron in the hidden layer that exhibited the lowest voltage during the accumulation phase, the results of
the model and circuit simulation differed significantly. This discrepancy highlights the ANN model’ inability
to fully capture the effects of the hardware’s intrinsic nonidealities E±
rev. Figure 7 c shows the results for the
PNN-to-IMC mapping in a similar manner. Unlike the case with the ANN-to-IMC mapping, the results of
the mathematical model and SPICE simulation were almost entirely consistent.
Figure 7 d presents the distribution of the timing discrepancies between the firing times observed in the
mathematical model and those obtained from SPICE simulations at the output layer. This histogram is
based on 50 test samples from the Iris dataset. The upper and lower panels illustrate the results for the
ANN-to-IMC mapping and PNN-to-IMC mapping, respectively. We note that because of the relatively small
scale of both the model and dataset, the comparison focused on the output layer’s firing times, rather than
recognition accuracy, to evaluate the modeling error. Both histograms exhibited a peak at zero, but the
spread of the histogram is noticeably broader in the ANN-to-IMC mapping compared to that in the PNN-to-
IMC mapping. The RMSE of the firing time differences was 39.04 ns for the ANN-to-IMC mapping and 1.97
ns for the SNN-to-IMC mapping, demonstrating a reduction in error by more than an order of magnitude.
These results indicate that incorporating non-ideal circuit characteristics, such as reversal potentials, into
the model can reduce the modeling error substantially.
Discussion
In this study, we integrated the dynamics of IMC circuits with non-ideal characteristics into a PNN model,
represented as an ODE. Specifically, we showed that the non-ideal property, wherein synaptic currents are
dependent on the magnitude of the membrane potential, can be mathematically formulated using a neuron
model that includes reversal potentials, E±
rev. We called this model the RC-Spike [54]. As the RC-Spike
model provides an analytical solution, it can be utilized for learning. However, the computational complexity
is proportional to the square of the number of input spikes, posing a challenge for scaling the network. To
mitigate this, we introduced DSTD, which successfully reduces the computational cost to linear scaling with
respect to the number of input spikes, similar to conventional ANNs. This advancement enables the efficient
training of large-scale neural networks.
Numerical simulations revealed that the DSTD method did not require a large number of discretization
steps. Particularly, in the RC-Spike model, offset randomization of discrete time toffset for each mini-batch
allowed effective learning with as few as four DSTD steps. By contrast, the TTFS-SNN model (see Appendix
C) required up to 30 DSTD steps, depending on the value of E±
rev. This may be due to the fact that neurons
can fire at any timing in TTFS-SNN models, which may require precise calculation of membrane potentials.
In experiments using the convolutional RC-Spike model trained on the CIFAR-10 dataset, performance
was maximized when |E±
rev| ∼ 3, a region where non-ideal characteristics were present. These findings suggest
that, by leveraging the analog dynamics as modeled in PNNs, circuit characteristics that were previously
regarded as non-ideal could be harnessed, turning them into an advantage for model performance.
DSTD is closely related to the learning algorithm of SNNs, as implied by the introduction of spike
variables. Recent studies have shown that by adopting deep learning algorithms, particularly error backprop-
agation algorithms, multilayer SNNs can be trained efficiently [68, 69]. Successful backpropagation-based
SNN training methods include the surrogate gradient method and timing-based methods.
The surrogate gradient method is a learning rule based on the backpropagation algorithm particularly
designed for discrete-time SNNs [70]. In these discrete-time SNNs, binary spike variables indicate the oc-
currence of spikes at each discrete time step [71]. By approximating the small changes in spikes caused by
membrane potential fluctuations with a suitable surrogate function, the entire network can be trained using
16

backpropagation through time (BPTT) [72, 73, 74, 71]. The surrogate gradient method has demonstrated
high performance in numerous studies [75, 76, 77]. Although some studies have demonstrated the theoretical
validity of the surrogate gradient [78, 79], the computation of the gradient remains heuristic. Moreover,
mapping discrete-time SNNs onto continuous-time analog hardware is not straightforward [80]. For instance,
reducing discretization errors requires shortening the time step, which increases the number of steps needed
for training and consequently escalates training costs.
The timing-based learning generally targets continuous-time SNNs and imposes the constraint that each
neuron can fire at most once [59], refered to as TTFS coding. In cases where an analytical solution for spike
timing can be obtained [55, 81, 82, 83], error backpropagation becomes straightforward. This learning rule
allows for exact gradient computation and implements temporal coding, an information processing mechanism
based on spike timing. However, Owing to computational efficiency challenges, the timing-based learning has
not been widely applied to large-scale networks, and its use in training models capable of solving modern
benchmarks like CIFAR-10 remains limited [84, 85]. Recently, extensions of the timing-based method that
relax the constraint of single spike firing have been proposed [86, 87]. Additionally, Kheradpisheh et al. [56]
implemented timing-based learning in discrete time, while Kim et al. [88] proposed a method that combines
the surrogate gradient method with the timing-based learning in discrete-time systems. However, the gradient
computation in these discrete-time SNNs remains heuristic. Park et al. [89] successfully trained time-
discretized two-phase SNNs using timing-based learning rules. However, the model employs instantaneous
post-synaptic potentials, which do not reflect the dynamics of continuous analog systems.
DSTD can be viewd as a hybrid method that combines the computational efficiency of discrete-time SNNs
used in the surrogate gradient method with the precise gradient computation found in continuous-time SNNs
employed by the timing-based method. While DSTD discretizes spikes in time, similar to the surrogate
gradient method, it differs in that the spike variables are represented by real values instead of binary ones.
These values depend on the original continuous spike timing, allowing for differentiation with respect to time.
Additionally, by increasing the number of steps, DSTD can exactly match ODEs, making the relationship
between approximate and exact gradients explicit (see Appendix B).
In this study, we developed a fast learning method by utilizing the analytical solutions of ODEs. ODE
systems can also be trained using the adjoint method without relying on analytical solutions [90, 73, 86].
However, the adjoint method requires computationally expensive ODE solvers, making it difficult to train
large-scale networks. To solve ODEs efficiently, a well-known method exits that discretizes time with rela-
tively coarse step sizes (moderate step sizes) and connects them using analytical solutions [91]. This method
has been compared with the Runge-Kutta method in simulations of the Hodgkin-Huxley model [92]. How-
ever, this approach assumes that input spikes occur at specified time steps, leading to discrepancies with
the exact solution when using coarse step sizes [91]. Moreover, because it handles discrete time, it cannot
capture gradients concerning spike timing displacements. The DSTD method enabled the use of this sim-
ulation technique as a training method by approximating spike variables as real numbers in discrete time.
Furthermore, by randomizing the time step offsets in each mini-batch, we successfully reduced the impact of
discretization errors on the loss function.
We designed and simulated a small-scale circuit to evaluate the effectiveness of learning models that
incorporate reversal potentials in real hardware. The simulations compared the performance of mapping
ANN models to hardware (the ANN-to-IMC mapping) with that of PNN models (the PNN-to-IMC mapping).
The results demonstrated that the proposed method reduced the modeling error to less than one-tenth of
that observed with the ANN-to-IMC mapping. These findings suggest that the adoption of PNN models that
incorporate the reversal potentials omit the necessity of introducing complex circuits to avoid the effects of
non-ideal characteristics [45, 30, 30, 52, 29].
We focused on the membrane potential dependence of current as a non-ideal characteristic in this study. In
actual circuits, various non-idealities arise depending on the memory type and architecture [18]. For instance,
in ReRAM, issues such as IR drop and sneak currents pose significant challenges [93]. Beyond deterministic
characteristics, real circuits are also affected by stochastic phenomena such as process variation and noise,
making reliable large-scale analog circuit design difficult [18]. Efforts to mitigate these non-idealities have
been made by many research groups [35]. Joshi et al. [94] minimized the effect of non-idealities by retraining
models while incorporating Gaussian noise. Cao et al. [95] and Rasch et al. [96] optimized parameters
such as mapping scale and quantization to minimize inference loss in hardware. Cramer et al. [80] used a
“chip-in-the-loop” approach, incorporating hardware characteristics into the learning process to better align
the model with hardware operation.
While these methods integrate hardware characteristics, they employ a top-down approach, where the
17

learned model is an ANN or SNN that is mapped onto hardware. By contrast, the present study directly
incorporated non-idealities into the learning process by training on the hardware’s intrinsic dynamics. Despite
these advancements, our method only incorporates a subset of non-ideal characteristics. To address a broader
range of non-idealities, more complex models need to be trained. One possible approach is to incorporate
non-idealities that do not harm learning efficiency into the model while addressing other non-idealities with
conventional methods [35]. Although this study adopted a model-based approach by representing analog
dynamics with ordinary differential equations, model-free approaches offer an alternative [41]. While model-
free methods are versatile, they require large datasets and are challenging to apply to many non-volatile
memory types. A promising direction for practical applications could involve combining the versatility of
model-free approaches with the advantages of model-based methods [97, 98].
Method
IMC-aware neuron model
We modeled the analog dynamics in the IMC circuit as a non-leaky integrate-and-fire model with reversal
potentials, called the IMC-aware neuron model. See Appendix A to confirm the equivalency between them.
We considered a feedforward neural network. The time evolution of the membrane potential of theith neuron
in the lth layer is given by
d
dt v(l)
i (t) = −αv(l)
i (t) +
X
q∈{+,−}
pq(l)
i (t)

Eq
rev − v(l)
i (t)

(6)
where α(≥ 0) is the leak constant. This model possesses two reversal potentials E+
rev and E−
rev (E+
rev > E −
rev)
corresponding to the positive and negative weights, respectively. The synaptic currents evoke depending on
the activity of the ion channels pq(l)
i (t). This activity is given by
p+(l)
i (t) = 1
E+rev
N (l−1)
X
j∈{wij ≥0}
wijθ

t − t(l−1)
j

, p −(l)
i (t) = 1
E−rev
N (l−1)
X
j∈{wij <0}
wijθ

t − t(l−1)
j

, (7)
where θ (·) is the Heaviside function. This synaptic activity corresponds to the assumption that there is no
leak in the synaptic current [55, 83]. Moreover, for simplicity, we adopted the non-leaky neuron model α = 0
[83, 99, 85, 100]. By rearranging variables in Eq. (6), we obtain the following equation
d
dt v(l)
i (t) = −f(l)
i (t)v(l)
i (t) + g(l)
i (t), (8)
where we define the following variables
f(l)
i (t) :=
X
q∈{+,−}
pq(l)
i (t) =
N (l−1)−1X
j=0
β(l)
ij w(l)
ij θ(t − t(l−1)
j ), (9)
g(l)
i (t) :=
X
q∈{+,−}
pq(l)
i Eq
rev =
N (l−1)−1X
j=0
w(l)
ij θ(t − t(l−1)
j ), (10)
β(l)
ij :=
δw(l)
ij ≥0
E+rev
+
δw(l)
ij <0
E−rev
, (11)
where δx is 1 only when x is true, otherwise 0. The analytical solution can be obtained with the method of
variation of parameters:
v(l)
i (t) = e−
R t
0 f (l)
i (s)ds ·
Z t
0
e
R s
0 f (l)
i (s′)ds′
g(l)
i (s)ds

, (12)
18

where we set v(l)
i (0) = 0. By arranging the spike in the time order {ˆt(l−1)
0 ≤ ˆt(l−1)
1 ≤ · · · ≤ ˆt(l−1)
N (l−1)−1}, and
representing the corresponding weights as { ˆw(l)
ij }, we define
ˆt(l)
j = t(l)
ˆj , ˆw(l)
ij = wiˆj, (13)
ˆj = arg min
k
{t(l)
k |ˆt(l)
i−1 ≤ t(l)
k , t (l)
−1 = 0}. (14)
With this time-ordered arrangement, the integral becomes a summation, then we obtain the following repre-
sentation of the membrane potential
v(l)
ij := v(l)
i (ˆt(l−1)
j ) (15)
=



0, for j = 0,
Pj−1
k=0
g(l)
ik
f (l)
ik

F (l)
ik −F (l)
i k −1

F (l)
i j −1
, for j > 0,
(16)
where we define
F (l)
ij :=
(
e
Pj
q=0 f (l)
iq

ˆt(l−1)
q+1 −ˆt(l−1)
q

for j ≥ 0,
1 for j = −1,
(17)
f(l)
ij := f(l)
i (ˆt(l−1)
j ) =
jX
k=0
ˆw(l)
ik β(l)
ij , (18)
g(l)
ij := g(l)
i (ˆt(l−1)
j ) =
jX
k=0
ˆw(l)
ik . (19)
Detailed derivation can be found in Appendix B.
RC-Spike model
The RC-Spike model has two time phases: the accumulation phase and firing phase. The time evolution is
described by the following differential equations [54] :
d
dt v(l)
i (t) =
(
−f(l)
i (t)v(l)
i (t) + g(l)
i (t), for l − 1 ≤ t < l (accumulation phase)
1, for l ≤ t < l + 1 (firing phase). (20)
The accumulation phase is the same as the IMC-aware neuron model (Eq. (8)). Notably, when |E±
rev| → ∞ ,
the final membrane potential v(l)
i (l) at the end of the accumulation phase matches the sum-of-products
operation in an artificial neuron model [45, 29, 46]. Specifically, it corresponds to PN (l−1)−1
j=0 w(l)
ij (l − t(l−1)
j ).
When |E±
rev| < ∞, the final membrane potential does not match the sum-of-products owing to the nonlinearity
[54]. The RC-Spike neuron fires when the membrane potential exceeds the firing threshold Vth = 1 during the
firing phase. The firing time is clipped within the interval of the firing phase. In the following formulation,
we used a shifted version of the accumulation phase and firing phase so that they fall within the interval [0,1]
for computational brevity.
With Eq. (16) and letting ˆt(l−1)
N (l−1) = 1, the membrane potential at the end of the accumulation phase is
given by
v(l)
i (1) := v(l)
i (ˆt(l−1)
N (l−1)) (21)
=
PN (l−1)−1
k=0
g(l)
ik
f (l)
ik

F (l)
ik − F (l)
i k −1

F (l)
i N (l−1)−1
, (22)
=
N (l−1)−1X
k=0
g(l)
ik
f(l)
ik

Z(l)
ik − Z(l)
i k −1

, (23)
19

where we defined
Z(l)
ij :=
F (l)
ij
Fi N (l−1)−1
(24)
=
(QN (l−1)−1
q=j+1 e
−f (l)
iq

ˆt(l−1)
q+1 −ˆt(l−1)
q

for j < N (l−1) − 1.
1 for j = N(l−1) − 1.
(25)
Algorithm 1 RC-Spike
1: Obtain input spikes t(0)
i from data
2: for all layers (l = 1, ..., L) do
3: Obtain ˆt(l−1)
j by sorting t(l−1)
j .
4: Calculate v(l)
i (1) with Eq. (22),
5: Calculate t(l)
i with Eq. (26).
6: end for
The firing time is readily obtained using Eq. (20) with the membrane potential at the end of the accu-
mulation phase:
t(l)
i = clip

1 − v(l)
i (1)

, (26)
clip(x) =



0, if x < 0,
x, if 0 ≤ x ≤ 1,
1, if 1 < x,
(27)
where we shifted the firing time so that the firing time falls within the interval [0, 1]. Algorithm 1 summarizes
the calculation method of the firing time at each layer of the RC-Spike model when input spikest(0)
i are given.
TTFS-SNN model
A neuron in the TTFS-SNN model fires when the membrane potential reaches the firing threshold. Unlike
the RC-Spike model, it is an event-driven system without phase switching. This model enables information
processing with an extremely small number of spikes by imposing the constraint that each neuron fires at
most once [59, 55, 56, 81, 83, 82]. The firing time can be calculated as follows. Let the firing time when it is
assumed as ˆt(l−1)
j−1 < t(l)
i ≤ ˆt(l−1)
j be t(l)
i j −1. In Eq. (16), by substituting ˆt(l−1)
j with t(l)
i j −1, the firing time can
be obtained from the firing condition v(l)
i (t(l)
i j −1) = Vth as follows:
t(l)
i j −1 = t(l−1)
j−1 + 1
f(l)
i j −1
ln


g(l)
i j −1
f (l)
i j −1
− v(l)
i j −1
g(l)
i j −1
f (l)
i j −1
− Vth

 . (28)
Then, the actual firing timing is obtained by
t(l)
i = min
j
{t(l)
i j −1| ˆt(l−1)
j−1 < t(l)
i ≤ ˆt(l−1)
j , 1 ≤ j ≤ N(l−1), ˆt(l−1)
N (l−1) = ∞}. (29)
We note that when no leak neuron ( α = 0) is used, another condition such v(l)
ij ≥ Vth can be used instead
of ˆt(l−1)
j−1 < t (l)
i ≤ ˆt(l−1)
j . If the condition of Eq. (28) is not satisfied, the neuron does not fire, and the
firing timing t(l)
i is assigned a value of ∞. To calculate Eq. (28), it is necessary to compute the membrane
potential (Eq. (16)) at all times when input spikes are received. However, F (l)
ij of Eq. (17) is an exponential
function, which can easily lead to overflow. To prevent this, we can instead compute

F (l)
ik − F (l)
i k −1

/F (l)
i j −1.
20

Table 1: Computational cost of a single layer for various models.
Nin: input dimension, Nout: output dimension, M: discrete time points
ANN RC-Spike TTFS
Conventional O(NinNout) O
 
N2
inNout

O
 
N3
inNout

With DSTD N/A O (M NinNout) O
 
M2NinNout

However, because this is a 3-dimensional tensor, it is extremely memory inefficient. By using the following
recursion
F (l)
ij = e
f (l)
ij

ˆt(l−1)
j+1 −ˆt(l−1)
j

F (l)
i j −1, (30)
the membrane potential update equation can be recursively written as follows:
v(l)
ij =
g(l)
i j −1
f (l)
i j −1

F (l)
i j −1 − F (l)
i j −2

F (l)
i j −1
+
Pj−2
q=0
g(l)
iq
f (l)
iq

F (l)
iq − F (l)
i q −1

e
f (l)
i j −1

t(l)
j −t(l)
j−1

F (l)
i j −2
(31)
=
g(l)
i j −1
f(l)
i j −1

1 − e
−f (l)
i j −1

t(l)
j −t(l)
j−1

+ e
−f (l)
i j −1

t(l)
j −t(l)
j−1

v(l)
i j −1 (32)
=
g(l)
i j −1
f(l)
i j −1
+
 
v(l)
i j −1 −
g(l)
i j −1
f(l)
i j −1
!
e
−f (l)
i j −1

t(l)
j −t(l)
j−1

. (33)
Algorithm 2 TTFS-SNN
1: Obtain input spikes t(0)
i from data
2: for all layers (l = 1, ..., L) do
3: Obtain ˆt(l−1)
j sorting t(l−1)
j .
4: Calculate v(l)
ij with Eq. (33)
5: Calculate t(l)
ij with Eq. (28)
6: Calculate t(l)
i solving Eq. (29)
7: end for
Algorithm 2 summarizes the computing method of the firing time for each layer when input spikes t(0)
i
are given.
Differentiable spike-time discretization
As explained in previous sections, when calculating the firing time, in the case of RC-Spike, the element of
matrix ( ∈ RNout×Nin) must be computed (see Eq. (23).) . Here, Nout is the output dimension and Nin is
the input dimension. Because each element requires O(Nin) computations, the total computational cost is
O(N2
inNout). For TTFS-SNN models, the element of matrix ( ∈ RNout×Nin) must be computed repeatedly
Nin times (see Eq. (28).). Because each element requires O(Nin) computations, the total computational
cost is O(N3
inNout). As shown in table 1, IMC-aware neurons require much more computations compared
with ANN models, which prevents the training of large-scale models. To mitigate this problem, we introduce
differentiable spike-time discretization (DSTD) technique.
First, consider a situation where the arrival times of the spikes received by the neuron are limited to the
following discrete time points [91]:
T (l)
m = m∆τ − t(l)
offset, (m = 0, 1, . . . , M), (34)
0 < t(l)
offset < ∆τ . (35)
21

Here, ∆τ is the discrete-time width, and t(l)
offset represents the offset time. In this case, Eq. (9) and Eq. (10)
can be rewritten as follows:
˜f(l)
im := f(l)
i (T (l−1)
m ) =
N (l)−1X
j=0
β(l)
ij w(l)
ij S(l−1)
jm , (36)
˜g(l)
im := g(l)
i (T (l−1)
m ) =
N (l)−1X
j=0
w(l)
ij S(l−1)
jm , (37)
where we define
S(l−1)
jm :=
mX
q=0
s(l−1)
jq . (38)
s(l)
jq is the jth spike variable at the layer l − 1 at time T (l)
q . The spike variable is a binary variable, where it
takes the value of 1 when a spike exits at a given time, and 0 otherwise. In this condition, the membrane
potential at the end of the accumulation phase can be calculated as
v(l)
i (1) =
M −1X
m=0
˜g(l)
im
˜f(l)
im

˜Z(l)
im − ˜Z(l)
i m −1

, (39)
where we defined
˜Z(l)
im :=
(QM −1
q=m+1 e
˜f (l)
iq

ˆt(l−1)
q+1 −ˆt(l−1)
q

for m < M − 1.
1 for m = M − 1.
(40)
Similarly, the firing time for the case of TTFS-SNN models is given by
t(l)
i = min
m
{˜t(l)
im|˜v(l)
im ≥ Vth, 0 ≤ m ≤ M }, (41)
where we defined
˜v(l)
im := v(l)
i

T (l)
m

(42)
= ˜g(l)
i m −1
ˆf(l)
i m −1
+
 
˜v(l)
i m −1 − ˜g(l)
i m −1
˜f(l)
i m −1
!
e
− ˜f (l)
i m −1

T (l)
i m −T (l)
i m −1

, (43)
˜t(l)
i m −1 := T (l)
m−1 − 1
˜f(l)
i m −1
ln


˜g(l)
i m −1
˜f (l)
i m −1
− ˜v(l)
im
˜g(l)
i j −1
˜f (l)
i j −1
− Vth

 . (44)
In the above computation, we compute the elements of the matrices ˜v(l)
im, ˜f(l)
im, ˜g(l)
im, ˜Z(l)
im ∈ RN (l)×M instead of
those of the matrices v(l)
ij , f(l)
ij , g(l)
ij , Z(l)
ij ∈ RN (l)×N (l−1)
, which implies that computational cost is drastically
reduced if M ≪ N(l−1) (table 1). However, there are two problems with this approach. The first issue is
that the spike timing is a continuous value ( t(l)
i ∈ R); therefore, the spike arrival time fundamentally does
not inherently exist as a discrete value. The second problem is that, because the spike timing is discretized,
it is not possible to calculate gradients with respect to time. DSTD solves these problems by approximating
the spike variable s(l)
jq as a real number in the following way:
{s(l)
j0 , s(l)
j1 , . . . , s(l)
j M −1} = DSTD

t(l)
j

n
T (l)
m
o
m=0,1,...,M

. (45)
The DSTD function is differentiable with respect to time, enabling learning through gradient descent methods.
In this study, we utilized the DSTD function as follows:
s(l−1)
jm = max
 
0,
∆τ − |T (l)
m − t(l−1)
j |
∆τ
!
. (46)
22

In the case of RC-Spike, because T (l)
M = 1, the time width at the final time changes depending on the offset
time toffset. Therefore, it is necessary to compute it as follows:
s(l−1)
jm =



max

0,
∆τ −|T (l)
m −t(l−1)
j |
∆τ

, for t(l)
i ≤ T (l)
M −1,
max

0,
t(l)
offset−|T (l)
m −t(l−1)
j |
t(l)
offset

, for t(l)
i > T (l)
M −1.
(47)
Algorithm 3 RC-Spike with DSTD
1: Obtain input spikes t(0)
i from data
2: for all layers (l = 1, ..., L) do
3: Draw t(l)
offset from (0, ∆τ)
4: Calculate s(l−1)
i with t(l−1)
i using Eq. (47).
5: Calculate v(l)
i (1) using Eq. (39).
6: Calculate t(l)
i using Eq. (26).
7: end for
Algorithm 4 TTFS-SNN with DSTD
1: Obtain input spikes t(0)
i from data
2: for all layers (l = 1, ..., L) do
3: Draw t(l)
offset from (0, ∆τ)
4: Calculate s(l−1)
jm with t(l−1)
j using Eq. (46).
5: Calculate ˜v(l)
ij using Eq. (43)
6: Calculate ˜t(l)
im using Eq. (44).
7: Calculate t(l)
i solving Eq. (41).
8: end for
The calculation method for the RC-Spike model using DSTD is shown in Algorithm 3, and the calculation
method for the TTFS-SNN model using DSTD is shown in Algorithm 4. It should be noted that when using
DSTD, sorting the spikes in time order is not required. Moreover, the matrix size of β(l)
ij (Eq. (11)) is
Nin × Nout. However, as this computational cost remains independent of the batch size, it becomes negligible
when processing large batch sizes. DSTD theoretically guarantees that when the discrete time width ∆ τ is
sufficiently small, the resulting membrane potential accurately approximates the exact ODEs. We proved
the following theorem.
Theorem 1. When a nonleaky neuron receives a number of spikes in the interval of [0 , 1], the error between
the exact membrane potential and the approximated membrane potential satisfies
|v(1) − ˆv(1)| = O(∆2
τ |E±
rev|−1), (48)
where O(·) is the big O notion. The more rigorous form of this theorem and its proof are given in Appendix
B.
23

Learning algorithms
The supervised learning of the RC-Spike model and the TTFS-SNN model was performed using the following
cost function:
C

t(L)
 κ

= Loss

t(L)
 κ

+ γ1Temp

t(L)

+ γ2Q, (49)
Loss

t(L)
 κ

=
N (L)−1X
i=0
κi ln Si

t(L)

, (50)
Si

t(L)

=
exp

t(L)
i
τsoft

PN (L)−1
j=0 exp

t(L)
j
τsoft
 , (51)
Temp

t(L)

=
N (L)−1X
i=0

t(L)
i − tref
2
, (52)
Q =
l=LX
l=1
X
j∈Γ(l)
i
w(l)
ij . (53)
Here, C(·) is the cost function, Loss( ·) is the classification loss function, and Temp(·) is the temporal penalty
term. N(L) represents the number of neurons in the output layer, and t(L) =

t(L)
0 , t(L)
1 , . . . , t(L)
N (L)−1

denotes
the firing times of the neurons in the output layer. κ =
 
κ0, κ1, . . . , κN (L)−1

is the teacher label, where the
element corresponding to the correct label is 1, and all other elements take a value of 0. Si(·) represents
the softmax function, and τsoft is a positive real number that performs scaling of the softmax. γ1 is a
positive constant that controls the significance of the temporal penalty term. The temporal penalty term can
stabilize the learning process [83]. Q(·) is used when training the TTFS-SNN model and stabilizes learning
by promoting firing [85]. Γ(l)
i is a set of indices of spikes that arrive before the ith neuron in the lth layer fires.
The strength of this regularization is adjusted byγ2. Learning was performed by minimizing this cost function
using gradient descent with the Adam optimizer [101] at a learning rate of η. In the numerical simulations of
the RC-Spike models, the following hyperparameters were used: η = 10−4, γ1 = 2.6, tref = 0.9, τsoft = 0.07,
γ2 = 0, and minibatch size of 32. For the case of TTFS models, we used the following hyperparameters:
η = 2 · 10−4, γ1 = 0.02, tref = 3.2, τsoft = 0.07, γ2 = 8 · 10−6, and minibatch size of 32.
Dataset
The Fashion-MNIST data consist of 60,000 training images and 10,000 test images. Each data point is a
28 × 28 image with a single channel, and the task was to predict the correct label from 10 categories, such as
T-shirts and trousers. The brightness of each pixel, xijk (i = 0, 1, ...,27, j = 0, 1, ...,27, k = 0), normalized
to the range [0 , 1], was converted into an input spike as follows, with the spike timing defined as
t(0)
ijk = τin(1 − xijk), (54)
where τin is the input time scale, which is set to 1 for all cases. For the fully connected neural network cases,
the input data were flattened as ˆxi(i = 0, 1, ...,783) and converted into input spikes
t(0)
i = τin(1 − xi). (55)
The CIFAR-10 dataset consists of 50,000 training data points and 10,000 test data points. Each data
point is an image with dimensions of 32 × 32 and three channels. Each data point is labeled with one of 10
categories such as airplane and automobile, and the task was to predict the correct label. The normalized data
xijk (i = 0, 1, ...,31, j = 0, 1, ...,31, k = 0, 1, 2) were converted into input spikes using Eq. (54). Additionally,
similar to previous research [84], data augmentation techniques such as horizontal flip, rotation, and crop
were applied.
24

Table 2: Parameters used in circuit design and simulation
HW parameter explanation value
Vdd supply voltage 1.8 V
Cm membrane capacitance 140 fF
T circ time interval 1 µs
Vswitch switching threshold of sensing inverters 0.428V
V0 resting membrane potential 1.3 V
V circ
th threshold voltage V0 - Vswitch(=0.872V)
V sense
b bias voltage for sensing inverter 0.95 V
V dis
b bias voltage for discharger circuit 0.554 V
λN nonideality coefficient of NMOS current 0.41
λP nonideality coefficient of PMOS current 0.75
λdis nonideality coefficient of discharging current 0.177
I(l)
ij synaptic current CmV circ
th (T circ)−1w(l)
ij
Model parameter explanation value
E+
rev positive reversal potential
 
V circ
th λN−1
(= 2.80)
E−
rev negative reversal potential −
 
V circ
th λP−1
(= −1.53)
Edis
rev positive reversal potential in firing phase
 
V circ
th λdis−1
(= 6.44)
Learning parameter explanation value
η learning rate 10 −3
batch size batch size 50
τsoft softmax scale 0.07
γT temporal penalty 0.1
γW weight’s 2-norm 10 −2
γV early spike penalty 0 .2
Hardware simulation
Algorithm 5 Hardware simulation processure with ANN-to-IMC mapping
1: Train RC-Spike model with |E±
rev| = 100.
2: Convert the trained weights into synaptic currents.
3: Optimize the positive and negative current scales by circuit simulation.
Algorithm 6 Hardware simulation processure with PNN-to-IMC mapping
1: Obtain the nonideality of memories λN(P) by circuit simulation.
2: Train RC-Spike model with E±
rev calculated from λN(P).
3: Convert the trained weights into synaptic currents.
4: Optimize the current scale by circuit simulation.
As shown in Fig. 7 a, when computing the ANN model with the IMC circuit, the ANN model was first
trained, and the learned weights were then mapped to the IMC circuit (the ANN-to-IMC mapping). We
used the RC-Spike model with |E±
rev| = 100 as the ANN model. When computing the RC-Spike model
with the IMC circuit, the reversal potentials E±
rev was first calculated through circuit simulation, and based
on that, the RC-Spike model was trained. Then, the trained weights were mapped onto the IMC circuit
(the PNN-to-IMC mapping). The relationship between the parameters of the IMC circuit and the model
parameters is shown in Table 2. Algorithm 5 and Algorithm 6 summarize the procedures for the cases of the
ANN-to-IMC mapping and the PNN-to-IMC mapping, respectively.
The RC-Spike model and designed IMC circuit comprise a fully connected network with single hidden
layer (5-5-5 network). We adopted the Iris dataset [66, 67] to train the RC-Spike model. The Iris dataset
consists of 150 data points, with each data point represented as a 4-dimensional vector. The task was to
classify each data point into one of three classes. To process these data using the RC-Spike model, the value
25

xi of each dimension of each data point was converted into input spikes as follows:
t(0)
i = τinxi(i = 0, 1, 2, 3). (56)
Additionally, a bias spike was introduced as t(0)
4 = 0. To make the mapping procedure efficient, significantly
large currents must be avoided. In addition, early firing spikes should be prevented, which make circuit
design difficult. We therefore added the following regularization terms to the cost function:
P =
2X
l=1
∥w(l)∥2
2, (57)
V =
4X
i=0

t(1)
i − 1
2
+

t(2)
i − 1
2
, (58)
with the coefficients γP and γV that control the effects of P and V , respectively. The parameters used in the
learning process is shown in table 2.
Acknowledgements
This work was partially supported by JST PRESTO Grant Number JPMJPR22C5, NEC Corporation,
SECOM Science and Technology Foundation, JST Moonshot R&D Grant Number JPMJMS2021, Institute
of AI and Beyond of UTokyo, the International Research Center for Neurointelligence (WPI-IRCN) at The
University of Tokyo Institutes for Advanced Study (UTIAS), JSPS KAKENHI Grant Number JP20H05921,
Cross-ministerial Strategic Innovation Promotion Program (SIP), the 3rd period of SIP , Grant Numbers
JPJ012207 and JPJ012425. Computational resource of AI Bridging Cloud Infrastructure (ABCI) provided
by National Institute of Advanced Industrial Science and Technology (AIST) was used. Yusuke Sakemi ex-
tends gratitude to Prof. Osamu Nomura and the Open Source Silicon Design Community for their invaluable
support in circuit design and simulation.
26

Figure 8: a. Charge-domain IMC circuits with resistors and transistor switches. The network weights are
represented by the conductance values of resistors, denoted as σ(l)±
i . Vdd is the supply voltage and Vgnd is the
ground voltage. b. Charge-domain IMC circuits with current sources and transistor switches. The network
weights are represented by the current value, I(l)P
ij for positive value and I(l)N
ij for negative value. In both
cases, two spikes from the l − 1 layer are recieved at times t(l−1)
1 and t(l−1)
2 . Due to the resulting synaptic
currents, the membrene potential V (l)
i is altered accordingly.
A RC-Spike-Equivalent Hardware models
The RC-Spike model incorporates the dependency of synaptic current on membrane potential, which is a
non-ideal characteristic inherent to charge-domain in-memory computing (IMC) systems. In Fig. 8, circuit
diagrams are presented for charge-domain IMC implementations using resistors and transistor switches, as
well as for those employing current sources. In the following sections, we demonstrate that the behavior of
these circuits aligns with that of the RC-Spike model.
A.1 Resistively coupled synapse model
According to Ohm’s law and Kirchhoff’s current law, the membrane potential of the circuit shown in Fig. 8
a evolves over time as follows:
Cm
d
dt V (l)
i (t) =



PN (l−1)−1
j=0 σ(l)+
ij

Vdd − V (l)
i (t)

θ

t − t(l−1)
j

+ σ(l)−
ij

Vgnd − V (l)
i (t)

θ

t − t(l−1)
j

for (l − 1)T circ ≤ t < lT circ (accumulation phase),
CmV circ
th
T circ , for lT circ ≤ t < (l + 1)T circ (spike generation phase),
(59)
where V (l)
i (t) represents the membrane potential of neuron i in layer l, corresponding to the voltage across
capacitor Cm. The time interval T circ defines the duration of either the accumulation phase or the firing phase,
while V circ
th denotes the firing threshold defined as Vswitch − V0. The terms σ(l)±
ij correspond to the electrical
conductances (inverse of resistance) representing the synaptic strength from neuronj in layer (l−1) to neuron
i in layer l. The positive (+) and negative ( −) symbols indicate current inflow and outflow, respectively.
When σ(l)+
ij > 0, we set σ(l)−
ij = 0, i.e., only current inflow occurs, and vice versa when σ(l)−
ij > 0. In IMC
circuits, a single weight is typically represented using two memory devices, one for the positive value and the
other for the negative value [19].
We show the equivalence of this differential equation to the RC-Spike model. First, we scale the voltage
and time variables as follows:
v(l)
i =

V (l)
i − V0

V circ
th
, E +
rev = Vdd − V0, E −
rev = Vgnd − V0, t ′ = t
T circ . (60)
Here, V0 denotes the resting membrane potential in the hardware (initial voltage value). We also assume
Vdd > V0 and Vgnd < V0. By substituting these into Eq. (59) and reverting the time variable notation from
27

t′ to t, we obtain
CmV circ
th
d
T circdt v(l)
i =



PN (l−1)−1
j=0 σ(l)+
ij

E+
rev − V circ
th v(l)
i

θ

t − t(l−1)
j

+ σ(l)−
ij

E−
rev − V circ
th v(l)
i

θ

t − t(l−1)
j

for (l − 1) ≤ t < l (accumulation phase),
CmV circ
th
T circ , for l ≤ t < (l + 1) (firing phase),
(61)
Simplifying this expression leads to
d
dt v(l)
i =



PN (l−1)−1
j=0
T circE+
revσ(l)+
ij
CmV circ
th

1 − V circ
th
E+
rev
v(l)
i

θ

t − t(l−1)
j

+
T circE−
revσ(l)−
ij
CmV circ
th

1 − V circ
th
E−
rev
v(l)
i

θ

t − t(l−1)
j

for (l − 1) ≤ t < l (accumulation phase),
1, for l ≤ t < (l + 1) (spike generation phase).
(62)
By defining the synaptic weights as a transformation of conductances,
w(l)±
ij :=
T circE±
revσ(l)±
ij
CmV circ
th
, (63)
and introducing the following parameters,
β± := V circ
th
E±rev
, (64)
the time evolution of the membrane potential can be expressed as
d
dt v(l)
i =



PN (l−1)−1
j=0 w(l)+
ij

1 − β+v(l)
i

θ

t − t(l−1)
j

+ w(l)−
ij

1 − β−v(l)
i

θ

t − t(l−1)
j

for (l − 1) ≤ t < l (accumulation phase)
1, for l ≤ t < (l + 1) (firing phase)
(65)
Because either w+
ij or w−
ij is zero, we can define
w(l)
ij =
(
w(l)+
ij , for w(l)+
ij ≥ 0,
w(l)−
ij , for w(l)+
ij < 0, (66)
β(l)
ij :=
(
β+ for w(l)
ij ≥ 0,
β− for w(l)
ij < 0. (67)
Thus, the membrane potential evolution simplifies to
d
dt v(l)
i =



PN (l−1)−1
j=0 w(l)
ij

1 − β(l)
ij v(l)
i

θ

t − t(l−1)
j

for (l − 1) ≤ t < l (accumulation phase)
1, for l ≤ t < (l + 1) (spike generation phase),
(68)
which is consistent with the RC-Spike model.
A.2 Non-ideal current source model
Consider a metal-oxide-semiconductor field-effect transistor (MOSFET) as a current source. While the
MOSFET operates as a current source in the saturation region, it exhibits non-ideal characteristics owing to
channel length modulation, as shown below [34]:
INMOS
D ≈ I
 
1 + λN VDS

, (69)
28

where I is the target current, VDS is the drain-source voltage, and λN is a parameter representing the
dependence of the current on VDS for the calse of an N-type MOSFET. Similarly, for a P-type MOSFET,
the current is given by
IPMOS
D ≈ I
 
1 + λP VDS

. (70)
Thus, the MOSFET-based current source exhibits a linear dependence on the drain voltage, which corresponds
to the membrane potential V (l)
i (t). Given that VDS can be regarded as positive (negative) the membrane
potential for the case of N(P)-type MOSFETs, the time evolution of the membrane potential is described as
follows:
Cm
d
dt V (l)
i (t) =



P
j I(l)P
ij

1 − λP (V (l)
i − V0)

θ

t − t(l−1)
j

− I(l)N
ij

1 + λN(V (l)
i − V0)

θ

t − t(l−1)
j

−Idis

1 − λdis

V (l)
i − V0

,
(71)
where I(l)P
ij represents the current flowing into the capacitor through the P-type MOSFET, and I(l)N
ij repre-
sents the current flowing out of the capacitor through the N-type MOSFET. These currents are triggered when
input spikes arrive, thereby closing the switch. In this case, we also consider the non-ideality of discharging
current Idis represented with λdis. Voltage and time can be rescaled as follows:
v(l)
i = −(V (l)
i − V0)
V circ
th
, t = T circt′. (72)
Note that the membrane potential in the circuit is inverted in polarity. Returning to the original time variable
t, the time evolution of the membrane potential is described as
−CmV circ
th
d
T circdt v(l)
i (t) =



P
j I(l)P
ij

1 + λP V circ
th v(l)
i

θ

t − t(l−1)
j

− I(l)N
ij

1 − λN V circ
th v(l)
i

θ

t − t(l−1)
j

−Idis

1 − λdisV circ
th v(l)
i

.
(73)
After some simplification, this becomes
d
dt v(l)
i =



P
j

T circI (l)N
ij
CmVth
(1 − λN V circ
th v(l)
i ) −
T circI (l)P
ij
CmV circ
th
(1 + λP V circ
th v(l)
i )

θ(t − t(l−1)
j )
T circI dis
CmVth

1 − λdisVthv(l)
i

.
(74)
Using the following variable definitions,
w(l)+
ij :=
T circI(l)N
ij
CmV circ
th
, w (l)−
ij := −
T circI(l)P
ij
CmV circ
th
(75)
β+ = V circ
th λN , β − = −V circ
th λP (76)
α = T circIdis
CmV circ
th
, β charge = λdisV circ
th , (77)
Eq. (74) can be rewritten as
d
dt v(l)
i =



P
j
h
w(l)+
ij (1 − β+v(l)
i ) + w(l)−
ij (1 − β−v(l)
i )
i
θ(t − t(l−1)
j )
α

1 − βdisv(l)
i
 (78)
Note that either w(l)+
ij or w(l)−
ij is zero. Thus, by defining
w(l)
ij =
(
w(l)+
ij , for w(l)+
ij ≥ 0,
w(l)−
ij , for w(l)+
ij < 0, (79)
β(l)
ij :=
(
β+ for w(l)
ij ≥ 0,
β− for w(l)
ij < 0, (80)
29

Figure 9: Schematic of random time points ti and regular time points τi. The time interval between two
regular time points is constant and is denoted as ∆ τ. The black solid line represents the values of fi or bi,
whereas the red solid line represents the approximate values of the parameters ˆfi or ˆgi.
the equation is further simplified as:
d
dt v(l)
i =



P
j w(l)
ij

1 − β(l)
ij v(l)
i

θ

t − t(l−1)
j

for (l − 1) ≤ t < l (accumulation phase)
α

1 − βdisv(l)
i

, for l ≤ t < (l + 1) (firing phase).
(81)
This is consistent with the RC-Spike model.
In the model-to-hardware mapping simulations, the RC decay effect was considered even in the firing
phase (βdis ̸= 0). If the firing phase spans the interval [0,1] for computational simplicity, its solution is
v(t) = 1 − (1 − βv(0)) e−αβt
β . (82)
Given the condition v(1) = 1 if v(0) = 0 1, we obtain α = − ln(1−β)
β , leading to
v(t) = 1 − (1 − βv(0)) (1 − β)t
β . (83)
Finally, by solving v(t) = 1 for t, the spike time is
t =
ln

1−β
1−βv(0)

ln (1 − β) (84)
= ln (1 − β) − ln (1 − βv(0))
ln (1 − β) . (85)
B Proofs
The differentiable spike-time discretization (DSTD) offers a fast learning process and strong theoretical
foundation. We demonstrate below that the time evolution obtained with DSTD converges to the rigorous
solution of ordinary differential equations (ODEs) as ∆ τ → 0, where ∆ τ is the step size of DSTD.
B.1 Approximation Error Oder regarding the step size of DSTD
Definition 1 (Interval Linear ODE). For random time points {t0 = 0, t1, t2, . . . , tn, tn+1 = 1}, the interval
linear ODE is defined as
˙x = −fix + gi, ∀t ∈ [ti, ti+1), x (0) = 0
xi = x(ti), i ∈ {0, . . . , n}. (86)
1Here, t = 0 and t = 1 represent the beginning and end time of the firing phase, respectively. The state v(0) = 0, which can
be obtained without input spikes, should produce a spike time of 1, which does not influence the subsequent layers.
30

Definition 2 (Regular time points) . Suppose that regular time points with τi ≜ i
N = i∆τ are defined as
{τ0, τ1, . . . , τN }. The relation of random time points {t0, t1, . . . , tn+1} is denoted as
i ≜ arg max
j
{τj | τj ≤ ti+1}, N i ≜ i − i − 1, s i ≜ ti − τi−1
Lemma 1. The following equations satisfy:
s0 = sn+1 = 0, t i+1 − ti = ∆τ Ni + si+1 − si. (87)
Proof: The edges of regular time points are given by
τ−1 = max
j
{τj | τj ≤ t0} = τ0 = 0,
τn = max
j
{τj | τj ≤ tn+1} = τN = 1.
Therefore,
s0 = t0 − τ−1 = 0 − 0 = 0
sn+1 = tn+1 − τn = 1 − 1 = 0.
Furthermore,
ti+1 − ti = (si+1 + τi) − (si + τi−1)
= ∆τ Ni + si+1 − si
Definition 3 (Approximated ODE). For regular time points {τ0, τ1, . . . , τN }, the approximated ODE is
defined as
˙ˆx = − ˆfj ˆx + ˆgj, ∀t ∈ [τj, τj+1), ˆx(τj) = ˆxj,
ˆxj+1 = ˆx(τj+1), (88)
where parameters ˆfi and ˆgi are approximated parameters of fi and gi, respectively, defined as
ˆfj ≜
(
fi j = i − 1 + 1, i + 2, . . .i − 1
ri+1fi + (1 − ri+1)fi+1 j = i
ˆgj ≜
(
gi j = i − 1 + 1, i + 2, . . .i − 1
ri+1gi + (1 − ri+1)gi+1 j = i
ri ≜ si
∆τ
.
(89)
Lemma 2 (Solution of Interval Linear ODE). The solution of interval linear ODE (86) and (88) is given by
xn+1 =
nX
i=0
Zi+1 − Zi
fi
gi, ˆxN =
N −1X
i=0
ˆZi+1 − ˆZi
ˆfi
ˆgi,
respectively, where Zi and ˆZi are denoted as
Zi ≜
nY
j=i
e−fj(tj+1−tj), ˆZi ≜
N −1Y
j=i
e− ˆfj∆τ .
Note that the product on the empty domain is defined as 1 (e.g., Qn−1
n xi = 1).
31

Proof: Since these ODEs are of the same form, this study proves only the random interval time points case
(86). The solution of linear ODEs for each interval [ ti, ti+1) is given by
x(t) = e−fi(t−ti)xi + (1 − e−fi(t−ti)) gi
fi
, ∀t ∈ [ti, ti+1).
The solution at each time points {t0, t1, . . . , tn+1} is given by
x(ti+1) = xi+1 = e−fi(ti+1−ti)xi + (1 − e−fi(ti+1−ti)) gi
fi
= Aixi + Cigi, x 0 = x1 = 0,
where Ai ≜ e−fi(ti+1−ti) and Ci ≜ 1−e−fi (ti+1 −ti )
fi
. Hence, the endpoint is given by
x(1) = xn+1 = Anxn + Cngn
= An(An−1xn−1 + Cn−1gn−1) + Cngn
=
nX
i=0
nY
j=i+1
AjCigi
=
nX
i=0
Zi+1 − Zi
fi
gi.
Using the same approach, the solution of the approximate ODE can be calculated.
Lemma 3. The formula between Zi and ˆZi yields the following:
ˆZi = Zi+1e−ri+1fi∆τ , ˆZi+1 = Zi+1e(1−ri+1)fi+1∆τ . (90)
Proof: Firstly, this study shows the sum of ˆfi∆τ. Under [ τi−1+1τi], the sum is calculated as
iX
j=i−1+1
ˆfk∆τ =
i−1X
j=i−1+1
fi∆τ + ˆfi∆τ
= (Ni − 1)fi∆τ + ri+1fi∆τ + (1 − ri+1)fi+1∆τ
= fi
 
(ti+1 − ti) − (si+1 − si)

− fi(∆τ − si+1) + fi+1(∆τ − si+1)
= fi(ti+1 − ti) − fi(∆τ − si) + fi+1(∆τ − si+1)
Hence, the sum of ˆfi∆τ between i + 1 to N is given by
N −1X
j=c+1
ˆfj∆τ =
nX
i=c+1
iX
j=i−1+1
ˆfj∆τ − ˆfN∆τ
=
nX
i=c+1
(fi(ti+1 − ti) − fi(∆τ − si) + fi+1(∆τ − si+1)) − fn+1∆τ
=
nX
i=c+1
fi(ti+1 − ti) + fn+1(∆τ − sn+1) − fc+1(∆τ − sc+1) − fn+1∆τ
=
nX
i=c+1
fi(ti+1 − ti) − fc+1(∆τ − sc+1)
Also, the sum of c to N − 1 is calculated as
N −1X
j=c
ˆfj∆τ =
N −1X
j=c+1
ˆfj∆τ + ˆfc∆τ
=
nX
i=c+1
fi(ti+1 − ti) − fc+1(∆τ − sc+1) + fcsc+1 + fc+1(∆τ − sc+1)
=
nX
i=c+1
fi(ti+1 − ti) + fcsc+1
32

Therefore, ˆZi and ˆZi+1 is given by
ˆZi = exp −
N −1X
j=c
ˆfj∆τ
= exp −
nX
i=c+1
fi(ti+1 − ti) − fcsc+1
= Zc+1e−fcrc+1∆τ .
ˆZi = Zc+1efc+1(1−rc+1)∆τ .
Note that this lemma presents the map approximated ODE solution coefficient to the original ODE
solution coefficient. The following theorem estimates the order of error of the approximate ODE.
Theorem 2. The error between an interval linear ODE (86) and the approximated ODE (88) satisfies
|x(1) − ˆx(1)| = O(∆2
τ), (91)
where O(·) is the big O notion.
Proof: The endpoint of the approximated ODE is written as
ˆx(1) =
N −1X
j=0
ˆZj+1 − ˆZj
ˆfj
ˆgj
=
nX
i=0
 ˆZi−1+1 − ˆZi−1
ˆfi−1
ˆgi +
i−1X
j=i−1+1
ˆZj+1 − ˆZj
ˆfj
ˆgj

=
nX
i=0
 ˆZi−1+1 − ˆZi−1
ˆfi−1
(rigi−1 + (1 − ri)gi) +
i−1X
j=i−1+1
ˆZj+1 − ˆZj
fi
gi

=
nX
i=0
 ˆZi − ˆZi−1+1
fi
+ ri+1
ˆZi+1 − ˆZi
ˆfi
+ (1 − ri)
ˆZi−1+1 − ˆZi−1
ˆfi−1

gi.
Using Zi and ˆZi relation formulations (90), the approximated solution ˆx(1) is given by
ˆx(1) =
nX
i=0
" 
e−ri+1fi∆τ
fi
+ ri+1
e(1−ri+1)fi+1∆τ − e−ri+1fi∆τ
ˆfi
!
Zi+1
−
 
e(1−ri)fi∆τ
fi
+ (1 − ri) e−rifi−1∆τ − e(1−ri)fi∆τ
ˆfi−1
!
Zi
#
gi.
Hence, the error of the approximated solution is written as
x(1) − ˆx(1) =
nX
i=0
Zi+1 − Zi
fi
gi −
N −1X
j=0
ˆZj+1 − ˆZj
ˆfj
ˆgj
=
nX
i=0

ui(∆τ)Zi+1gi − vi(∆τ)Zigi

,
where ui(·) and vi(·) is defined as
ui(∆τ) ≜ 1 − e−ri+1fi∆τ
fi
− ri+1
e(1−ri+1)fi+1∆τ − e−ri+1fi∆τ
ˆfi
vi(∆τ) ≜ 1 − e(1−ri)fi∆τ
fi
− (1 − ri) e−rifi−1∆τ − e(1−ri)fi∆τ
ˆfi−1
.
33

Here, the order of ui(·) and vi(·) are O(∆2
τ), respectively, because the limit of each function satisfies 2
lim
∆τ →0
ui(∆τ) = 1 − e0
fi
− ri+1
e0 − e0
ˆfi
= 0,
lim
∆τ →0
vi(∆τ) = 1 − e0
fi
− (1 − ri) e0 − e0
ˆfi−1
= 0,
lim
∆τ →0
∂ui
∂∆τ
(∆τ) = firi+1
fi
− ri+1
ˆfi
ˆfi
= 0,
lim
∆τ →0
∂vi
∂∆τ
(∆τ) = (1 − ri)fi
fi
− (1 − ri)
ˆfi−1
ˆfi−1
= 0.
lim
∆τ →0
∂2ui
∂∆2τ
(∆τ) = −r2
i+1fi − ri+1
(1 − ri+1)2f2
i+1 − r2
i+1f2
i
ˆfi
= −r2
i+1fi − ri+1 [(1 − ri+1)fi+1 − ri+1fi] < ∞
lim
∆τ →0
∂2vi
∂∆2τ
(∆τ) = −(1 − ri)2fi + (1 − ri) −r2
i f2
i−1 + (1 − ri)2f2
i
ˆfi−1
= −(1 − ri)2fi − (1 − ri) [rifi−1 − (1 − ri)fi] < ∞
According to L’Hˆ opital’s rule, the following result is established
lim
∆τ →0
ui(∆τ)
∆2τ
= 1
2 lim
∆τ →0
∂2ui
∂∆2τ
(∆τ) < ∞
lim
∆τ →0
vi(∆τ)
∆2τ
= 1
2 lim
∆τ →0
∂2vi
∂∆2τ
(∆τ) < ∞.
The absolute error is written as
|x(1) − ˆx(1)|
∆2τ
=
Pn
i=0

ui(∆τ)Zi+1gi − vi(∆τ)Zigi

∆2τ
≤
nX
i=0

ui(∆τ)
∆2τ
 |Zi+1||gi| +

vi(∆τ)
∆2τ
 |Zi||gi|
≤ (n + 1) max
i
|gi|

max
i

ui(∆τ)
∆2τ
 + max
i

vi(∆τ)
∆2τ


< ∞
Therefore, the absolute error is O(∆2
τ).
Remark 1. In the case where x is multidimensional, the error order is proportional to the dimension of the
variable, but the same property holds for ∆ τ.
Remark 2. This result assumes that the number of random time points {t1, . . . , tn} in each regular time
interval [τi, τi+1) is one or less. This assumption is satisfactorily satisfied as N increases. Furthermore,
similar results can be obtained by generalizing the constitutive rules of the approximation parameters ˆfi and
ˆgi.
B.2 Approximation Error Oder regarding reversal potentials
We study the case of α = 0 and E+
rev = −E−
rev. In this case, the RC-Spike model is expressed as
˙x = −βfix + gi, ∀t ∈ [ti, ti+1), x (0) = 0
xi = x(ti), i ∈ {0, . . . , n}. (92)
2Note that {ri ∈ [0, 1]}i∈N are random variables that may be sampled differently for each value of ∆ τ . In the following
argument, we treat these variables as independent constants and evaluate the worst case. By doing this, we can estimate the
upper bound of the error.
34

Here, β = 1
|E±
rev|. Given that the analysis in SubsectionB.1 remains valid when substituting fi with βfi, we
redefine the variables accordingly:
ui(β) ≜ 1 − e−βri+1fi∆τ
βfi
− ri+1
eβ(1−ri+1)fi+1∆τ − e−βri+1fi∆τ
β ˆfi
vi(β) ≜ 1 − eβ(1−ri)fi∆τ
βfi
− (1 − ri) e−βrifi−1∆τ − eβ(1−ri)fi∆τ
β ˆfi−1
,
Zi(β) ≜
nY
j=i
e−βfj(tj+1−tj).
First, the following limit can be derived
lim
β→0
ui (β) = ri+1∆t − ri+1
(1 − ri+1)fi+1∆τ + ri+1fi∆τ
ri+1fi + (1 − ri+1)fi+1
= 0, (93)
lim
β→0
vi (β) = (1 − ri)∆τ − (1 − ri) rifi−1∆τ + (1 − ri)fi∆τ
rifi−1 + (1 − ri)fi
= 0, (94)
lim
β→0
Zi(β) = 1. (95)
Next, we calculate the limits of the derivatives of ui and vi using L’Hˆ opital’s rule:
lim
β→0
∂ui
∂β (β) = lim
β→0
h βri+1fi∆τ e−βri+1fi∆τ −
 
1 − e−βri+1fi∆τ

fi
− ri+1
β∆τ

(1 − ri+1)fi+1eβ(1−ri+1)fi+1∆τ + ri+1fie−βri+1fi∆τ

−

eβ(1−ri+1)fi+1∆τ − e−βri+1fi∆τ

ˆfi
i
β−2
(96)
= lim
β→0
"
ri+1fi∆τ e−βri+1fi∆τ − βr2
i+1f2
i ∆2
τ e−βri+1fi∆τ − ri+1fi∆τ e−βri+1fi∆τ
fi
− ri+1
ˆfi
(
∆τ
h
(1 − ri+1)fi+1eβ(1−ri+1)fi+1∆τ + ri+1fie−βri+1fi∆τ
i
+ β∆2
τ
h
(1 − ri+1)2f2
i+1eβ(1−ri+1)fi+1∆τ − r2
i+1f2
i e−βri+1fi∆τ
i
− ∆τ
h
(1 − ri+1)fi+1eβ(1−ri+1)fi+1∆τ + ri+1fie−βri+1fi∆τ ∆τ
i)#
(2β)−1 (97)
= lim
β→0
−βr2
i+1fi∆2
τ e−βri+1fi∆τ − ri+1β∆2
τ
ˆfi

(1 − ri+1)2f2
i+1eβ(1−ri+1)fi+1∆τ − r2
i+1f2
i e−βri+1fi∆τ
	
2β (98)
= r2
i+1fi∆2
τ
2 − ri+1∆2
τ
2 [(1 − ri+1)fi+1 − ri+1fi] < ∞. (99)
35

Similarly, we obtain
lim
β→0
∂vi
∂β = lim
β→0
"
−β(1 − ri)fi∆τ eβ(1−ri)fi∆τ − (1 − eβ(1−ri)fi∆τ )
fi
− 1 − ri
ˆfi

β∆τ
h
−rifi−1e−βrifi−1∆τ − (1 − ri)fieβ(1−ri)fi∆τ
i
−
h
e−βrifi−1∆τ − eβ(1−ri)fi∆τ
i#
β−2
= lim
β→0
"
−(1 − ri)fi∆τ eβ(1−ri)fi∆τ − β(1 − ri)2f2
i ∆2
τ eβ(1−ri)fi∆τ + (1 − ri)fi∆τ eβ(1−ri)fi∆τ )
fi
(100)
− 1 − ri
ˆfi

∆τ
h
−rifi−1e−βrifi−1∆τ − (1 − ri)fieβ(1−ri)fi∆τ
i
(101)
+ β∆τ
h
r2
i f2
i−1e−βrifi−1∆τ − (1 − ri)2f2
i eβ(1−ri)fi∆τ
i
+
h
rifi−1∆τ e−βrifi−1∆τ + (1 − ri)fi∆τ eβ(1−ri)fi∆τ
i#
(2β)−1
= lim
β→0
−β(1 − ri)2fi∆2
τ eβ(1−ri)∆τ − (1−ri)β∆2
τ
ˆfi

r2
i f2
i−1e−βrifi−1∆τ − (1 − ri)2f2
i eβ(1−ri)fi∆τ

2β
(102)
= −(1 − ri)2fi∆2
τ
2 − (1 − ri)∆2
τ
2 (rifi−1 − (1 − ri)fi) < ∞. (103)
The absolute error is estimated as
lim
β→0
|x(1) − ˆx(1)|
β =

nX
i=0
 ui(β)
β Zi+1(β)gi − vi(β)
β Zi(β)gi

≤
nX
i=0

ui(β)
β
 |Zi+1(β)| |gi| +

gi(β)
β
 |Zi(β)| |gi|
≤ (n + 1) max
i
|gi|

max
i

ui(β)
β
 + max
i

gi(β)
β


< ∞.
Therefore, the absolute error is O(β). Note that β = |E±
rev|−1.
C Simulation results for TTFS-SNNs
In time-to-first-spike-coded spiking neural networks (TTFS-SNNs), neurons in each layer fire upon reaching
the firing threshold Vth in response to incoming spike signals, after which the membrane potential is fixed
to zero. Consequently, each neuron is limited to firing at most once. TTFS-SNNs enable highly efficient
information processing with minimal spike activity, and supports ultra-low-latency inference by allowing
classification tasks to be completed once the first spike is generated in the output layer [55, 102, 82, 83].
However, neuron models incorporating reversal potentials remain unexplored enough in the context of TTFS-
SNNs [83]. As demonstrated in RC-Spike models, the use of differentiable spike-time discretization (DSTD)
facilitates efficient computation in TTFS-SNNs. We applied DSTD to train a TTFS-SNN on the Fashion-
MNIST dataset [60]. The network architecture employed was Conv(16)-Conv(16)-PL-Conv(32)-Conv(32)-
PL-128-10, where Conv( n) denotes a convolutional layer with n channels, a 3 × 3 kernel, and a stride of 1,
while PL represents a pooling layer with a 2 × 2 kernel. Training was conducted with a mini-batch size of 32
over 150 epochs.
In TTFS-SNNs, the synaptic weights must be initialized properly, as learning cannot proceed without
neuronal firing. For instance, Comsa et al. [102] employed a shifted Glorot initial weight distribution for
this purpose. In the following, we derive an initial weight distribution that minimally affects the firing time
36

distribution of the network when the reversal potential E±
rev is varied. We assumed that N input spikes are
uniformly sampled from the time interval [0 , 1]. Additionally, the synaptic weights are assumed to follow a
uniform distribution over the interval [0 , wmax/N]. Under these assumptions, the temporal evolution of the
non-leaky membrane potential is described as
d
dt v(t) = (1 − βv(t)) W (t), (104)
W (t) :=
X
j∈Γ(t)
wj
N . (105)
where we adopted a neuron model that omits leak currents and synaptic decay [83], and Γ( t) denotes the set
of spikes received up to time t. In addition, we assume β = |E±
rev|−1. In the limit as N → ∞, W (t) converges
to wmaxt
2 owing to the law of large numbers 3. By replacing W (t) with wmaxt
2 , solving the differential equation
yields the following expression for the membrane potential:
v(t) = 1 − exp(− wmaxβt2
4 )
β . (106)
To achieve an average delay time per layer of tref/L, and with the firing threshold set to 1, the following
condition for the maximum synaptic weight is derived:
wmax = −4w∗ L2
β(tref)2 ln(1 − β), (107)
where w∗ is a positive constant that adjusts the actual distribution. We set w∗ = 2. Note that in TTFS-
SNNs, the shape of the firing time distribution undergoes significant changes in subsequent layers, which
presents challenges. However, applying a factor of ln(1−β)
β provided a reasonable correction.
Figure 10 panels a-c display the temporal dynamics of membrane potentials after training. The results
correspond to three different reversal potentials: |E±
rev| = 2 ( a), |E±
rev| = 2 .8 (b), and |E±
rev| = 31 .6 (c).
In each figure, the upper and lower panels represent the membrane potential in the final fully connected
layer and output layer, respectively. For all experiments shown in Figure 10, we set E−
rev = −E+
rev. The
results indicate that a smaller |E±
rev| prevents the membrane potential in the final fully connected layer from
becoming excessively negative, and a similar trend was observed in the output layer. This behavior is likely
a result of E+
rev and E−
rev acting as the upper and lower bounds of the membrane potential, respectively.
Figure 10 d shows the effect of varying DSTD steps M on recognition accuracy. The upper and lower
panels present the results for |E+
rev| = 4 and |E±
rev| = 2, respectively. Each panel compares the performance
between randomized DSTD offset (random offset) and fixed offset conditions. The results are expressed as
the mean and standard deviation of five independent trials with random weight initialization. For |E+
rev| = 4,
convergence was achieved with approximately 20 DSTD steps, whereas for E+
rev = 2, convergence required
approximately 40 steps. This observation is explained by the stronger nonlinearity associated with smaller
|E±
rev|, which degrades approximation of DSTD. Additionally, for smaller DSTD steps (M < 10), the random
offset condition consistently yielded better recognition performance than the fixed offset. Therefore, random
offset may improve the gradient characteristics during DSTD calculations. Further investigation into the
nature of these gradients will be addressed in future work.
Figure 10 e illustrates the variation in recognition accuracy as a function of |E±
rev|. The upper and lower
panels display results for DSTD steps M = 30 and M = 10, respectively. In both panels, the results for
random and fixed offsets are depicted. For M = 30, accuracy converged at approximately |E±
rev| = 3, whereas
for M = 10, convergence occurred at |E±
rev| = 4. Furthermore, in both cases, the use of random offsets led
to faster convergence (i.e., at smaller M values) compared with fixed offsets. When the reversal potential
became large, the model behaved similarly to a neuron model that does not account for reversal potentials.
Accordingly, a model trained with |E±
rev| = 32 was employed in the test phase with varying E±
rev values
(ANN)4. This procedure is analogous to mapping a model trained without considering hardware non-ideal
characteristics onto real hardware. In this case, the performance significantly deteriorated when |E±
rev| fell
3The expected number of spikes up to time t is N t. Therefore, the expected total weight up to time t is E
hP
j∈Γ(t) wj
i
=
N t× E [wj] = N t× wmax
2 because input spike timing and weights are independent random variables. Thus, we obtain E [W (t)] =
wmaxt
2 . As N → ∞, the actual sum W (t) converges to its expected value owing to the law of large numbers.
4This is not an ANN model, but we call this model ANN for convenience.
37

d e
a b c
Figure 10: Learning results for convolutional TTFS-SNN models with reversal potentials on
Fashion-MNIST dataset. a-c. The time evolution of membrane potentials in the trained network. The
top panels show results from the final hidden layer, while the bottom panels display the output layer. The
values of |E±
rev| are set to 2.0 (a), 2.8 (b), and 31.6 (c). d. Recognition accuracy as a function of the number
of discretized time steps M in differentiable spike-time discretization (DSTD). The reversal potentials are
set to E+
rev = −E−
rev. The top panel corresponds to E+
rev = 4, and the bottom panel to E+
rev = 2. Each case
compares the results of randomizing the offset during training (random offset) and those without randomizing
the offset (fixed offset). e. Recognition accuracy for varying E+
rev(= −E−
rev) between 1.2 and 31.6. The top
panel shows results for DSTD steps of 30, while the bottom panel shows results for DSTD steps of 10.
Both random and fixed offset results are included in each case. Additionally, for the model trained with
E+
rev = 31.6 under random offset, the test results for varying E+
rev are also shown (ANN). Furthermore, the
results of model optimization via rescaling of both positive and negative weights by 1% are presented for
each E+
rev (ANN (scaled)).
38

a b
Figure 11: Dependence of the recognition performance of the RC-Spike model on |E±
rev| trained with Fashion-
MNIST dataset under various noise intensities σspike. a. Performance of the fully connected RC-Spike model
(784-400-400-10). b. Performance of the convolutional RC-Spike models. In both a. and b., each panel
represents results obtained under different noise standard deviations σspike. For all cases, we used DSTD
steps of 10 with random offset.
below approximately 10. To reduce the impact of these non-ideal characteristics, the positive and negative
weight scales were optimized for each E±
rev (ANN (scaled)) via grid search with magnification increments
of 0.01. After optimizing these scales, recognition accuracy improved markedly. However, the performance
remained substantially inferior compared with that of models trained explicitly considering reversal potentials.
D Effects of spike timing noise on RC-Spike models
The RC-Spike model exhibited a tendency to learn small membrane potentials when the reversal potential
|E±
rev| was small, as a means to mitigate its influence. However, in practical hardware implementations, the
precision of membrane potentials is inherently limited. As a result, using small membrane potentials may
lead to significant performance degradation. To address this limitation, noise was introduced into the spike
timing during performance evaluation.
Figure 11 illustrates the recognition performance of the RC-Spike model as a function of |E±
rev| across
different spike timing noise. The noise is modeled as Gaussian, with a mean of 0 and a standard deviation of
σspike. We trained the models using DSTD steps of 10 with random offset. Figure 11 a presents the results
obtained from a fully connected network (784-400-400-10) trained on the Fashion-MNIST dataset. Across all
noise levels, performance degraded when |E±
rev| was extremely small (∼ 1), which corresponds to pronounced
non-ideal characteristics in hardware. As |E±
rev| increased, the model performance improved in an almost
monotonic fashion. Notably, in the absence of noise ( σspike = 0.00), performance peaked at |E±
rev| ∼ 10.
Figure 11 b shows the results for a convolutional network trained on the Fashion-MNIST dataset. The
network architecture consists of Conv(64)-Conv(64)-PL-Conv(128)-Conv(128)-PL-Conv(256)-Conv(256)-PL-
512-10, where Conv(N) represents a convolutional layer with a 3 × 3 kernel, N output channels, and a stride
of 1, while PL denotes a pooling layer with a 2 × 2 kernel and a stride of 2. When the noise intensity
39

a b
Figure 12: Dependence of the recognition performance of the RC-Spike model on |E±
rev| trained with CIFAR-
10 dataset under various noise intensities. We used DSTD steps of 10 with random offset.a. The performance
results for the VGG7 architecture. b. The performance results for large-width VGG7 architecture.
was high ( σspike = 0.04), the performance, similar to the fully connected network results, improved almost
monotonically as |E±
rev| increased. However, under moderate noise conditions, performance was maximized
at a specific value of |E±
rev|. Moreover, with lower noise intensities, the maximum performance was achieved
at smaller values of |E±
rev|.
As shown in Fig. 12 a, we present an additional simulation using the same convolutional network trained
on the CIFAR-10 dataset. Consistent with the findings in Fig. 11, lower noise intensities tended to yield peak
performance at smaller values of |E±
rev|. Furthermore, comparable results are observed with a convolutional
network of doubled channel width, as illustrated in Fig. 12 b.
This analysis highlights the interplay between reversal potential |E±
rev| and spike timing noise σspike,
which could play a critical role in designing systems that integrate both models and hardware. In Table 3,
we summarize the performance results. We show the best cases over the range of reversal potential values
|E±
rev|.
E Effects of DSTD steps in test phase
Neuron models incorporating reversal potentials impose high computational costs, making it challenging to
evaluate such models with ODE solvers or exact solutions, even during the test phase. Consequently, we
employed DSTD even for the test phase as well. To mitigate the impact of approximation errors introduced
by DSTD, we set the number of DSTD steps M to a sufficiently large value. Fig. 13 illustrates the results of
re-evaluating the trained model under different DSTD steps M during the test phase. We selected M = 30
for the RC-Spike model and M = 60 for the TTFS-SNN model, both of which were found to be adequate for
the convergence of recognition accuracy.
40

a
b
c
Figure 13: Effect of DSTD steps M during the test phase. Each figure presents the variation in
recognition accuracy as a function of the number of DSTD steps M during the test phase using trained
models. Specifically, a fully connected RC-Spike model (784-400-400-10) trained on the Fashion-MNIST
dataset (a), convolutional RC-Spike model trained on the CIFAR-10 dataset ( b), and convolutional TTFS-
SNN model (c) were evaluated. For the RC-Spike models, the number of steps during training was set to 15,
while for the TTFS-SNN model, it was set to 30. In all cases, random offsets were applied during training.
The error bars indicate standard deviations over 10 evaluations.
41

Table 3: Performance results for RC-Spike models. CNN architecture refers to Conv(64)-Conv(64)-PL-
Conv(128)-Conv(128)-PL-Conv(256)-Conv(256)-PL-512-10, where Conv(N) represents a convolutional layer
with a 3 ×3 kernel, N output channels, and a stride of 1, while PL denotes a pooling layer with a 2 ×2 kernel
and a stride of 2.
Dataset Architecture σspike |E±
rev| Accuracy
Fashion-MNIST -400-400-10 0.00 7.4 90.46 %
Fashion-MNIST -400-400-10 0.005 30.7 90.52 %
Fashion-MNIST -400-400-10 0.01 30.7 90.40 %
Fashion-MNIST -400-400-10 0.02 8.4 90.49 %
Fashion-MNIST -400-400-10 0.04 8.4 90.12 %
Fashion-MNIST CNN 0.00 1.3 94.11 %
Fashion-MNIST CNN 0.005 1.6 94.06 %
Fashion-MNIST CNN 0.01 3.3 94.09 %
Fashion-MNIST CNN 0.02 8.4 93.93 %
Fashion-MNIST CNN 0.04 17.0 93.63 %
CIFAR-10 CNN 0.00 2.3 89.13 %
CIFAR-10 CNN 0.005 5.9 88.89 %
CIFAR-10 CNN 0.01 3.3 88.51 %
CIFAR-10 CNN 0.02 4.6 88.00 &
CIFAR-10 CNN 0.04 17.0 86.76 %
CIFAR-10 CNN (width doubled) 0.00 1.1 90.27 %
CIFAR-10 CNN (width doubled) 0.005 1.8 90.16 %
CIFAR-10 CNN (width doubled) 0.01 2.0 90.25 %
CIFAR-10 CNN (width doubled) 0.02 3.7 89.65 &
CIFAR-10 CNN (width doubled) 0.04 15.1 88.42 %
F Weight scaling optimization in test phase
The positive reversal potential ( E+
rev) reduces the current as the membrane potential increases, while the
negative reversal potential ( E−
rev) increases the current under the same conditions. This behavior suggests
that, under a rough approximation, the reversal potentials modulate the total effective current. Consequently,
the impact of the reversal potentials may be partially mitigated without training a PNN model through weight
scaling adjustments.
Figure 14 presents the recognition performance obtained for specific values of reversal potentials during
the test phase using the following ANN models. The ANN models were trained with |E±
rev| = 100 for the case
of RC-Spike models and |E±
rev| = 31.6 for the case of TTFS-SNN models. The positive and negative weights
of the network were uniformly scaled in increments of 0.1. In each plot, the scaling factor that achieved the
highest recognition performance is highlighted with blue star-shaped symbols. These results demonstrate
that, across all scenarios, weight scaling consistently enhances recognition performance.
To map model weights onto hardware, it is necessary to scale the current values on the hardware to
mitigate the effects of parasitic capacitance inherent in IMC circuits. In addition , for the case of ANN-to-
IMC mapping, the effects of E±
rev must be mitigated. Therefore, tor ANN-to-IMC mapping, both positive
and negative currents were optimally scaled. By contrast, for SNN-to-IMC mapping, the absolute value of
the current is scaled uniformly without distinguishing between positive and negative currents. The results of
current scaling for ANN-to-IMC mapping are presented in Fig. 15 a, while those for SNN-to-IMC mapping
are shown in Fig. 15 b.
G Circuit details
The RC-Spike circuit introduced in this study was designed using the SkyWater technology node (sky130)
processing design kit (PDK) [44]. The sky130 process represents the 8th generation SONOS technology
node, featuring a minimum transistor gate length of 150 nm and a five-layer metal interconnect system. We
employed entirely open-source tools, with schematic design performed using xschem [62], layout design and
parasitic extraction using magic [63], layout-versus-schematic check using netgen [65], and circuit simulations
42

a b
c
Figure 14: We present the results of optimizing the scaling of both positive and negative weights during the
test phase for various values of |E±
rev|. For the ANN models tested, |E±
rev| was set to 100 for the RC-Spike
models and 31.6 for the TTFS-SNN models. The best scaling point is plotted with a blue star-shaped symbol.
a. Results for the RC-Spike model with two hidden layers using the Fashion-MNIST dataset. b. Results
for the RC-Spike model with a convolutional architecture using the CIFAR-10 dataset. c. Results for the
TTFS-SNN model using the Fashion-MNIST dataset.
43

a b
Figure 15: a. Optimization of current values during ANN-to-IMC mapping. A grid search was conducted on
positive and negative currents with a step size of 1%. The evaluation metric is the root mean square error
(RMSE) between the spike timing of the output layer from the RC-Spike model and that from the SPICE
simulation. The optimal point is marked by an orange star. b. Optimization of current values during SNN-
to-IMC mapping. The absolute value of the current is scaled uniformly in approximately 1% increments,
without distinguishing between positive and negative currents.
using ngspice [64]. Note that all circuit simulations presented in this study are post-layout simulations carried
out by extracting parasitic capacitances and resistances from the circuit layout [103]. The RC-Spike circuit
comprises two major components: a synapse circuit and neuron circuit. Detailed descriptions of these circuits
are provided in the following subsections.
Synapse circuits
Figure 16 a presents the synapse circuit layout result, where five spike signals (Spike #1–5) originating from
the preceding layer are input from the left. Each spike signal consists of two components: the original binary
signal and an inverted binary signal. These signals propagate through the fourth metal layer (horizontal green
lines). Upon receiving a spike signal, the synapse circuit generates a current that flows into the dendrites.
The dendrites are implemented using the second metal layer (vertical pink lines) and are connected to the
corresponding neuron circuits. Figure 16 b depicts a single synapse block, specifically the one enclosed by
orange dashed lines and arrows in Fig. 16 a. Each synapse block is composed of two N-type MOSFETs
(NMOS) transistors and two P-type MOSFETs (PMOS) transistors. In the synapse block connecting Spike
#j and Neuron #i, NSij represents the selector NMOS transistor, NM ij the memory NMOS transistor, PS ij
the selector PMOS transistors, and PM ij the memory PMOS transistors. The selector NMOS and PMOS
transistors are activated upon receiving a spike signal, allowing current to flow, while the memory NMOS and
PMOS transistors modulate the magnitude of the current. The current magnitude of the memory MOSFETs
is controlled by the bias voltage applied to their gate terminals; however, in this circuit, for simplicity, the
bias voltage was explicitly applied in the simulation. In actual in-memory computing (IMC) circuits, SRAM
circuits [46] or floating-gate technology [29] may be employed to control the magnitude of currents. The
MOSFETs used in each synapse block have a gate length of 250 nm and a gate width of 1 µm.
Memory MOS transistors (NM ij and PM ij) do not operate as an ideal current source, as the output
current is dependent on the drain voltage, which corresponds to the membrane potential in our circuit. Figs.
17 a and b present the simulation results illustrating the membrane potential dependence of synaptic currents
in memory MOS transistors under various gate voltages. Both NMOS and PMOS devices exhibit significant
current variations in response to changes in membrane potential. This behavior is primarily attributed to
channel-length modulation (CLM). The simulation demonstrates that the design achieves an excellent linear
fit with the data for membrane potentials ranging from 0.46V to 1.36V. From this fit, the CLM coefficients,
λN and λP , are obtained (Eq. (69) and Eq. (70)).
Figs. 17 c and d show the relationship between synaptic current and the CLM coefficient at a resting
(or initial) membrane potential of 1.3 V, under various gate voltage conditions. In the figure, the synaptic
current is plotted along the horizontal axis, while the corresponding gate voltage and CLM coefficient are
44

a b
NS42
NM42
PS42
PM42
Spike #1
Spike #2
Spike #3
Spike #4
Spike #5
Neuron #1 Neuron #2 Neuron #3 Neuron #4 Neuron #5
Figure 16: a. Layout of the array of synapse circuit blocks. In this schematic, five spike signals from the
preceding layer are transferred from the left side. These input spike signals trigger the generation of synaptic
currents, which are subsequently summed and directed toward the neuron circuit. b. Enlarged view of a
single synapse block, highlighted by orange dashed lines and arrows in a. Each synapse block comprises two
N-type MOSFETs (NMOS transistors) and two P-type MOSFETs (PMOS transistors). The ( i, j) synaptic
block connects the jth spike signal to the ith neuron circuit, where NS ij and PS ij serve as the NMOS and
PMOS selectors, respectively, and NM ij and PM ij function as the corresponding memory elements. The
selectors activate upon receiving a spike signal, enabling current flow, while the memory elements modulate
the magnitude of the current.
depicted on the vertical axis. While the CLM coefficient is dependent on the synaptic current at the resting
membrane potential, the variation remains relatively small, approximately 5%. In this study, for the sake of
simplicity, we have modeled the reversal potential as independent of the membrane potential. However, for
more accurate modeling, it would be beneficial to introduce a reversal potential E±
rev(·) that depends on the
membrane potential.
Neuron circuits
Figure 18 a presents the layout of the neuron circuit. The current from the synaptic circuit enters from the
top and is stored in the metal-insulator-metal capacitor (MIMCAP), represented by the yellow dashed line in
the figure. MIMCAP is implemented using the fourth and fifth metal layers, with a capacitance of 103.3 fF.
Digital control signals and bias signals for the neuron circuit propagate along the third metal layer (denoted
by the horizontal gray lines). In the MIMCAP configuration, the fourth metal layer is grounded, while the
fifth metal layer corresponds to the membrane potential, effectively suppressing capacitive coupling between
the digital signals and the metal line for the membrane potential.
The components enclosed by the orange dashed lines correspond to the sensing inverter and discharger
circuits, with their respective schematics depicted in Fig. 18 b and Fig. 18 c, respectively. The sensing
inverter is designed with a low firing threshold voltage, achieved by using high-threshold-voltage (HVT)
PMOS transistors and low-threshold-voltage (LVT) NMOS transistors. It is important to note that the
membrane potential is inverted, and in this circuit, the firing threshold voltage is set lower than the resting
membrane potential. Furthermore, by applying the bias voltage V sense
b to the PMOS transistor, the firing
threshold voltage can be reduced even further, while simultaneously minimizing the leakage current. This
circuit is active only when the Vphase signal is in the ON state.
The discharger circuit discharges the charge accumulated in MIMCAP to the ground line when the Vphase
signal is ON. The discharge current is controlled by an NMOS transistor, which is biased by the voltage V dis
b .
This NMOS transistor also plays a role in suppressing capacitive coupling between the Vphase signal and the
metal line for the membrane potential.
Fig. 19 a presents the input-output characteristics of the sensing inverter under various bias voltages
45

a
c
b
d
Figure 17: Current characteristics of synapse circuits. a. Dependence of drain current on drain voltage
for an N-type MOSFET (NMOS) for gate voltages varying in 0.2 V increments from 0.4 V to 0.65 V. b.
Dependence of drain current on drain voltage for a P-type MOSFET (PMOS) for gate voltages varying in
0.2 V increments from 1.3 V to 1.05 V. In a and b, both NMOS and PMOS devices have a gate length of
250 nm and a gate width of 1 µm. The vertical black dashed lines in the figures indicate the fitting range
of [0.43 V, 1.36 V], where a linear fit is applied to determine the non-ideal characteristic factor for each gate
voltage. c. Gate voltage dependence of the non-ideal characteristics. The horizontal axis represents the
drain current, while the two vertical axes indicate the corresponding gate voltage and the non-ideal factor,
λN, for the NMOS. Additionally, the green histogram illustrates the distribution of currents (corresponding
to weights) across the entire network. d. Same as c but illustrates the non-ideal factor λP for the PMOS.
46

Sensing 
inverter
Discharger
Synaptic 
current
Spikes
MIMCAP
a b c
Figure 18: a. Neuron circuit layout. Synaptic currents are introduced from the top via the synapse circuit.
The membrane capacitance is implemented using a metal-insulator-metal capacitor (MIMCAP) between the
fourth and fifth metal layers, with dimensions of 7 µm × 7 µm and a capacitance of 103.3 fF. The discharger
circuit and sensing inverter circuit are outlined by the orange dashed lines. The discharger circuit regulates
the discharge of the charge stored in the MIMCAP at a defined rate, while the sensing inverter detects when
the membrane potential drops below the firing threshold. Digital control signals and bias signals for the
neuron circuit propagate through the third metal layer (indicated by horizontal gray lines). b. Schematic
of sensing inverter circuit. To lower the firing threshold, an NMOS transistor with a low threshold voltage
(LVT) is connected at the input, and a PMOS transistor with a high threshold voltage (HVT) is used.
Additionally, a PMOS transistor connected to V sense
b allows the threshold voltage and current levels to be
fine-tuned. MOSFETs controlled by the Vphase signal and its inverted logic Vphase ensure that the sensing
inverter is active only when the Vphase signal is ON. c. Schematic of discharger circuit. When the Vphase
signal is ON, the charge stored in the membrane capacitance is discharged to the ground. The discharge
current is controlled by the V dis
b signal.
a b
Figure 19: a. Input-output voltage characteristics of the sensing inverter circuit under various bias voltages
V sense
b . b. Power dissipation characteristics of the sensing inverter circuit under various bias voltages V sense
b .
47

a b
Figure 20: a. Dependence of the discharger circuit current on membrane potential under various bias voltages
of V dis
b . The vertical black dashed lines in the figures indicate the fitting range of [0 .43 V, 1.36 V], where a
linear fit is applied to determine the non-ideal characteristic factor λdis for each gate voltage. b. Dependence
of the CLM coefficient on current in the discharger circuit. The horizontal axis represents the drain current,
while the two vertical axes indicate the corresponding gate voltage and the non-ideal factor, λdis, for the
NMOS.
V sense
b . As described earlier, the bias voltage V sense
b modulates the firing threshold. Fig. 19 b depicts the
variation in through-current levels as a function of V sense
b . A significant reduction in through-current led
to a slower rise in the output voltage of the sensing inverter, which, in turn, increased the total amount of
through-current of the subsequent NAND gate. To mitigate excessive power dissipation in the subsequent
NAND gate, the V sense
b bias voltage was set to 0.95 V.
The discharger circuit utilizes an NMOS as a current source, leading to a current output that is dependent
on the membrane potential. Fig. 20 a illustrates the membrane potential dependence of the current in the
discharger circuit. Similar to the behavior observed in memory cells, a nearly perfect linear fit can be achieved
within the potential range of 0.46 to 1.36 V. Fig. 20 b presents the CLM coefficient λdis corresponding to the
current at the resting membrane potential. The bias voltage V dis
b was configured at 0.554 V, which ensures
that no firing occurs during the firing phase in the absence of input spikes.
References
[1] Dong, S., Wang, P. & Abbas, K. A survey on deep learning and its applications. Computer Science
Review 40, 100379 (2021).
[2] Kaplan, J. et al. Scaling laws for neural language models. arXiv:2001.08361 (2020).
[3] Bahri, Y., Dyer, E., Kaplan, J., Lee, J. & Sharma, U. Explaining neural scaling laws. Proceedings of
the National Academy of Sciences 121, e2311878121 (2024).
[4] OpenAI et al. GPT-4 technical report. arXiv:2303.08774 (2024).
[5] Dubey, A. et al. The Llama 3 herd of models. arXiv:2407.21783 (2024).
[6] Liu, Y. et al. Sora: A review on background, technology, limitations, and opportunities of large vision
models. arXiv:2402.17177 (2024).
[7] OpenAI. Learning to reason with LLMs (2024). URL https://openai.com/index/
learning-to-reason-with-llms/ .
[8] Thompson, N. C., Greenewald, K., Lee, K. & Manso, G. F. The computational limits of deep learning.
arXiv:2007.05558 (2020).
[9] Patterson, D. et al. Carbon emissions and large neural network training. arXiv 2104.10350 (2021).
48

[10] Murshed, M. G. S. et al. Machine learning at the network edge: A survey. ACM Comput. Surv. 54,
1–37 (2021).
[11] Krizhevsky, A., Sutskever, I. & Hinton, G. E. Imagenet classification with deep convolutional neural
networks. In Advances in Neural Information Processing Systems , vol. 25 (2012).
[12] Horowitz, M. 1.1 computing’s energy problem (and what we can do about it). In 2014 IEEE Interna-
tional Solid-State Circuits Conference Digest of Technical Papers (ISSCC) , 10–14 (2014).
[13] Dally, B. Hardware for deep learning. In 2023 IEEE Hot Chips 35 Symposium (HCS) , 1–58 (Los
Alamitos, CA, USA, 2023).
[14] Jouppi, N. P. et al. In-datacenter performance analysis of a tensor processing unit. SIGARCH Comput.
Archit. News 45, 1–12 (2017).
[15] Sze, V., Chen, Y.-H., Yang, T.-J. & Emer, J. S. Efficient processing of deep neural networks: A tutorial
and survey. Proceedings of the IEEE 105, 2295–2329 (2017).
[16] Guo, K. et al. Neural network accelerator comparison. https://nicsefc.ee.tsinghua.edu.cn/project.html
(2024).
[17] Verma, N. et al. In-memory computing: Advances and prospects. IEEE Solid-State Circuits Magazine
11, 43–55 (2019).
[18] Roy, K., Chakraborty, I., Ali, M., Ankit, A. & Agrawal, A. In-memory computing in emerging mem-
ory technologies for machine learning: An overview. In 2020 57th ACM/IEEE Design Automation
Conference (DAC), 1–6 (2020).
[19] Sebastian, A., Le Gallo, M., Khaddam-Aljameh, R. & Eleftheriou, E. Memory devices and applications
for in-memory computing. Nature Nanotechnology 15, 529–544 (2020).
[20] Song, M.-K. et al. Recent advances and future prospects for memristive materials, devices, and systems.
ACS Nano 17, 11994–12039 (2023).
[21] Duan, X. et al. Memristor-based neuromorphic chips. Advanced Materials 36, 2310704 (2024).
[22] Aguirre, F. et al. Hardware implementation of memristor-based artificial neural networks. Nature
communications 15, 1974 (2024).
[23] Fujiwara, H. et al. A 5-nm 254-tops/w 221-tops/mm2 fully-digital computing-in-memory macro sup-
porting wide-range dynamic-voltage-frequency scaling and simultaneous mac and write operations. In
2022 IEEE International Solid-State Circuits Conference (ISSCC) , vol. 65, 1–3 (2022).
[24] Valavi, H., Ramadge, P. J., Nestler, E. & Verma, N. A 64-tile 2.4-mb in-memory-computing cnn
accelerator employing charge-domain compute. IEEE Journal of Solid-State Circuits 54, 1789–1799
(2019).
[25] Wu, P.-C. et al. A 28nm 1mb time-domain computing-in-memory 6t-sram macro with a 6.6ns latency,
1241gops and 37.01tops/w for 8b-mac operations for edge-ai devices. In 2022 IEEE International
Solid-State Circuits Conference (ISSCC) , vol. 65, 1–3 (2022).
[26] Wang, H. et al. A charge domain SRAM compute-in-memory macro with C-2C ladder-based 8-bit MAC
unit in 22-nm FinFET process for edge inference. IEEE Journal of Solid-State Circuits 58, 1037–1050
(2023).
[27] Yoshioka, K. 34.5 a 818-4094 TOPS/W capacitor-reconfigured CIM macro for unified acceleration of
CNNs and transformers. In 2024 IEEE International Solid-State Circuits Conference (ISSCC) , vol. 67,
574–576 (2024).
[28] Tagata, H., Sato, T. & Awano, H. Double MAC on a cell: A 22-nm 8T-SRAM based analog in-memory
accelerator for binary/ternary neural networks featuring split wordline. IEEE Open Journal of Circuits
and Systems 1–1 (2024).
49

[29] Bavandpour, M., Mahmoodi, M. R. & Strukov, D. B. Energy-efficient time-domain vector-by-matrix
multiplier for neurocomputing and beyond. IEEE Transactions on Circuits and Systems II: Express
Briefs 66, 1512–1516 (2019).
[30] Xiao, T. P. et al. An accurate, error-tolerant, and energy-efficient neural network inference engine
based on sonos analog memory. IEEE Transactions on Circuits and Systems I: Regular Papers 69,
1480–1493 (2022).
[31] Wan, W. et al. A compute-in-memory chip based on resistive random-access memory. Nature 608,
504–512 (2022).
[32] Dalgaty, T. et al. Mosaic: in-memory computing and routing for small-world spike-based neuromorphic
systems. Nature Communications 15, 142 (2024).
[33] Le Gallo, M. et al. A 64-core mixed-signal in-memory compute chip based on phase-change memory
for deep neural network inference. Nature Electronics 6, 680–693 (2023).
[34] Razavi, B. Design of Analog CMOS Integrated Circuit, Second Edition (McGraw-Hidd Education,
2017).
[35] Krestinskaya, O. et al. Neural architecture search for in-memory computing-based deep learning accel-
erators. Nature Reviews Electrical Engineering 1–17 (2024).
[36] Byrne, J. H., Heidelberger, R. & Waxham, M. N. From molecules to networks: an introduction to
cellular and molecular neuroscience (Academic Press, 2014).
[37] Cutsuridis, V., Graham, B. P., Cobb, S. & Vida, I. Hippocampal microcircuits: a computational
modeler’s resource book (Springer, 2019).
[38] Ito, S. GABA and glycine in the developing brain. The journal of physiological sciences 66, 375–379
(2016).
[39] Mead, C. How we created neuromorphic engineering. Nature Electronics 3, 434–435 (2020).
[40] Frenkel, C., Bol, D. & Indiveri, G. Bottom-up and top-down approaches for the design of neuromorphic
processing systems: Tradeoffs and synergies between natural and artificial intelligence. Proceedings of
the IEEE 111, 623–652 (2023).
[41] Wright, L. G. et al. Deep physical neural networks trained with backpropagation. Nature 601, 549–555
(2022).
[42] Momeni, A. et al. Training of physical neural networks. arXiv:2406.03372 (2024).
[43] Krizhevsky, A. Learning multiple layers of features from tiny images. URL https://www.cs.
toronto.edu/˜kriz/learning-features-2009-TR.pdf .
[44] Google and SkyWater Technology. SKY130 PDK’s documentation. URL https://skywater-pdk.
readthedocs.io/en/main/.
[45] Cai, F. et al. A fully integrated reprogrammable memristor–CMOS system for efficient multiply–
accumulate operations. Nature Electronics 2, 290–299 (2019).
[46] Yamaguchi, M., Iwamoto, G., Nishimura, Y., Tamukoh, H. & Morie, T. An energy-efficient time-domain
analog CMOS BinaryConnect neural network processor based on a pulse-width modulation approach.
IEEE Access 9, 2644–2654 (2021).
[47] Saito, D. et al. IGZO-based compute cell for analog in-memory computing—dtco analysis to enable
ultralow-power AI at edge. IEEE Transactions on Electron Devices 67, 4616–4620 (2020).
[48] Hayakawa, Y. et al. Highly reliable TaOx ReRAM with centralized filament for 28-nm embedded
application. In 2015 Symposium on VLSI Technology (VLSI Technology) , T14–T15 (2015).
50

[49] Ankiti, A. et al. PUMA: A programmable ultra-efficient memristor-based accelerator for machine
learning inference. In Proceedings of the Twenty-Fourth International Conference on Architectural
Support for Programming Languages and Operating Systems , 715—-731 (2019).
[50] Liu, T.-y. et al. A 130.7-mm2 2-layer 32-gb ReRAM memory device in 24-nm technology. IEEE Journal
of Solid-State Circuits 49, 140–153 (2014).
[51] Zhou, K. et al. High-density 3-D stackable crossbar 2D2R nvTCAM with low-power intelligent search
for fast packet forwarding in 5G applications. IEEE Journal of Solid-State Circuits 56, 988–1000
(2021).
[52] Wang, Q., Tamukoh, H. & Morie, T. A time-domain analog weighted-sum calculation model for
extremely low power VLSI implementation of multi-layer neural networks. arXiv:1810.06819 (2018).
[53] Gerstner, W., Kistler, W. M., Naud, R. & Paninski, L. Neuronal dynamics: From single neurons to
networks and models of cognition (Cambridge University Press, 2014).
[54] Sakemi, Y., Morino, K., Morie, T., Hosomi, T. & Aihara, K. A spiking neural network with resistively
coupled synapses using time-to-first-spike coding towards efficient charge-domain computing. In 2022
IEEE International Symposium on Circuits and Systems (ISCAS) , 2152–2156 (2022).
[55] Mostafa, H. Supervised learning based on temporal coding in spiking neural networks. IEEE Transac-
tions on Neural Networks and Learning Systems 29, 3227–3235 (2018).
[56] Kheradpisheh, S. R. & Masquelier, T. Temporal backpropagation for spiking neural networks with one
spike per neuron. International Journal of Neural Systems 30, 2050027 (2020).
[57] Baydin, A. G., Pearlmutter, B. A., Radul, A. A. & Siskind, J. M. Automatic differentiation in machine
learning: a survey. Journal of Machine Learning Research 18, 1–43 (2018).
[58] Paszke, A. et al. Automatic differentiation in pytorch (2017). URL https://openreview.net/
forum?id=BJJsrmfCZ.
[59] Bohte, S. M., Kok, J. N. & Poutr´ e, H. L. Error-backpropagation in temporally encoded networks of
spiking neurons. Neurocomputing 48, 17–37 (2002).
[60] Xiao, H., Rasul, K. & Vollgraf, R. Fashion-MNIST: a novel image dataset for benchmarking machine
learning algorithms. arXiv:1708.07747 (2017).
[61] Simonyan, K. & Zisserman, A. Very deep convolutional networks for large-scale image recognition.
arXiv:1409.1556 (2015).
[62] XSCHEM : schematic capture and netlisting EDA tool. https://xschem.sourceforge.io/
stefan/index.html.
[63] Magic VLSI. http://opencircuitdesign.com/magic/.
[64] ngspice - open source spice simulator. https://ngspice.sourceforge.io/.
[65] netgen. http://opencircuitdesign.com/netgen/.
[66] FISHER, R. A. The use of multiple measurements in taxonomic problems. Annals of Eugenics 7, 179–
188 (1936). URL https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-1809.
1936.tb02137.x.
[67] Pedregosa, F. et al. Scikit-learn: Machine learning in Python. Journal of Machine Learning Research
12, 2825–2830 (2011).
[68] Dampfhoffer, M., Mesquida, T., Valentian, A. & Anghel, L. Backpropagation-based learning techniques
for deep spiking neural networks: A survey. IEEE Transactions on Neural Networks and Learning
Systems 1–16 (2023).
[69] Eshraghian, J. K. et al. Training spiking neural networks using lessons from deep learning. Proceedings
of the IEEE 111, 1016–1054 (2023).
51

[70] Neftci, E. O., Mostafa, H. & Zenke, F. Surrogate gradient learning in spiking neural networks: Bringing
the power of gradient-based optimization to spiking neural networks. IEEE Signal Processing Magazine
36, 51–63 (2019).
[71] Wu, Y., Deng, L., Li, G., Zhu, J. & Shi, L. Spatio-temporal backpropagation for training high-
performance spiking neural networks. Frontiers in Neuroscience 12 (2018).
[72] Neftci, E. O., Augustine, C., Paul, S. & Detorakis, G. Event-driven random back-propagation: Enabling
neuromorphic deep learning machines. Frontiers in Neuroscience 11 (2017).
[73] Huh, D. & Sejnowski, T. J. Gradient descent for spiking neural networks. In Proceedings of the 32nd
International Conference on Neural Information Processing Systems , 1440—-1450 (2018).
[74] Zenke, F. & Ganguli, S. SuperSpike: Supervised Learning in Multilayer Spiking Neural Networks.
Neural Computation 30, 1514–1541 (2018).
[75] Rathi, N., Srinivasan, G., Panda, P. & Roy, K. Enabling deep spiking neural networks with hybrid
conversion and spike timing dependent backpropagation (2020).
[76] Kim, Y. & Panda, P. Revisiting batch normalization for training low-latency deep spiking neural
networks from scratch. Frontiers in Neuroscience 15 (2021).
[77] Yin, B., Corradi, F. & Boht´ e, S. M. Accurate and efficient time-domain classification with adaptive
spiking recurrent neural networks. Nature Machine Intelligence 3, 905–913 (2021).
[78] Zenke, F. & Vogels, T. P. The Remarkable Robustness of Surrogate Gradient Learning for Instilling
Complex Function in Spiking Neural Networks. Neural Computation 33, 899–925 (2021).
[79] Gygax, J. & Zenke, F. Elucidating the theoretical underpinnings of surrogate gradient learning in
spiking neural networks. arXiv:2404.14964 (2024).
[80] Cramer, B. et al. Surrogate gradients for analog neuromorphic computing. Proceedings of the National
Academy of Sciences 119, e2109194119 (2022).
[81] Comsa, I. M. et al. Temporal coding in spiking neural networks with alpha synaptic function. In IEEE
International Conference on Acoustics, Speech and Signal Processing (ICASSP) , 8529–8533 (2020).
[82] G¨ oltz, J.et al. Fast and energy-efficient neuromorphic deep learning with first-spike times. Nature
Machine Intelligence 3, 823–835 (2021).
[83] Sakemi, Y., Morino, K., Morie, T. & Aihara, K. A supervised learning algorithm for multilayer
spiking neural networks based on temporal coding toward energy-efficient VLSI processor design. IEEE
Transactions on Neural Networks and Learning Systems 34, 394–408 (2023).
[84] Zhou, S., Li, X., Chen, Y., Chandrasekaran, S. T. & Sanyal, A. Temporal-coded deep spiking neural
network with easy training and robust performance. Proceedings of the AAAI Conference on Artificial
Intelligence 35, 11143–11151 (2021).
[85] Sakemi, Y., Yamamoto, K., Hosomi, T. & Aihara, K. Sparse-firing regularization methods for spiking
neural networks with time-to-first-spike coding. Scientific Reports 13, 22897 (2023).
[86] Wunderlich, T. C. & Pehle, C. Event-based backpropagation can compute exact gradients for spiking
neural networks. Scientific Reports 11, 12829 (2021).
[87] Yamamoto, K., Sakemi, Y. & Aihara, K. Can timing-based backpropagation overcome single-spike
restrictions in spiking neural networks? In 2024 International Joint Conference on Neural Networks
(IJCNN), 1–9 (2024).
[88] Kim, J., Kim, K. & Kim, J.-J. Unifying activation- and timing-based learning rules for spiking neural
networks. Advances in Neural Information Processing Systems 33, 19534–19544 (2020).
[89] Park, S., Kim, S., Na, B. & Yoon, S. T2FSNN: Deep spiking neural networks with time-to-first-spike
coding. In 2020 57th ACM/IEEE Design Automation Conference (DAC) , 1–6 (2020).
52

[90] Chen, R. T. Q., Rubanova, Y., Bettencourt, J. & Duvenaud, D. K. Neural ordinary differential
equations. In Advances in Neural Information Processing Systems , vol. 31 (2018).
[91] Rotter, S. & Diesmann, M. Exact digital simulation of time-invariant linear systems with applications
to neuronal modeling. Biological cybernetics 81, 381–402 (1999).
[92] Tian, Z.-q. K. & Zhou, D. Exponential time differencing algorithm for pulse-coupled hodgkin-huxley
neural networks. Frontiers in Computational Neuroscience 14 (2020).
[93] Chen, J., Yang, S., Wu, H., Indiveri, G. & Payvand, M. Scaling limits of memristor-based routers for
asynchronous neuromorphic systems. IEEE Transactions on Circuits and Systems II: Express Briefs
71, 1576–1580 (2024).
[94] Joshi, V. et al. Accurate deep neural network inference using computational phase-change memory.
Nature communications 11, 2473 (2020).
[95] Cao, T. et al. A non-idealities aware software–hardware co-design framework for edge-ai deep neural
network implemented on memristive crossbar. IEEE Journal on Emerging and Selected Topics in
Circuits and Systems 12, 934–943 (2022).
[96] Rasch, M. J. et al. Hardware-aware training for large-scale and diverse deep learning inference workloads
using in-memory computing-based accelerators. Nature communications 14, 5282 (2023).
[97] Raissi, M., Perdikaris, P. & Karniadakis, G. Physics-informed neural networks: A deep learning
framework for solving forward and inverse problems involving nonlinear partial differential equations.
Journal of Computational Physics 378, 686–707 (2019).
[98] Okamoto, Y. & Kojima, R. Learning deep dissipative dynamics. arXiv:2408.11479 (2024).
[99] Zhang, M. et al. Rectified linear postsynaptic potential function for backpropagation in deep spiking
neural networks. IEEE Transactions on Neural Networks and Learning Systems 33, 1947–1958 (2022).
[100] Maass, W. Computing with Spiking Neurons. In Pulsed Neural Networks (The MIT Press, 1998).
[101] Kingma, D. P. & Ba, J. Adam: A method for stochastic optimization. arXiv 1412.6980 (2014).
[102] Com¸ sa, I.-M.et al. Temporal coding in spiking neural networks with alpha synaptic function: Learning
with backpropagation. IEEE Transactions on Neural Networks and Learning Systems 33, 5939–5952
(2022).
[103] Edwards, T. Chipalooza workshop #3: Physical verification. https://www.youtube.com/watch?
v=PCpsgkmM6u4 (2024).
53
