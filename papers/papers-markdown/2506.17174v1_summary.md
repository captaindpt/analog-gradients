# High-accuracy inference using HfOxSy/HfS2 Memristors

**Authors:** Aferdita Xhameni, Antonio Lombardo (UCL)
**Year:** 2025
**arXiv:** 2506.17174v1

## Core Thesis
This paper demonstrates that **2D material-based memristors** (HfOxSy/HfS2) can achieve near-SOTA accuracy in neural network inference tasks while being **forming-free** and **compliance-free** - solving two major practical problems that plague traditional memristor implementations.

## The Key Innovation: Better Memristors

This paper represents an **evolution** of memristor technology by using **2D van der Waals semiconductors** (hafnium disulfide - HfS2) that are partially oxidized to create HfOxSy/HfS2 structures.

**Why this matters:**
```
Traditional Memristors (Paper 1 - TaOx/HfOx):
    ❌ Need "electroforming" (one-time initialization with high voltage)
    ❌ Need current compliance circuits (1T1M - one transistor per memristor)
    ❌ Complex peripheral circuitry

This Paper's Memristors (HfOxSy/HfS2):
    ✅ Forming-free (ready to use immediately)
    ✅ Compliance-free (self-limiting, won't damage itself)
    ✅ Simpler circuits, better scaling
    ✅ Based on 2D layered materials (pristine interfaces)
```

## Device Performance: The Numbers

**Key Characteristics:**
- **31 conductance states** (~5 bits of precision)
- **Ultra-low voltage:** 0.7V-0.995V (sub-1V operation!)
- **Ultra-fast switching:** 160ns-350ns pulses
- **Low energy:** ~24nJ per programming cycle
- **Highly linear:** R² = 0.999 for both potentiation & depression
- **Symmetric:** Very similar behavior for increasing/decreasing conductance
- **100 cycles tested:** Stable, repeatable performance

**Compare to typical digital:**
```
Digital RAM: 1.2V-3.3V
This memristor: <1V

Traditional RRAM forming: 3-5V
This memristor: No forming needed
```

## The Workflow: From Lab Device to ML Simulation

They use **IBM's AIHWKIT** (Analog Hardware Acceleration Kit) to simulate what would happen if you built a neural network chip with these memristors:

```
1. Measure real device characteristics:
   - Potentiation/depression linearity
   - Cycle-to-cycle variation
   - Retention (how long states last)

2. Train neural network on GPU (digital, perfect):
   - MNIST: 98.07% accuracy
   - CIFAR-10: 87.51% accuracy

3. Simulate deploying to memristor hardware:
   - Account for device limitations
   - Hardware-Aware Training (HWAT): Retrain with device constraints
   - Add drift compensation

4. Evaluate accuracy loss:
   - MNIST: 98.00% (only 0.07% loss!)
   - CIFAR-10: 86.80% (only 0.9% loss!)
```

## Performance Results

**MNIST (handwritten digits):**
- SOTA digital: 98.07%
- With HfOxSy/HfS2 memristors: 98.00%
- **Difference: <0.07%** - essentially identical!

**CIFAR-10 (color images, much harder):**
- SOTA digital: 87.51%
- With HfOxSy/HfS2 memristors: 86.80%
- **Difference: <0.9%** - excellent for analog hardware!

**Network size (CIFAR-10):**
- ~420,000 memristors needed (differential configuration)
- ~210,000 weights stored
- 3 ResNet blocks + fully connected layer

## The Critical Insight About Device Variation

They systematically studied what causes accuracy loss:

```
Device-to-Device Variation (DTOD): BIGGEST FACTOR
    - At 30% DTOD variation: <1% accuracy loss
    - This is achievable with existing fabrication (ALD for HfO2)

Cycle-to-Cycle Variation: WELL CONTROLLED
    - Their linearity is excellent (R² = 0.999)
    - Standard deviation < bin width
    - States don't overlap

Drift Over Time: MANAGEABLE
    - With hardware-aware training + drift compensation: maintains accuracy
    - Without compensation: significant degradation
```

## Comparison to Other Memristors

They compare to 4 other prominent memristor papers:

| Device | MNIST Loss | CIFAR-10 Loss | Max Voltage | Forming? | Compliance? |
|--------|-----------|---------------|-------------|----------|-------------|
| Yao (TaOx/HfOx) | 1.8% | 1.5% | 2.5V | Yes | Yes |
| Pan (HfO2) | 0.4% | - | 2.5V | Yes | Yes |
| Nguyen | 0.2% | - | 3V | Yes | Yes |
| Lu (SnS) | - | 0.9% | 1V | No | Yes |
| **This work (HfOxSy/HfS2)** | **0.07%** | **0.9%** | **<1V** | **No** | **No** |

**Their advantages:**
- Lowest voltage with high accuracy
- No forming OR compliance needed
- Based on 2D materials (better interfaces, defect engineering)

## The Technical Breakthrough: Tailored Pulse Trains

Like other advanced memristors, they use **adaptive pulsing schemes:**

```
Not this:
    [Same voltage pulse] × 100 → Non-linear conductance change

But this:
    [0.70V, 160ns] → [0.10V, 160ns] → ... → [0.89V, 255ns]
    Increasing voltage and width → Linear conductance change

Result:
    - 31 distinguishable states
    - High symmetry between potentiation & depression
    - Low variation
```

**Trade-off:**
- ✅ Much better linearity and precision
- ❌ More complex peripheral circuits
- ❌ Slightly higher latency

But their simulations account for this realistically.

## Connection to Papers 1 & 2

| Aspect | Paper 1 (Memristors) | Paper 2 (Photonics) | This Paper |
|--------|---------------------|-------------------|------------|
| Medium | TaOx/HfOx electrons | Light (photons) | HfOxSy/HfS2 electrons |
| Training | On-chip | External | External (then deploy) |
| Speed | Nanoseconds | Picoseconds | Nanoseconds |
| Precision | ~4-5 bits | ~8 bits | ~5 bits |
| Key advance | First on-chip training | Fastest inference | Best practical device |
| Forming? | Yes | N/A | **No** |
| Compliance? | Yes | N/A | **No** |

## The Paradigm This Represents

**Practical, Deployable Analog AI:**

```
Paper 1: Proof of concept (but needs forming + compliance)
Paper 2: Ultimate speed (but no on-chip training yet)
This Paper: Practical implementation path
    - No forming → Simpler manufacturing
    - No compliance → No 1T1M needed → Higher density
    - Sub-1V operation → Lower power
    - High accuracy with realistic device model
```

## Why 2D Materials Matter Here

**van der Waals semiconductors (like HfS2) enable:**

1. **Pristine interfaces:** Atomically sharp oxide/semiconductor boundary
2. **Precise defect engineering:** Control oxidation to create HfOxSy layer
3. **Self-limiting behavior:** Layered structure naturally limits current
4. **No forming needed:** Pre-existing conductive pathways from controlled oxidation

```
Traditional oxide memristors:
    [Metal] / [Deposited oxide - random defects] / [Metal]
    → Needs forming to create initial filament
    → Needs compliance to prevent damage

2D material memristors:
    [Metal] / [Controlled HfOxSy - ordered defects] / [HfS2 layers] / [Metal]
    → Pathways already exist from fabrication
    → Layered structure self-limits
```

## Realistic Simulation Approach

They account for many non-idealities:

✅ **Cycle-to-cycle variation** (measured from 100 cycles)
✅ **Device-to-device variation** (simulated at 30% based on literature)
✅ **Conductance drift** (using IBM's ReRAMWan2022 drift model)
✅ **IR drop** (voltage drops in crossbar wires)
✅ **ADC/DAC limitations** (8-bit converters)
✅ **Weight quantization** (5-bit states)
✅ **Peripheral circuitry** (realistic implementation)

This makes their accuracy predictions trustworthy.

## Hardware-Aware Training (HWAT)

A key technique they use:

```
Standard approach:
    1. Train perfectly on GPU
    2. Deploy to imperfect memristors
    3. ❌ Accuracy drops significantly

HWAT approach:
    1. Train on GPU
    2. Simulate memristor imperfections during last 5-20 epochs
    3. Network learns to be robust to device limitations
    4. Deploy to memristors
    5. ✅ Much better accuracy retention

For CIFAR-10: ~9% accuracy difference with vs without HWAT!
```

## The Grand Vision This Supports

This paper shows the path to **commercially viable analog AI chips:**

```
Requirements for industry adoption:
    ✅ No forming (simpler manufacturing, no pre-programming)
    ✅ No compliance (higher density, simpler circuits)
    ✅ Low voltage (<1V for advanced nodes)
    ✅ Fast switching (<1µs)
    ✅ High accuracy (< 1% loss vs digital)
    ✅ Scalable fabrication method (demonstrated for similar materials)
    ✅ Realistic simulation framework (AIHWKIT)

This paper: Checks all boxes (or shows clear path)
```

## Limitations & Future Work

**Current limitations:**
- Still train on GPU, then deploy (not full on-chip learning like Paper 1)
- Devices fabricated individually (not large arrays yet)
- DTOD variation assumed from literature (needs experimental verification)
- Network size limited (3 ResNet blocks, not 50+)

**Future directions:**
- Scale up to large crossbar arrays
- Demonstrate on-chip training (updating weights in hardware)
- Test larger, deeper networks
- Verify DTOD variation experimentally
- Optimize for even lower energy

## Connection to Your Original Insight

This paper directly addresses your question from a **practical engineering perspective:**

**You asked:** "Why use deterministic digital gates to simulate learning?"

**Paper 1 said:** "Use memristors that physically learn (but need forming + compliance)"

**Paper 2 said:** "Use light instead of electrons (but no on-chip learning)"

**This paper says:** "Use better memristors (2D materials) that solve the practical problems holding back analog AI"

The key insight: **Material science enables better analog computing**

```
Traditional memristors: Random defects → Needs forming → Needs compliance
2D material memristors: Engineered defects → No forming → No compliance → Practical!

The substrate matters: Not just "use analog," but "use the RIGHT analog"
```

## Bottom Line

This is about **making analog AI practical for real deployment:**

- **Highest accuracy** demonstrated for analog hardware (< 1% loss on CIFAR-10)
- **Lowest operating voltage** while maintaining accuracy (<1V)
- **Simplest operation** (no forming, no compliance)
- **Realistic simulation** (accounts for all major non-idealities)
- **Clear path to manufacturing** (2D materials, demonstrated techniques)

The answer to "how do we build machines that actually learn in hardware" is getting clearer: **engineer the material structure at the atomic level** (2D materials) to get the electrical properties you need for analog computation.
