# Training Physical Neural Networks for Analog In-Memory Computing

**Authors:** Yusuke Sakemi, Kazuki Yamamoto, Takahiro Morino, Hiroshi Hosomi, Tetsuya Asai, Tomoki Hosaka, Kohei M. Itoh, Toshikazu Hashimoto, Hakaru Tamukoh
**Year:** 2024
**Paper ID:** 2412.09010v1

## Core Thesis

This paper proposes **Physical Neural Networks (PNNs)** as a framework for training analog in-memory computing (IMC) circuits directly on real hardware using **differentiable spike-time discretization (DSTD)**. The authors argue that instead of mapping pre-trained networks to analog hardware (which suffers from hardware non-idealities), we should train networks that embrace the physical characteristics of the circuits themselves—essentially using the analog IMC hardware as the neural network.

## The Central Problem

Traditional approaches to analog IMC face a fundamental mismatch:
1. Neural networks are trained in ideal software environments
2. Networks are then mapped to analog hardware with non-idealities (device variations, non-linearity, noise)
3. Performance degrades significantly due to the reality gap between simulation and hardware

The authors' insight: **What if the analog circuit itself IS the neural network, and we train it directly using its actual physical behavior?**

## Technical Approach

### RC-Spike Model

The authors developed the **RC-Spike circuit model** that implements time-to-first-spike (TTFS) spiking neural networks using CMOS technology:

**Key Circuit Components:**
- **Synapse circuits**: 1T1R (1-transistor-1-resistor) topology using MOSFET current sources
  - NMOS for excitatory currents (positive weights)
  - PMOS for inhibitory currents (negative weights)
  - Each synapse: 250nm gate length, 1μm gate width
  - Synaptic current depends on membrane potential (non-ideal behavior modeled)

- **Neuron circuits**:
  - MIMCAP (metal-insulator-metal capacitor) for charge storage: 103.3 fF capacitance
  - Sensing inverter with adjustable firing threshold
  - Discharger circuit for resetting membrane potential
  - 4th and 5th metal layers for MIMCAP to reduce capacitive coupling

**Physical Non-Idealities Explicitly Modeled:**
1. **Channel-length modulation (CLM)**: Current sources aren't ideal—output current depends on drain voltage (membrane potential)
   - λN and λP coefficients extracted from SPICE simulations
   - ~5% variation in CLM coefficient across operating range

2. **Reversal potentials**: E+_rev and E-_rev determine the equilibrium potentials for excitatory and inhibitory currents
   - These are hardware-specific parameters that must be learned

3. **Capacitive coupling**: Between digital control signals and membrane potential lines

4. **Leakage currents**: Minimized but not eliminated

### Differentiable Spike-Time Discretization (DSTD)

The breakthrough innovation for training:

**The Problem:** Spike times are discrete events that are non-differentiable—can't use standard backpropagation.

**The Solution:** DSTD makes spike timing differentiable by:
1. Treating the membrane potential evolution as a continuous dynamical system
2. Using implicit function theorem to compute gradients through spike times
3. Allowing backpropagation through the circuit equations directly

**Mathematical Framework:**
- Forward pass: Simulate actual RC circuit dynamics to get spike times
- Backward pass: Compute gradients ∂L/∂θ where θ includes:
  - Synaptic weights (gate voltages controlling MOSFET currents)
  - Neuron biases
  - Physical parameters like E+_rev and E-_rev

### Training Process

**Two-Stage Training:**

1. **ANN-to-IMC mapping**:
   - Train a standard ANN using software
   - Map weights to RC-Spike circuit parameters
   - Fine-tune physical parameters (E+_rev, E-_rev) using grid search
   - Validation: SPICE simulation matches RC-Spike model behavior

2. **SNN-to-IMC mapping**:
   - Train TTFS spiking neural network directly using DSTD
   - Network learns to work with the circuit's physical characteristics
   - No separate mapping phase—circuit and algorithm co-designed

**Fabrication and Testing:**
- Designed using **Sky130 PDK** (SkyWater 130nm process)
- 8th generation SONOS (Silicon-Oxide-Nitride-Oxide-Silicon)
- 150nm gate length for memory elements
- Full post-layout SPICE simulations with parasitic extraction
- Tools: XSCHEM (schematic), Magic VLSI (layout), ngspice (simulation)

## Performance Results

### Iris Dataset (Classical ML Benchmark)
- **ANN-to-IMC**: 93.33% accuracy
- **SNN-to-IMC**: 97.78% accuracy
- **Insight**: Direct training on hardware (SNN-to-IMC) outperforms mapping approach

### Fashion-MNIST (Image Classification)
- **ANN-to-IMC**: 83.64% accuracy
- **SNN-to-IMC**: 86.59% accuracy
- **Network**: 784-256-128-10 architecture
- Shows scalability to realistic computer vision tasks

### CIFAR-10 (Complex Images)
- **SNN-to-IMC with VGG-5**: 64.33% accuracy
- Demonstrates feasibility on challenging datasets
- Performance gap due to hardware constraints (small circuit, limited precision)

### Energy Efficiency
- **TTFS coding**: Only one spike per neuron → minimal energy
- Time-domain computing: No high-precision ADCs needed
- Charge-domain integration: Natural accumulation in capacitors

## Connections to Other Papers

### Paper 1 (2017 Memristor Training):
- Paper 1: First on-chip training with memristors (ReRAM)
- Paper 6 (this one): Training with CMOS circuits (MOSFETs as analog memory)
- **Common theme**: Both train directly on hardware rather than mapping pre-trained models
- **Difference**: Memristors vs. CMOS; current-domain vs. time-domain computing

### Paper 2 (Photonic Computing):
- Paper 2: Used photonic hardware for forward/backward passes
- Paper 6: Uses analog CMOS circuits with spike-time coding
- **Common theme**: Both treat physical hardware as the computational substrate
- **Key similarity**: Both train networks that embrace hardware non-idealities

### Paper 3 (HfOxSy 2D Materials):
- Paper 3: Novel memristor materials with better analog properties
- Paper 6: CMOS-based approach (more manufacturable with existing fabs)
- **Trade-off**: Paper 3 has better precision; Paper 6 has better manufacturability

### Paper 4 (AGAD/Chopped-TTv2):
- Paper 4: Training algorithms for analog hardware with gradient accumulation
- Paper 6: DSTD method for spike-time-based training
- **Common theme**: Both develop specialized training methods for analog hardware
- **Difference**: Paper 4 uses rate-based coding; Paper 6 uses time-based (TTFS) coding

### Paper 5 (Bulk-Switching Memristor CIM):
- Paper 5: Mixed-precision training with selective weight updates
- Paper 6: Direct hardware training with spike-time discretization
- **Common theme**: Both address hardware limitations through co-designed algorithms
- **Complementary**: Paper 5 focuses on endurance; Paper 6 focuses on modeling non-idealities

## Relevance to "Bizarre that we use deterministic silicon to simulate learning"

This paper directly addresses the central paradox by proposing:

**Instead of fighting physics, embrace it:**
1. **Don't map networks to hardware**—train the hardware directly as a physical neural network
2. **Don't hide non-idealities**—model them explicitly and train through them
3. **Use analog circuits for analog computation**—embrace continuous dynamics, noise, and variations
4. **Time-domain encoding**: Spike times naturally emerge from RC circuit dynamics

**The Physical Becomes Computational:**
- Membrane potentials → neuron states
- Current sources (MOSFETs) → synaptic weights
- Capacitors → temporal integration
- Circuit dynamics → learning dynamics

**Key Innovation:** DSTD enables gradient-based optimization through the actual physics of the circuit. The hardware IS the neural network—not a substrate that approximates one.

## Why This Matters

**Theoretical Significance:**
- Bridges machine learning and physical computing
- Shows that analog circuits can be trained end-to-end with backpropagation
- Validates the "physical neural network" paradigm

**Practical Benefits:**
1. **Better hardware utilization**: Networks trained for specific hardware characteristics
2. **No accuracy loss from mapping**: Train directly on target hardware
3. **Manufacturable**: Uses standard CMOS (Sky130 PDK)
4. **Energy efficient**: Time-domain computing + TTFS coding

**Limitations:**
- Requires differentiable circuit models (DSTD is complex to implement)
- Training on hardware is slower than software simulation
- Scalability to very large networks unclear
- Need fabrication to validate fully

## The Grand Vision

This paper proposes a radical rethinking of neural network hardware:

**Traditional paradigm:**
Software (ideal) → Train → Map → Hardware (non-ideal) → Performance degradation

**Physical Neural Network paradigm:**
Hardware (real physics) → Model → Train directly → Optimized for hardware

The authors demonstrate that we can train neural networks that don't just tolerate hardware imperfections—they're **optimized for them**. The analog circuit isn't trying to be a perfect digital computer. It's being itself, and that's exactly what we train for.

## Technical Details

**Circuit Parameters:**
- Supply voltage: 1.8V
- Resting membrane potential: 1.3V
- MIMCAP: 103.3 fF
- Synapse MOSFET: 250nm × 1μm
- Process: Sky130 (130nm CMOS)

**Optimization:**
- Grid search for E+_rev and E-_rev
- RMSE minimization between RC-Spike model and SPICE
- Uniform current scaling for SNN-to-IMC mapping

**Software Implementation:**
- PyTorch with custom autograd functions
- Automatic differentiation through circuit equations
- Full post-layout SPICE validation
