# Towards Exact Gradient-based Training on Analog In-memory Computing

**Authors:** Zhaoxian Wu, Tayfun Gokmen, Malte J. Rasch, Tianyi Chen
**Institutions:** Rensselaer Polytechnic Institute, IBM T. J. Watson Research Center
**Year:** 2024
**Paper ID:** 2406.12774v1

## Core Thesis

This paper provides the **first rigorous theoretical foundation** for gradient-based training on analog in-memory computing (AIMC) devices. The authors prove that standard SGD **converges inexactly** on analog hardware due to asymmetric device updates, establish a fundamental performance limit via lower bound, and prove that the **Tiki-Taka algorithm eliminates this asymptotic error** and achieves exact convergence to critical points.

## The Fundamental Problem

**Observation:** When you run SGD on analog devices, reducing the learning rate does NOT reduce the final error (as it should with digital SGD). Instead, there's a persistent error floor that won't go away.

**Why digital SGD theory fails:**
- Digital SGD: `W_{k+1} = W_k - α(∇f(W_k) + ε_k)`
- Theory says: error → 0 as α → 0
- Reality on analog: error plateaus regardless of α

**The missing piece:** Asymmetric device updates

## Mathematical Modeling of Analog Training

### Asymmetric Linear Device (ALD)

The authors model analog weight updates using response factors:
- **Up response:** `q+(w) = 1 - (w - w⋄)/τ`
- **Down response:** `q-(w) = 1 + (w - w⋄)/τ`

Where:
- `τ`: device parameter controlling asymmetry range
- `w⋄`: symmetric point (usually 0)

**Pulse Update Mechanism:**
- Weights change via electrical pulses
- Number of pulses ∝ |Δw| (update magnitude)
- Each pulse changes conductance by small amount Δw_min
- Response depends on current weight value and direction

**Key Properties:**
1. **Saturation:** As |w| → τ, updates become ineffective (response → 0)
2. **Fast Reset:** Updates toward w⋄ are amplified (faster response)
3. **Bounded Weight:** Weights naturally stay within [-τ, τ] without explicit clipping

### Proposed Analog SGD Dynamic

**Analog SGD:**
```
W_{k+1} = W_k - α(∇f(W_k) + ε_k) - (α/τ)|∇f(W_k) + ε_k| ⊙ W_k
```

The extra term `-(α/τ)|∇f(W_k) + ε_k| ⊙ W_k` captures asymmetric bias:
- Pushes weights toward symmetric point (0)
- Magnitude depends on both gradient noise and current weight
- Cannot be eliminated by reducing learning rate

## Theoretical Results

### Theorem 1: Analog SGD Converges Inexactly

**Convergence bound:**
```
(1/K) Σ E[||∇f(W_k)||²] ≤ O(√(σ²/K) · 1/(1-W²_max/τ²)) + 4σ²S_K
```

**Two terms:**
1. **Transient error:** `O(√(σ²/K))` - vanishes with iterations (standard SGD rate)
2. **Asymptotic error:** `4σ²S_K` - does NOT vanish, depends on:
   - `σ²`: noise variance (from mini-batching and device non-idealities)
   - `S_K = (1/K)Σ (||W_k||²∞/τ²)/(1-||W_k||²∞/τ²)`: saturation factor

**Key insights:**
- Even with infinite training (K → ∞), error remains
- Error increases with noise and weight saturation
- Factor `1/(1-W²_max/τ²)` slows convergence when weights near boundary

### Theorem 2: Lower Bound (Fundamental Limit)

**Lower bound:**
```
(1/K) Σ E[||∇f(W_k)||²] = Ω(4σ²S_K + 1/√K)
```

**Implications:**
- The asymptotic error `4σ²S_K` is NOT an artifact of analysis
- It's a **fundamental limitation** of using SGD on asymmetric analog devices
- No better SGD-based algorithm can eliminate this error
- Proves need for algorithm specifically designed for analog hardware

**Proof technique:** Constructed explicit example where:
- Quadratic loss function
- Scalar weight (D=1)
- Carefully designed noise distribution that exploits asymmetry
- Shows error floor persists regardless of learning rate

## Tiki-Taka Algorithm

**Motivation:** Reduce noise impact by using auxiliary array

**Two-array approach:**
1. **Main array (W_k):** Stores actual model weights
2. **Auxiliary array (P_k):** Accumulates gradients, initialized to 0

**Update rules:**
```
P_{k+1} = P_k + β(∇f(W_k) + ε_k) - (β/τ)|∇f(W_k) + ε_k| ⊙ P_k   (TT-P)
W_{k+1} = W_k - αP_{k+1} - (α/τ)|P_{k+1}| ⊙ W_k                   (TT-W)
```

**Key idea:**
- P_k acts as gradient estimator
- Asymmetric bias on P_k pushes it toward 0
- When ∇f(W_k) = 0 (at critical point), P_k stabilizes near 0
- Transfer from P_k to W_k has much less noise amplification

**Stability analysis:**
- Expected drift of Analog SGD at critical point: `O(α)`
- Expected drift of Tiki-Taka at critical point: `O(αβ)`
- With β << 1, Tiki-Taka stays near critical point

### Theorem 3: Tiki-Taka Converges Exactly

**Convergence bound:**
```
(1/K) Σ E[||∇f(W_k)||²] ≤ O(√(σ²L/K) · 1/(1-33P²_max/τ²))
```

**Key results:**
1. **Zero asymptotic error:** lim_{K→∞} → 0 (exact convergence!)
2. **Bounded auxiliary array:** `E[||P_k||²∞] ≤ P²_max = 41L²τ⁴D/(c²σ²)`
   - P_max → 0 as noise increases (more noise = better P_k control)
   - Requires sufficient noise: `σ² and D` large enough
3. **Better saturation factor:** `P²_max/τ² << W²_max/τ²` typically
   - P_k hovers near 0, W_k does not (unless 0 is critical point)
   - Faster convergence than Analog SGD

**Requirements (Assumption 5):**
- **Non-zero noise:** `E[|g + ε|] ≥ cσ` for constant c > 0
- Noise pushes P_k toward 0: `E[P_{k+1}] = (1 - (β/τ)E[|ε_k|]) ⊙ P_k`
- Without noise, P_k would drift unbounded

**Optimal learning rate:** `α = O(1/√K)`, `β = 8αL`

## Experimental Validation

### Synthetic Least Squares Problem

**Setup:** Quadratic loss `f(W) = (1/2)||A(W-W*)||²`

**Figure 1 verification:**
- Digital SGD: error decreases as α decreases (as expected)
- Analog SGD: error plateau regardless of α (proves inexact convergence)
- Matches theoretical prediction

**Figure 3 verification:**
- Proposed analog dynamic (Eq. 3) closely matches AIHWKIT simulator
- Much better fit than digital SGD dynamic (Eq. 2)
- Validates mathematical model

**Ablation studies (Figure 4):**
- Asymptotic error increases with smaller τ (more asymmetry)
- Asymptotic error increases with larger σ² (more noise)
- Error independent of initialization (proves it's not optimization artifact)

### Real Datasets

**MNIST with FCN (3 fully-connected layers):**
- τ = 0.78, 0.80: Analog SGD achieves 97-98% but converges slowly
- τ = 0.70: Analog SGD fails (11.35% accuracy)
- Tiki-Taka: 97.5-97.6% across all τ (robust!)
- Digital SGD: 98.15% (baseline)

**MNIST with CNN:**
- Analog SGD: drops >6% accuracy for all τ tested
- Tiki-Taka: 98.8% accuracy (only ~0.3% drop from digital)

**CIFAR-10 with ResNet:**
- Both Analog SGD and Tiki-Taka achieve comparable or better than digital
- ResNet-18: 93.74% (TT) vs 93.03% (digital)
- ResNet-34: 95.15% (TT) vs 93.44% (digital)
- Surprising: analog noise may act as regularization

## Connections to Other Papers

### Paper 1 (First Memristor Training, 2017):
- Paper 1: Empirical demonstration on actual ReRAM hardware
- Paper 7 (this): **First rigorous theory** explaining why/when analog training works
- **Complementary:** Paper 1 shows it works; Paper 7 explains the mathematics

### Paper 4 (AGAD/Chopped-TTv2, 2024):
- Paper 4: Empirical algorithm (c-TTv2) with gradient accumulation
- Paper 7: **Proves Tiki-Taka (basis of TTv2) achieves exact convergence**
- **Connection:** Both use auxiliary arrays and periodic updates
- Paper 7 provides missing theoretical justification for Paper 4's approach

### Paper 5 (Bulk-Switching Memristor CIM, 2023):
- Paper 5: Mixed-precision with selective updates (500x fewer writes)
- Paper 7: Tiki-Taka also reduces main array updates (αβ << α)
- **Common theme:** Reduce frequency of noisy analog updates
- Different approaches to same problem

### Paper 6 (Physical Neural Networks, 2024):
- Paper 6: Train directly on hardware using DSTD (spike-time discretization)
- Paper 7: Train on analog hardware using Tiki-Taka
- **Contrast:**
  - Paper 6: Spiking (time-domain), CMOS-based
  - Paper 7: Rate-based (weight-domain), crossbar-based
- **Common theme:** Both develop training methods that embrace hardware physics

## Relevance to "Bizarre that we use deterministic silicon to simulate learning"

This paper **mathematically proves** why using digital algorithms (SGD) on analog hardware fails:

**The Core Issue:**
1. Neural networks are gradient-based learning systems
2. Learning requires noise (stochasticity from mini-batching)
3. Analog devices have asymmetric responses + their own noise
4. **Interaction between algorithm noise and device asymmetry creates bias**
5. This bias cannot be eliminated by standard digital techniques (reducing α)

**The Insight:**
- Don't try to make analog behave like digital
- Design algorithms (Tiki-Taka) that **exploit analog properties**:
  - Use noise to stabilize auxiliary array near 0
  - Use asymmetric bias to auto-regulate array magnitudes
  - Use two arrays to separate noisy accumulation from clean storage

**Mathematical Proof of Necessity:**
- Lower bound (Theorem 2) proves SGD fundamentally limited on analog
- Tiki-Taka (Theorem 3) proves exact convergence is achievable
- Gap between theorems shows **algorithm-hardware co-design is essential**

## Technical Contributions

**Novel Mathematical Framework:**
1. First discrete-time model of analog SGD with asymmetric updates
2. First convergence rate with explicit asymptotic error term
3. First lower bound proving fundamental limits
4. First exact convergence proof for analog training algorithm

**Key Assumptions:**
- **Assumption 4 (Bounded Saturation):** `||W_k||∞ ≤ W_max < τ`
  - Proven to hold under strong convexity
  - Empirically validated for neural networks

- **Assumption 5 (Non-zero Noise):** `E[|g + ε|] ≥ cσ > 0`
  - Required for Tiki-Taka stability
  - Naturally satisfied by mini-batch sampling + device noise

**Proof Techniques:**
- **Tracking Lemma:** Bounds how well P_k approximates scaled gradient
- **Weight Decay Lemma:** Proves P_k remains bounded via noise-induced decay
- **Lyapunov Function:** Combines objective value with auxiliary array terms
- **Telescope Summation:** Shows accumulated error over iterations

## Why This Matters

**For Theory:**
- Closes gap between empirical success and theoretical understanding
- Proves exact gradient-based training possible on imperfect analog hardware
- Establishes that hardware non-idealities can be features, not bugs

**For Practice:**
- Explains when/why Tiki-Taka (used in Papers 4, 20) works
- Provides design guidelines for analog training algorithms
- Shows sufficient noise is beneficial (counterintuitive!)

**For Future Work:**
- Framework extends to other devices (PCM, ECRAM, etc.)
- Can analyze other algorithms (momentum, Adam, etc.)
- Opens path to hardware-aware algorithm design

## Limitations

**Theoretical:**
- Analysis specific to Asymmetric Linear Devices (ALD)
- Extension to other device models (power-law, exponential) remains open
- Strong convexity used for some proofs (though experiments use non-convex NNs)

**Practical:**
- Tiki-Taka requires 2x hardware (two crossbar arrays)
- Transfer between arrays has cost (mitigated in TTv2)
- Requires sufficient noise (may not hold for all applications)

**Experimental:**
- Simulations only (AIHWKIT), not actual hardware validation
- Limited to relatively small models and datasets
- Gap between theory (D → ∞) and practice (finite D)

## The Big Picture

This paper provides **mathematical proof** of what practitioners discovered empirically:

**Traditional View (Wrong):**
Analog hardware → Try to emulate digital algorithms → Fight imperfections

**This Paper's View (Right):**
Analog hardware → Design algorithms for analog physics → Exploit imperfections

**Key Equation:**
```
Analog SGD: fundamental error = 4σ²S_K  (provably unavoidable)
Tiki-Taka: fundamental error = 0        (provably achievable)
```

The difference is **algorithm design** that respects hardware physics rather than fighting it.

**Connection to Central Theme:**
We don't need perfect deterministic silicon. We need:
1. Algorithms that work with noisy, imperfect, analog hardware
2. Mathematical understanding of when/why they work
3. **This paper provides both**

The "bizarre" approach isn't using analog hardware—it's trying to force analog hardware to behave digitally. This paper proves we can do better.
