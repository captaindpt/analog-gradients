# Deep learning with coherent nanophotonic circuits

**Authors:** Yichen Shen et al.
**Year:** 2020 (updated)
**arXiv:** 2006.01981v2

## Core Thesis
Instead of using electrons in silicon, this paper demonstrates neural networks built with **PHOTONS** (light) in nanophotonic circuits, achieving ultra-fast, ultra-low-power matrix multiplication.

## The Fundamental Innovation: Computing With Light

```
Traditional Computing:
    Electrons moving through wires/transistors
    Limited by resistance, capacitance, heat
    Speed of light doesn't matter (electrons move slowly)

Photonic Computing:
    PHOTONS moving through waveguides
    No resistance, no heat generation
    Literally moves at speed of light
    Matrix multiplication via optical interference
```

## How It Works: Physics as Computation

**The Magic: Mach-Zehnder Interferometers (MZIs)**

```
Input light →
    [Split into two paths] →
        [Phase shift on each path = "weight"] →
            [Recombine → interference] →
                Output light intensity

The interference pattern COMPUTES the multiplication!
```

**Matrix Multiplication via Light:**

```
For matrix W × vector v:

    v₁ ──┐
    v₂ ──┼──► [Mesh of MZIs] ──► W·v
    v₃ ──┘      (each MZI = one multiplication)

All multiplications happen SIMULTANEOUSLY via light propagation
Time complexity: O(1) regardless of matrix size! (limited by speed of light)
```

## The Architecture

**Physical Implementation:**
- Silicon photonic chip
- Mesh of Mach-Zehnder interferometers
- Input: Encode data as light intensity/phase
- Weights: Programmed as phase shifts in MZIs
- Output: Detected by photodiodes

**Singular Value Decomposition (SVD) Encoding:**
Any matrix W can be decomposed:
```
W = U Σ V†

Where U, V are unitary matrices (can be implemented as MZI meshes)
Σ is diagonal (can be implemented as variable attenuators)
```

## Performance Results

**Compared to digital computers:**
- Speed: ~1000× faster for matrix operations
- Energy: ~100,000× more efficient (aJ/operation vs fJ/operation)
- Bandwidth: THz potential vs GHz digital limit

**Demonstrated tasks:**
- Vowel recognition: 92% accuracy
- Image classification: Comparable to digital
- Real-time processing: Video-rate inference

## Why Photonics is Radical

**The Advantages:**
1. **Parallel by nature:** All paths compute simultaneously
2. **Zero resistance:** Photons don't interact with each other
3. **No heat:** Minimal energy dissipation
4. **Coherent interference:** Natural analog computation
5. **High bandwidth:** THz frequency vs GHz electronic

**The Physics Does the Math:**
```
Electronic:
    1. Fetch operands from memory
    2. Send to ALU
    3. Perform multiplication (many clock cycles)
    4. Write back result

Photonic:
    1. Light enters chip
    2. Interference happens naturally
    3. Result emerges
    (No fetch, no execute, no writeback - just physics!)
```

## Technical Challenges

1. **Programming weights:** Need to calibrate phase shifters precisely
2. **Non-linearity:** Linear optics only - need external non-linear activations
3. **Fabrication tolerances:** Nanoscale precision required
4. **Analog-digital interface:** Converting between light and electrons
5. **Training:** Weights still programmed externally (no on-chip learning yet)

## The Paradigm It Represents

```
Current AI:
    Digital CPU/GPU → Simulates matrix math → Slow, power-hungry

Photonic AI:
    Light naturally interferes → Matrix math emerges → Fast, energy-free

The computation isn't simulated - it's a physical phenomenon!
```

## Comparison to Memristors (Paper 1)

| Aspect | Memristors | Photonics |
|--------|-----------|-----------|
| Physical medium | Electrons (resistance) | Photons (light) |
| Speed | Nanoseconds | Picoseconds |
| Energy | Femtojoules | Attojoules |
| On-chip training | Yes | Not yet |
| Parallelism | High | Extreme |
| Precision | ~4-5 bits | ~8 bits |
| Main advantage | Learning in hardware | Speed & efficiency |

## The Deep Insight

This paper answers your question from a different angle:

**Instead of:** "Make silicon adaptive like a brain"
**It asks:** "Why use electrons at all?"

```
Neurons communicate with chemicals (slow, analog, parallel)
Current AI uses electrons (faster, but still wrong substrate)
Photonics uses light (fastest possible, massively parallel)

Maybe the right substrate isn't even matter-based - it's light-based!
```

## Profound Implication

The computation happens at **literally the speed of light**. The matrix multiplication isn't executed sequentially - it happens as a single physical event when light propagates through the circuit.

```
Digital: for i, for j: sum += w[i][j] * v[i]  (millions of operations)
Photonic: [light enters] → [interference] → [result]  (one physical event)
```

## Limitations

- Still needs external training (weights programmed from outside)
- Non-linear activations require external electronics
- Analog-to-digital conversion bottleneck
- Fabrication complexity
- Limited to linear operations natively

## Future Vision

Imagine a chip where:
- Inference happens at speed of light
- Power consumption nearly zero
- No memory bandwidth bottleneck (weights are physical)
- Scales to massive matrices without time penalty

## Connection to Your Insight

This is another answer to "why use deterministic digital gates?":

**Don't use gates at all. Use waves.**

The network isn't simulated in software, implemented in electronics, or even in memristors - it's implemented in **coherent light interference patterns**. The most fundamental computation possible.
